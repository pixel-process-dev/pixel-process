{"title":"Random Forests","markdown":{"yaml":{"title":"Random Forests","navtitle":"Random Forest (NB)","subtitle":"Deep Dive: Decision Trees & Random Forests","description":"In-depth exploration of hot decision tree and random forest models work.","format":{"html":{"page-layout":"full","title-block-banner":true}}},"headingText":"Overview","containsRefs":false,"markdown":"\n\nThis notebook will use the Iris flower dataset from sklearn to introduce classification with Random Forest.\n\nFeature importance and partial dependency plots will be created once the model is trained.\n\n\n1. Set up train-test data\n1. Review decision trees\n1. Visualize a decision tree\n1. Introduction to random forest\n1. Train and predict with a random forest\n1. Visualize feature importances\n1. Create partial dependency plots for random forest\n1. Knowledge check and questions\n1. Additional EDA & Visualizations\n\n## Prerequisites\n- Python imports\n- Train-test split\n- Classification metrics\n- Decision Trees\n- Measures of node impurity (Shannon Entropy and Gini Index)\n\n# Learning Objectives\n\n1. Apply a random forest classifier to a dataset\n1. Visualize feature importances from a trained random forest model\n\n# Dependencies\nThis notebook was made with the following packages:\n1. python=3.7.6\n1. sklearn=1.2.2\n1. matplotlib=3.1.3\n1. pandas=1.0.1\n\n# Set up data set \n\n## Train-Test Split\n\n# Decision Tree Review\n\nWe will first build a decision tree model to review their structure before moving on to random forest classification\n\n# Decision Tree Knowledge\n1. Are decision trees deterministic?\n    - Yes they are deterministic. The best split will be found at each iterative step and will be used. \n1. How are decision trees split determined?\n    - Information gain or entropy reduction\n1. Are decision trees parametric? \n    - No, they are not parametric. Splits may differ in direction based on values.\n1. Decision trees often have high variance, why might that be?\n    - May split on wrong features or overfit to data.\n\n# From tree to forest\n\n1. How might multiple decision trees be leveraged to reduce variance?\n    1. Create multiple classifiers and average the results. Multiple weak learners can ofter produce a strong learner.\n1. Would multiple deterministic decision trees be useful?\n    1. Only if they were NOT deterministic.\n1. How could they not be deteministic?\n    1. Bootstrapping data and limiting features at each step\n\n# Random Forest\n- An ensemble method that combines many decision trees which have been given different subsets of the data and features to create a strong learner \n- Decision made on majority vote\n- Reduces variance and creates a non-deterministic model\n- Generally use a large number of bushy trees\n- Can get excellent performance with minimum tuning\n\n\n# Random Forest Example\n\n# Visualizing trees in the forest\n\nFocusing only on the first split in the two trees above, we can see differences in the splits used to build the forest.\n\nThe first one split on sepal lenth <= 5.35 and the second on petal length <=2.45. The depth of the trees also varies. This is due to the randomness induced in the trees with bootstrapping and feature selection. \n\n\n# Random Forest Feature Importances\n\n# Partial Dependency Plots\n\nShows the marginal effect of a feature on predictions.\n\nShows effect of predictions when all observations have a feature set to a particular value.\n\n\n# Advantages of Random Forests\n\n1. Ensemble model (Wisdom of the Crowd)\n1. Good out-of-box performance\n1. Multiple trees can be trained at once \n\n# Cons of Random Forests\n1. Expensive to train\n2. Can produce very large model files\n\n# Model Comparison and evaluation\n\n1. Which model performed better?\n2. Which feature had the most influence on the random forest model?\n\n\n# Knowledge Check\n1. A random forest is the same as combining many decision trees?\n1. Name two ways in which random forests are made non-deterministic. \n1. Random forest classifiers are parametric?\n1. Name two visuals to help intrepret random forest models.\n\n# Review Objectives \n1. Apply a random forest classifier to a dataset.\n1. Visualize feature importances from a trained random forest model.\n\n# Next Steps\n1. Tune the random forest classifier\n    1. Number of estimators\n    1. Criterion: default is gini, can also try entropy\n    1. Max depth, min samples\n    1. Number of features\n1. Create a random forest regressor and test on sklearn Boston housing data\n    1. Compare to decision tree model\n    1. Plot feature importances\n    1. Create partial dependency plots\n\n# Full Day Activities\n1. Code random forest from scratch\n1. Code partial dependecy plot function from scratch\n\n# Additional EDA & Visualizations\n","srcMarkdownNoYaml":"\n\nThis notebook will use the Iris flower dataset from sklearn to introduce classification with Random Forest.\n\nFeature importance and partial dependency plots will be created once the model is trained.\n\n# Overview\n\n1. Set up train-test data\n1. Review decision trees\n1. Visualize a decision tree\n1. Introduction to random forest\n1. Train and predict with a random forest\n1. Visualize feature importances\n1. Create partial dependency plots for random forest\n1. Knowledge check and questions\n1. Additional EDA & Visualizations\n\n## Prerequisites\n- Python imports\n- Train-test split\n- Classification metrics\n- Decision Trees\n- Measures of node impurity (Shannon Entropy and Gini Index)\n\n# Learning Objectives\n\n1. Apply a random forest classifier to a dataset\n1. Visualize feature importances from a trained random forest model\n\n# Dependencies\nThis notebook was made with the following packages:\n1. python=3.7.6\n1. sklearn=1.2.2\n1. matplotlib=3.1.3\n1. pandas=1.0.1\n\n# Set up data set \n\n## Train-Test Split\n\n# Decision Tree Review\n\nWe will first build a decision tree model to review their structure before moving on to random forest classification\n\n# Decision Tree Knowledge\n1. Are decision trees deterministic?\n    - Yes they are deterministic. The best split will be found at each iterative step and will be used. \n1. How are decision trees split determined?\n    - Information gain or entropy reduction\n1. Are decision trees parametric? \n    - No, they are not parametric. Splits may differ in direction based on values.\n1. Decision trees often have high variance, why might that be?\n    - May split on wrong features or overfit to data.\n\n# From tree to forest\n\n1. How might multiple decision trees be leveraged to reduce variance?\n    1. Create multiple classifiers and average the results. Multiple weak learners can ofter produce a strong learner.\n1. Would multiple deterministic decision trees be useful?\n    1. Only if they were NOT deterministic.\n1. How could they not be deteministic?\n    1. Bootstrapping data and limiting features at each step\n\n# Random Forest\n- An ensemble method that combines many decision trees which have been given different subsets of the data and features to create a strong learner \n- Decision made on majority vote\n- Reduces variance and creates a non-deterministic model\n- Generally use a large number of bushy trees\n- Can get excellent performance with minimum tuning\n\n\n# Random Forest Example\n\n# Visualizing trees in the forest\n\nFocusing only on the first split in the two trees above, we can see differences in the splits used to build the forest.\n\nThe first one split on sepal lenth <= 5.35 and the second on petal length <=2.45. The depth of the trees also varies. This is due to the randomness induced in the trees with bootstrapping and feature selection. \n\n\n# Random Forest Feature Importances\n\n# Partial Dependency Plots\n\nShows the marginal effect of a feature on predictions.\n\nShows effect of predictions when all observations have a feature set to a particular value.\n\n\n# Advantages of Random Forests\n\n1. Ensemble model (Wisdom of the Crowd)\n1. Good out-of-box performance\n1. Multiple trees can be trained at once \n\n# Cons of Random Forests\n1. Expensive to train\n2. Can produce very large model files\n\n# Model Comparison and evaluation\n\n1. Which model performed better?\n2. Which feature had the most influence on the random forest model?\n\n\n# Knowledge Check\n1. A random forest is the same as combining many decision trees?\n1. Name two ways in which random forests are made non-deterministic. \n1. Random forest classifiers are parametric?\n1. Name two visuals to help intrepret random forest models.\n\n# Review Objectives \n1. Apply a random forest classifier to a dataset.\n1. Visualize feature importances from a trained random forest model.\n\n# Next Steps\n1. Tune the random forest classifier\n    1. Number of estimators\n    1. Criterion: default is gini, can also try entropy\n    1. Max depth, min samples\n    1. Number of features\n1. Create a random forest regressor and test on sklearn Boston housing data\n    1. Compare to decision tree model\n    1. Plot feature importances\n    1. Create partial dependency plots\n\n# Full Day Activities\n1. Code random forest from scratch\n1. Code partial dependecy plot function from scratch\n\n# Additional EDA & Visualizations\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../assets/css/tokens.css","../assets/css/themes.css","../assets/css/body-bg.css","../assets/css/navbar.css","../assets/css/sidebar.css","../assets/css/toc.css","../assets/css/links.css","../assets/css/headers.css","../assets/css/footer.css","../assets/css/title-block.css","../assets/css/code-blocks.css","../assets/css/code-blocks-interactive.css","../assets/css/custom-callouts.css","../assets/css/category-grid.css","../assets/css/tab-cards.css","../assets/css/flipbook.css","../assets/css/tables.css","../assets/css/faqs.css","../assets/css/buttons.css","../assets/css/quick-links.css","../assets/css/branding.css"],"include-after-body":["../assets/html/wip-footer.html","../assets/html/custom-footer.html"],"toc":true,"output-file":"random-forest.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","theme":{"light":"flatly","dark":"darkly"},"google-fonts":["Inter"],"page-layout":"full","code-copy":true,"title-block-banner":true,"plotly":true,"sidebar":"machine-learning","title":"Random Forests","navtitle":"Random Forest (NB)","subtitle":"Deep Dive: Decision Trees & Random Forests","description":"In-depth exploration of hot decision tree and random forest models work."},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}