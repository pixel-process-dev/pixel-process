[
  {
    "objectID": "machine-learning/regression-housing.html",
    "href": "machine-learning/regression-housing.html",
    "title": "Housing Regression",
    "section": "",
    "text": "This dataset is composed of over 20,000 rows and 9 columns.\nNotebook goals:\n\nComplete regression analysis with train-test split and two models for comparison\nData investigation of summary statistics and visualizations\nMetric evaluation and performance visualization\n\n\n\n\n# import os\n# import numpy as np\nimport pandas as pd\n# import seaborn as sns\n# import matplotlib.pyplot as plt\nfrom skimpy import skim\n\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import (\n    mean_squared_error,\n    explained_variance_score\n)\n\n\n\n\n\n\n\nImport the data from sklearn\nTransfer data into pandas DataFrame\nBasic data overview\n\nNote: Data is returned as a bunch object, similar to a dictionary. We’ll convert it to a pandas df.\n\n# Load data\ndata = fetch_california_housing()\n\n\ntype(data)\n\nsklearn.utils._bunch.Bunch\n\n\n\ndata.keys()\n\ndict_keys(['data', 'target', 'frame', 'target_names', 'feature_names', 'DESCR'])\n\n\n\n# Convert data to pandas dataframe\ndf = pd.DataFrame(data=data['data'], columns=data['feature_names'])\n\n\ndf.head()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n\n\n\n\n\n\n\n\ndf.shape\n\n(20640, 8)\n\n\n\n\n\ndf[\"target\"] = data['target']\n\n\ndf.head()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\ntarget\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n4.526\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n3.585\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n3.521\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n3.413\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n3.422\n\n\n\n\n\n\n\n\n# NOTE: 1 additional column\ndf.shape\n\n(20640, 9)\n\n\n\n\n\n\n# Built-in pandas function\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 9 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   MedInc      20640 non-null  float64\n 1   HouseAge    20640 non-null  float64\n 2   AveRooms    20640 non-null  float64\n 3   AveBedrms   20640 non-null  float64\n 4   Population  20640 non-null  float64\n 5   AveOccup    20640 non-null  float64\n 6   Latitude    20640 non-null  float64\n 7   Longitude   20640 non-null  float64\n 8   target      20640 non-null  float64\ndtypes: float64(9)\nmemory usage: 1.4 MB\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\ntarget\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n\n\nmean\n3.870671\n28.639486\n5.429000\n1.096675\n1425.476744\n3.070655\n35.631861\n-119.569704\n2.068558\n\n\nstd\n1.899822\n12.585558\n2.474173\n0.473911\n1132.462122\n10.386050\n2.135952\n2.003532\n1.153956\n\n\nmin\n0.499900\n1.000000\n0.846154\n0.333333\n3.000000\n0.692308\n32.540000\n-124.350000\n0.149990\n\n\n25%\n2.563400\n18.000000\n4.440716\n1.006079\n787.000000\n2.429741\n33.930000\n-121.800000\n1.196000\n\n\n50%\n3.534800\n29.000000\n5.229129\n1.048780\n1166.000000\n2.818116\n34.260000\n-118.490000\n1.797000\n\n\n75%\n4.743250\n37.000000\n6.052381\n1.099526\n1725.000000\n3.282261\n37.710000\n-118.010000\n2.647250\n\n\nmax\n15.000100\n52.000000\n141.909091\n34.066667\n35682.000000\n1243.333333\n41.950000\n-114.310000\n5.000010\n\n\n\n\n\n\n\n\nskim(df)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ Dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 20640  │ │ float64     │ 9     │                                                          │\n│ │ Number of columns │ 9      │ └─────────────┴───────┘                                                          │\n│ └───────────────────┴────────┘                                                                                  │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━┳━━━━━┳━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┓  │\n│ ┃ column       ┃ NA  ┃ NA %  ┃ mean     ┃ sd       ┃ p0      ┃ p25     ┃ p50     ┃ p75    ┃ p100    ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━╇━━━━━╇━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━┩  │\n│ │ MedInc       │   0 │     0 │    3.871 │      1.9 │  0.4999 │   2.563 │   3.535 │  4.743 │      15 │  ▆█▂   │  │\n│ │ HouseAge     │   0 │     0 │    28.64 │    12.59 │       1 │      18 │      29 │     37 │      52 │ ▂▆███▅ │  │\n│ │ AveRooms     │   0 │     0 │    5.429 │    2.474 │  0.8462 │   4.441 │   5.229 │  6.052 │   141.9 │   █    │  │\n│ │ AveBedrms    │   0 │     0 │    1.097 │   0.4739 │  0.3333 │   1.006 │   1.049 │    1.1 │   34.07 │   █    │  │\n│ │ Population   │   0 │     0 │     1425 │     1132 │       3 │     787 │    1166 │   1725 │   35680 │   █    │  │\n│ │ AveOccup     │   0 │     0 │    3.071 │    10.39 │  0.6923 │    2.43 │   2.818 │  3.282 │    1243 │   █    │  │\n│ │ Latitude     │   0 │     0 │    35.63 │    2.136 │   32.54 │   33.93 │   34.26 │  37.71 │   41.95 │ █▃▁▆▁  │  │\n│ │ Longitude    │   0 │     0 │   -119.6 │    2.004 │  -124.3 │  -121.8 │  -118.5 │   -118 │  -114.3 │ ▁▆▂█▃  │  │\n│ │ target       │   0 │     0 │    2.069 │    1.154 │    0.15 │   1.196 │   1.797 │  2.647 │       5 │ ▄█▆▃▂▂ │  │\n│ └──────────────┴─────┴───────┴──────────┴──────────┴─────────┴─────────┴─────────┴────────┴─────────┴────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\n\n\n\n\n\nSplitting data before EDA can be helpful to avoid data leakage or incorrect assumptions about what the data shows.\nEDA and training will use only the train data. Test data will be used for evaluation only.\nNote: Splitting is normally done with X (features) and y (target) separated to avoid data leakage. Here, we will training and test data and split X, y before model training.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\ntrain_df, test_df = train_test_split(df, test_size=0.33, random_state=42)\n\n\n\n\n\nprint(f'Shape of original data: {df.shape}')\nprint(f'Shape of training data: {train_df.shape}')\nprint(f'Shape of training data: {test_df.shape}')\n\nShape of original data: (20640, 9)\nShape of training data: (13828, 9)\nShape of training data: (6812, 9)\n\n\n\nprint(f'Percent of data in training: {len(train_df)/len(df):.0%}')\nprint(f'Percent of data in test: {len(test_df)/len(df):.0%}')\n\nPercent of data in training: 67%\nPercent of data in test: 33%\n\n\n\n\nIt is best not to peek at test.\n\nskim(train_df)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ Dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 13828  │ │ float64     │ 9     │                                                          │\n│ │ Number of columns │ 9      │ └─────────────┴───────┘                                                          │\n│ └───────────────────┴────────┘                                                                                  │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━┳━━━━━┳━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┓  │\n│ ┃ column       ┃ NA  ┃ NA %  ┃ mean     ┃ sd       ┃ p0      ┃ p25     ┃ p50     ┃ p75    ┃ p100    ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━╇━━━━━╇━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━┩  │\n│ │ MedInc       │   0 │     0 │    3.877 │    1.903 │  0.4999 │   2.569 │   3.539 │  4.757 │      15 │  ▅█▂   │  │\n│ │ HouseAge     │   0 │     0 │    28.56 │     12.6 │       1 │      18 │      29 │     37 │      52 │ ▂▆███▅ │  │\n│ │ AveRooms     │   0 │     0 │    5.437 │    2.449 │  0.8889 │    4.46 │   5.232 │  6.059 │   141.9 │   █    │  │\n│ │ AveBedrms    │   0 │     0 │    1.098 │   0.4457 │  0.3333 │   1.007 │    1.05 │    1.1 │   25.64 │   █    │  │\n│ │ Population   │   0 │     0 │     1431 │     1146 │       3 │     793 │    1170 │   1729 │   35680 │   █    │  │\n│ │ AveOccup     │   0 │     0 │    3.129 │    12.65 │  0.6923 │   2.432 │    2.82 │  3.282 │    1243 │   █    │  │\n│ │ Latitude     │   0 │     0 │    35.65 │    2.134 │   32.55 │   33.94 │   34.27 │  37.72 │   41.95 │ █▃▁▆▁  │  │\n│ │ Longitude    │   0 │     0 │   -119.6 │    2.005 │  -124.3 │  -121.8 │  -118.5 │   -118 │  -114.3 │ ▁▆▂█▃  │  │\n│ │ target       │   0 │     0 │    2.067 │    1.154 │    0.15 │   1.194 │   1.792 │   2.64 │       5 │ ▄█▆▃▂▂ │  │\n│ └──────────────┴─────┴───────┴──────────┴──────────┴─────────┴─────────┴─────────┴────────┴─────────┴────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\n\n# Built-in pandas function, returns df\ndf_desc = df.describe()\ndf_desc\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\ntarget\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n\n\nmean\n3.870671\n28.639486\n5.429000\n1.096675\n1425.476744\n3.070655\n35.631861\n-119.569704\n2.068558\n\n\nstd\n1.899822\n12.585558\n2.474173\n0.473911\n1132.462122\n10.386050\n2.135952\n2.003532\n1.153956\n\n\nmin\n0.499900\n1.000000\n0.846154\n0.333333\n3.000000\n0.692308\n32.540000\n-124.350000\n0.149990\n\n\n25%\n2.563400\n18.000000\n4.440716\n1.006079\n787.000000\n2.429741\n33.930000\n-121.800000\n1.196000\n\n\n50%\n3.534800\n29.000000\n5.229129\n1.048780\n1166.000000\n2.818116\n34.260000\n-118.490000\n1.797000\n\n\n75%\n4.743250\n37.000000\n6.052381\n1.099526\n1725.000000\n3.282261\n37.710000\n-118.010000\n2.647250\n\n\nmax\n15.000100\n52.000000\n141.909091\n34.066667\n35682.000000\n1243.333333\n41.950000\n-114.310000\n5.000010\n\n\n\n\n\n\n\n\n\n\n\n\n# Instantiate model, fit model, save model\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[21], line 3\n      1 # Instantiate model, fit model, save model\n      2 lr = LinearRegression()\n----&gt; 3 lr.fit(X_train, y_train)\n\nNameError: name 'X_train' is not defined\n\n\n\n\n# Instantiate model, fit model, save model\nrf = RandomForestRegressor()\nrf.fit(X_train, y_train)\n\n\n# Predict on train with both models\n# NOTE: test metrics are more insightful\nlr_train_preds = lr.predict(X_train)\nrf_train_preds = rf.predict(X_train)\n\n\n# Calculate mean squared error for both models\nlr_mse = mean_squared_error(y_train, lr_train_preds)\nrf_mse = mean_squared_error(y_train, rf_train_preds)\n\n\n# Print calculations\nprint(f\"The MSE for the linear regression models is : {lr_mse: .2f}\")\nprint(f\"The MSE for the random forest regression models is : {rf_mse: .2f}\")\n\n\n# Plot both predictions\nplt.figure(figsize=(10,10))\nplt.scatter(y_train, lr_train_preds, c='crimson', label='Linear Regression')\nplt.scatter(y_train, rf_train_preds, c='gold', label='RF Regression')\n\nplt.xlabel('True Values', fontsize=15)\nplt.ylabel('Predictions', fontsize=15)\nplt.title('Training Error', fontsize=15)\n\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n# linear regression predict\nlr_preds = lr.predict(X_test)\nlr_preds\n\n\n# random forest regression predict\nrf_preds = rf.predict(X_test)\nrf_preds\n\n\n# Calculate explained variance for both models\nlr_evs = explained_variance_score(y_test, lr_preds)\nrf_evs = explained_variance_score(y_test, rf_preds)\n\n\n# Display explained variance scores\nprint(f'The explained variance score for the linear regression models is: {lr_evs: .2f}')\nprint(f'The explained variance score for the random forest regression models is: {rf_evs: .2f}')\n\n\n# Calculate mean squared error (MSE)\nlr_mse = mean_squared_error(y_test, lr_preds)\nrf_mse = mean_squared_error(y_test, rf_preds)\n\n\n# Display MSE\nprint(f\"The MSE for the linear regression models is : {lr_mse: .2f}\")\nprint(f\"The MSE for the random forest regression models is : {rf_mse: .2f}\")\n\n\n# create y_df with real and predicted values\ny_df=pd.DataFrame({'y_true': y_test, 'lr_preds': lr_preds, 'rf_preds': rf_preds})\n\n\n# Check df\ny_df.head()\n\n\n# Get correlation across real, lr, and rf values\ny_df.corr()\n\n\n# Seaborn pair plot on y data\nsns.pairplot(y_df)\n\n\n# Plot results\nplt.figure(figsize=(10,10))\nplt.scatter(y_test.target, lr_preds, c='crimson', label='Linear Regression')\nplt.scatter(y_test.target, rf_preds, c='gold', label='RF Regression')\n\nplt.xlabel('True Values', fontsize=15)\nplt.ylabel('Predictions', fontsize=15)\nplt.title('Test Error', fontsize=15)\n\nplt.legend()\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Pixel Process",
      "Regression",
      "Housing Regression"
    ]
  },
  {
    "objectID": "machine-learning/regression-housing.html#data",
    "href": "machine-learning/regression-housing.html#data",
    "title": "Housing Regression",
    "section": "",
    "text": "Import the data from sklearn\nTransfer data into pandas DataFrame\nBasic data overview\n\nNote: Data is returned as a bunch object, similar to a dictionary. We’ll convert it to a pandas df.\n\n# Load data\ndata = fetch_california_housing()\n\n\ntype(data)\n\nsklearn.utils._bunch.Bunch\n\n\n\ndata.keys()\n\ndict_keys(['data', 'target', 'frame', 'target_names', 'feature_names', 'DESCR'])\n\n\n\n# Convert data to pandas dataframe\ndf = pd.DataFrame(data=data['data'], columns=data['feature_names'])\n\n\ndf.head()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n\n\n\n\n\n\n\n\ndf.shape\n\n(20640, 8)\n\n\n\n\n\ndf[\"target\"] = data['target']\n\n\ndf.head()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\ntarget\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n4.526\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n3.585\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n3.521\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n3.413\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n3.422\n\n\n\n\n\n\n\n\n# NOTE: 1 additional column\ndf.shape\n\n(20640, 9)\n\n\n\n\n\n\n# Built-in pandas function\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 9 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   MedInc      20640 non-null  float64\n 1   HouseAge    20640 non-null  float64\n 2   AveRooms    20640 non-null  float64\n 3   AveBedrms   20640 non-null  float64\n 4   Population  20640 non-null  float64\n 5   AveOccup    20640 non-null  float64\n 6   Latitude    20640 non-null  float64\n 7   Longitude   20640 non-null  float64\n 8   target      20640 non-null  float64\ndtypes: float64(9)\nmemory usage: 1.4 MB\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\ntarget\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n\n\nmean\n3.870671\n28.639486\n5.429000\n1.096675\n1425.476744\n3.070655\n35.631861\n-119.569704\n2.068558\n\n\nstd\n1.899822\n12.585558\n2.474173\n0.473911\n1132.462122\n10.386050\n2.135952\n2.003532\n1.153956\n\n\nmin\n0.499900\n1.000000\n0.846154\n0.333333\n3.000000\n0.692308\n32.540000\n-124.350000\n0.149990\n\n\n25%\n2.563400\n18.000000\n4.440716\n1.006079\n787.000000\n2.429741\n33.930000\n-121.800000\n1.196000\n\n\n50%\n3.534800\n29.000000\n5.229129\n1.048780\n1166.000000\n2.818116\n34.260000\n-118.490000\n1.797000\n\n\n75%\n4.743250\n37.000000\n6.052381\n1.099526\n1725.000000\n3.282261\n37.710000\n-118.010000\n2.647250\n\n\nmax\n15.000100\n52.000000\n141.909091\n34.066667\n35682.000000\n1243.333333\n41.950000\n-114.310000\n5.000010\n\n\n\n\n\n\n\n\nskim(df)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ Dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 20640  │ │ float64     │ 9     │                                                          │\n│ │ Number of columns │ 9      │ └─────────────┴───────┘                                                          │\n│ └───────────────────┴────────┘                                                                                  │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━┳━━━━━┳━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┓  │\n│ ┃ column       ┃ NA  ┃ NA %  ┃ mean     ┃ sd       ┃ p0      ┃ p25     ┃ p50     ┃ p75    ┃ p100    ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━╇━━━━━╇━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━┩  │\n│ │ MedInc       │   0 │     0 │    3.871 │      1.9 │  0.4999 │   2.563 │   3.535 │  4.743 │      15 │  ▆█▂   │  │\n│ │ HouseAge     │   0 │     0 │    28.64 │    12.59 │       1 │      18 │      29 │     37 │      52 │ ▂▆███▅ │  │\n│ │ AveRooms     │   0 │     0 │    5.429 │    2.474 │  0.8462 │   4.441 │   5.229 │  6.052 │   141.9 │   █    │  │\n│ │ AveBedrms    │   0 │     0 │    1.097 │   0.4739 │  0.3333 │   1.006 │   1.049 │    1.1 │   34.07 │   █    │  │\n│ │ Population   │   0 │     0 │     1425 │     1132 │       3 │     787 │    1166 │   1725 │   35680 │   █    │  │\n│ │ AveOccup     │   0 │     0 │    3.071 │    10.39 │  0.6923 │    2.43 │   2.818 │  3.282 │    1243 │   █    │  │\n│ │ Latitude     │   0 │     0 │    35.63 │    2.136 │   32.54 │   33.93 │   34.26 │  37.71 │   41.95 │ █▃▁▆▁  │  │\n│ │ Longitude    │   0 │     0 │   -119.6 │    2.004 │  -124.3 │  -121.8 │  -118.5 │   -118 │  -114.3 │ ▁▆▂█▃  │  │\n│ │ target       │   0 │     0 │    2.069 │    1.154 │    0.15 │   1.196 │   1.797 │  2.647 │       5 │ ▄█▆▃▂▂ │  │\n│ └──────────────┴─────┴───────┴──────────┴──────────┴─────────┴─────────┴─────────┴────────┴─────────┴────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯",
    "crumbs": [
      "Pixel Process",
      "Regression",
      "Housing Regression"
    ]
  },
  {
    "objectID": "machine-learning/regression-housing.html#train-test-split",
    "href": "machine-learning/regression-housing.html#train-test-split",
    "title": "Housing Regression",
    "section": "",
    "text": "Splitting data before EDA can be helpful to avoid data leakage or incorrect assumptions about what the data shows.\nEDA and training will use only the train data. Test data will be used for evaluation only.\nNote: Splitting is normally done with X (features) and y (target) separated to avoid data leakage. Here, we will training and test data and split X, y before model training.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\ntrain_df, test_df = train_test_split(df, test_size=0.33, random_state=42)",
    "crumbs": [
      "Pixel Process",
      "Regression",
      "Housing Regression"
    ]
  },
  {
    "objectID": "machine-learning/regression-housing.html#eda",
    "href": "machine-learning/regression-housing.html#eda",
    "title": "Housing Regression",
    "section": "",
    "text": "print(f'Shape of original data: {df.shape}')\nprint(f'Shape of training data: {train_df.shape}')\nprint(f'Shape of training data: {test_df.shape}')\n\nShape of original data: (20640, 9)\nShape of training data: (13828, 9)\nShape of training data: (6812, 9)\n\n\n\nprint(f'Percent of data in training: {len(train_df)/len(df):.0%}')\nprint(f'Percent of data in test: {len(test_df)/len(df):.0%}')\n\nPercent of data in training: 67%\nPercent of data in test: 33%\n\n\n\n\nIt is best not to peek at test.\n\nskim(train_df)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ Dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 13828  │ │ float64     │ 9     │                                                          │\n│ │ Number of columns │ 9      │ └─────────────┴───────┘                                                          │\n│ └───────────────────┴────────┘                                                                                  │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━┳━━━━━┳━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┓  │\n│ ┃ column       ┃ NA  ┃ NA %  ┃ mean     ┃ sd       ┃ p0      ┃ p25     ┃ p50     ┃ p75    ┃ p100    ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━╇━━━━━╇━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━┩  │\n│ │ MedInc       │   0 │     0 │    3.877 │    1.903 │  0.4999 │   2.569 │   3.539 │  4.757 │      15 │  ▅█▂   │  │\n│ │ HouseAge     │   0 │     0 │    28.56 │     12.6 │       1 │      18 │      29 │     37 │      52 │ ▂▆███▅ │  │\n│ │ AveRooms     │   0 │     0 │    5.437 │    2.449 │  0.8889 │    4.46 │   5.232 │  6.059 │   141.9 │   █    │  │\n│ │ AveBedrms    │   0 │     0 │    1.098 │   0.4457 │  0.3333 │   1.007 │    1.05 │    1.1 │   25.64 │   █    │  │\n│ │ Population   │   0 │     0 │     1431 │     1146 │       3 │     793 │    1170 │   1729 │   35680 │   █    │  │\n│ │ AveOccup     │   0 │     0 │    3.129 │    12.65 │  0.6923 │   2.432 │    2.82 │  3.282 │    1243 │   █    │  │\n│ │ Latitude     │   0 │     0 │    35.65 │    2.134 │   32.55 │   33.94 │   34.27 │  37.72 │   41.95 │ █▃▁▆▁  │  │\n│ │ Longitude    │   0 │     0 │   -119.6 │    2.005 │  -124.3 │  -121.8 │  -118.5 │   -118 │  -114.3 │ ▁▆▂█▃  │  │\n│ │ target       │   0 │     0 │    2.067 │    1.154 │    0.15 │   1.194 │   1.792 │   2.64 │       5 │ ▄█▆▃▂▂ │  │\n│ └──────────────┴─────┴───────┴──────────┴──────────┴─────────┴─────────┴─────────┴────────┴─────────┴────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\n\n# Built-in pandas function, returns df\ndf_desc = df.describe()\ndf_desc\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\ntarget\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n\n\nmean\n3.870671\n28.639486\n5.429000\n1.096675\n1425.476744\n3.070655\n35.631861\n-119.569704\n2.068558\n\n\nstd\n1.899822\n12.585558\n2.474173\n0.473911\n1132.462122\n10.386050\n2.135952\n2.003532\n1.153956\n\n\nmin\n0.499900\n1.000000\n0.846154\n0.333333\n3.000000\n0.692308\n32.540000\n-124.350000\n0.149990\n\n\n25%\n2.563400\n18.000000\n4.440716\n1.006079\n787.000000\n2.429741\n33.930000\n-121.800000\n1.196000\n\n\n50%\n3.534800\n29.000000\n5.229129\n1.048780\n1166.000000\n2.818116\n34.260000\n-118.490000\n1.797000\n\n\n75%\n4.743250\n37.000000\n6.052381\n1.099526\n1725.000000\n3.282261\n37.710000\n-118.010000\n2.647250\n\n\nmax\n15.000100\n52.000000\n141.909091\n34.066667\n35682.000000\n1243.333333\n41.950000\n-114.310000\n5.000010",
    "crumbs": [
      "Pixel Process",
      "Regression",
      "Housing Regression"
    ]
  },
  {
    "objectID": "machine-learning/regression-housing.html#train-models-notebook",
    "href": "machine-learning/regression-housing.html#train-models-notebook",
    "title": "Housing Regression",
    "section": "",
    "text": "# Instantiate model, fit model, save model\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[21], line 3\n      1 # Instantiate model, fit model, save model\n      2 lr = LinearRegression()\n----&gt; 3 lr.fit(X_train, y_train)\n\nNameError: name 'X_train' is not defined\n\n\n\n\n# Instantiate model, fit model, save model\nrf = RandomForestRegressor()\nrf.fit(X_train, y_train)\n\n\n# Predict on train with both models\n# NOTE: test metrics are more insightful\nlr_train_preds = lr.predict(X_train)\nrf_train_preds = rf.predict(X_train)\n\n\n# Calculate mean squared error for both models\nlr_mse = mean_squared_error(y_train, lr_train_preds)\nrf_mse = mean_squared_error(y_train, rf_train_preds)\n\n\n# Print calculations\nprint(f\"The MSE for the linear regression models is : {lr_mse: .2f}\")\nprint(f\"The MSE for the random forest regression models is : {rf_mse: .2f}\")\n\n\n# Plot both predictions\nplt.figure(figsize=(10,10))\nplt.scatter(y_train, lr_train_preds, c='crimson', label='Linear Regression')\nplt.scatter(y_train, rf_train_preds, c='gold', label='RF Regression')\n\nplt.xlabel('True Values', fontsize=15)\nplt.ylabel('Predictions', fontsize=15)\nplt.title('Training Error', fontsize=15)\n\nplt.legend()\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Pixel Process",
      "Regression",
      "Housing Regression"
    ]
  },
  {
    "objectID": "machine-learning/regression-housing.html#evaluate-models-notebook",
    "href": "machine-learning/regression-housing.html#evaluate-models-notebook",
    "title": "Housing Regression",
    "section": "",
    "text": "# linear regression predict\nlr_preds = lr.predict(X_test)\nlr_preds\n\n\n# random forest regression predict\nrf_preds = rf.predict(X_test)\nrf_preds\n\n\n# Calculate explained variance for both models\nlr_evs = explained_variance_score(y_test, lr_preds)\nrf_evs = explained_variance_score(y_test, rf_preds)\n\n\n# Display explained variance scores\nprint(f'The explained variance score for the linear regression models is: {lr_evs: .2f}')\nprint(f'The explained variance score for the random forest regression models is: {rf_evs: .2f}')\n\n\n# Calculate mean squared error (MSE)\nlr_mse = mean_squared_error(y_test, lr_preds)\nrf_mse = mean_squared_error(y_test, rf_preds)\n\n\n# Display MSE\nprint(f\"The MSE for the linear regression models is : {lr_mse: .2f}\")\nprint(f\"The MSE for the random forest regression models is : {rf_mse: .2f}\")\n\n\n# create y_df with real and predicted values\ny_df=pd.DataFrame({'y_true': y_test, 'lr_preds': lr_preds, 'rf_preds': rf_preds})\n\n\n# Check df\ny_df.head()\n\n\n# Get correlation across real, lr, and rf values\ny_df.corr()\n\n\n# Seaborn pair plot on y data\nsns.pairplot(y_df)\n\n\n# Plot results\nplt.figure(figsize=(10,10))\nplt.scatter(y_test.target, lr_preds, c='crimson', label='Linear Regression')\nplt.scatter(y_test.target, rf_preds, c='gold', label='RF Regression')\n\nplt.xlabel('True Values', fontsize=15)\nplt.ylabel('Predictions', fontsize=15)\nplt.title('Test Error', fontsize=15)\n\nplt.legend()\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Pixel Process",
      "Regression",
      "Housing Regression"
    ]
  },
  {
    "objectID": "machine-learning/faqs.html",
    "href": "machine-learning/faqs.html",
    "title": "Machine Learning: Frequently Asked Questions",
    "section": "",
    "text": "Choosing the right model and metrics make all the difference in ML. If you need guidance for this, you’re in the right place.\n\nToggle All FAQs\n\n\n\n\n\n\n\nWhat is supervised learning?\n\nSupervised learning is a type of machine learning where the model is trained on labeled data — meaning each input has a known output. Examples include classification and regression tasks.\n\n\n\n\nWhat is unsupervised learning?\n\nUnsupervised learning involves training a model on data without labeled outcomes. It is used to find patterns or groupings in the data, such as with clustering or dimensionality reduction.\n\n\n\n\nWhat is classification?\n\nClassification is a supervised learning task where the goal is to assign inputs to one of several predefined categories. Examples include spam detection or emotion recognition.\n\n\n\n\nWhat is regression?\n\nRegression is a supervised learning task where the model predicts a continuous value, such as price, temperature, or probability.\n\n\n\n\nWhat are common evaluation metrics?\n\nEvaluation metrics help assess model performance. For classification: accuracy, precision, recall, and F1 score are common. For regression: MSE (mean squared error) and RMSE (root mean squared error) are often used.\n\n\n\n\nWhat is accuracy?\n\nAccuracy is the proportion of correct predictions over total predictions. It’s useful when class distribution is balanced.\n\n\n\n\nWhat is precision?\n\nPrecision is the proportion of true positives among all predicted positives. It’s useful when false positives are costly.\n\n\n\n\nWhat is recall?\n\nRecall is the proportion of true positives captured out of all actual positives. It’s useful when false negatives are costly.\n\n\n\n\nWhat is F1 score?\n\nThe F1 score is the harmonic mean of precision and recall. It balances both concerns when there’s a trade-off.\n\n\n\n\nWhat is MSE?\n\nMSE (Mean Squared Error) is the average of the squared differences between predicted and actual values. It penalizes larger errors more.\n\n\n\n\nWhat is RMSE?\n\nRMSE (Root Mean Squared Error) is the square root of MSE, making it more interpretable as it’s in the same unit as the target variable.\n\n\n\n\nWhat is a train-test split?\n\nTrain-test splitting divides your dataset into a portion used to train the model and another used to test its performance on unseen data.\n\n\n\n\nWhy do GPUs help with big data or deep learning?\n\nGPUs can perform many calculations in parallel, making them much faster than CPUs for training large models, especially in deep learning.\n\n\n\n\nWhy does model deployment matter?\n\nDeployment makes your model accessible — turning it into a service that users, apps, or systems can interact with. Without deployment, your model can’t provide value in real-world applications.\n\n\n\n\nWhat is a CI/CD pipeline?\n\nCI/CD (Continuous Integration / Continuous Deployment) is a DevOps approach that automates testing, integration, and deployment. It ensures your models and code can be updated and delivered reliably and quickly.",
    "crumbs": [
      "Pixel Process",
      "Machine Learning: Frequently Asked Questions"
    ]
  },
  {
    "objectID": "machine-learning/faqs.html#faqs",
    "href": "machine-learning/faqs.html#faqs",
    "title": "Machine Learning: Frequently Asked Questions",
    "section": "",
    "text": "Choosing the right model and metrics make all the difference in ML. If you need guidance for this, you’re in the right place.\n\nToggle All FAQs\n\n\n\n\n\n\n\nWhat is supervised learning?\n\nSupervised learning is a type of machine learning where the model is trained on labeled data — meaning each input has a known output. Examples include classification and regression tasks.\n\n\n\n\nWhat is unsupervised learning?\n\nUnsupervised learning involves training a model on data without labeled outcomes. It is used to find patterns or groupings in the data, such as with clustering or dimensionality reduction.\n\n\n\n\nWhat is classification?\n\nClassification is a supervised learning task where the goal is to assign inputs to one of several predefined categories. Examples include spam detection or emotion recognition.\n\n\n\n\nWhat is regression?\n\nRegression is a supervised learning task where the model predicts a continuous value, such as price, temperature, or probability.\n\n\n\n\nWhat are common evaluation metrics?\n\nEvaluation metrics help assess model performance. For classification: accuracy, precision, recall, and F1 score are common. For regression: MSE (mean squared error) and RMSE (root mean squared error) are often used.\n\n\n\n\nWhat is accuracy?\n\nAccuracy is the proportion of correct predictions over total predictions. It’s useful when class distribution is balanced.\n\n\n\n\nWhat is precision?\n\nPrecision is the proportion of true positives among all predicted positives. It’s useful when false positives are costly.\n\n\n\n\nWhat is recall?\n\nRecall is the proportion of true positives captured out of all actual positives. It’s useful when false negatives are costly.\n\n\n\n\nWhat is F1 score?\n\nThe F1 score is the harmonic mean of precision and recall. It balances both concerns when there’s a trade-off.\n\n\n\n\nWhat is MSE?\n\nMSE (Mean Squared Error) is the average of the squared differences between predicted and actual values. It penalizes larger errors more.\n\n\n\n\nWhat is RMSE?\n\nRMSE (Root Mean Squared Error) is the square root of MSE, making it more interpretable as it’s in the same unit as the target variable.\n\n\n\n\nWhat is a train-test split?\n\nTrain-test splitting divides your dataset into a portion used to train the model and another used to test its performance on unseen data.\n\n\n\n\nWhy do GPUs help with big data or deep learning?\n\nGPUs can perform many calculations in parallel, making them much faster than CPUs for training large models, especially in deep learning.\n\n\n\n\nWhy does model deployment matter?\n\nDeployment makes your model accessible — turning it into a service that users, apps, or systems can interact with. Without deployment, your model can’t provide value in real-world applications.\n\n\n\n\nWhat is a CI/CD pipeline?\n\nCI/CD (Continuous Integration / Continuous Deployment) is a DevOps approach that automates testing, integration, and deployment. It ensures your models and code can be updated and delivered reliably and quickly.",
    "crumbs": [
      "Pixel Process",
      "Machine Learning: Frequently Asked Questions"
    ]
  },
  {
    "objectID": "machine-learning/ModelEvaluation.html",
    "href": "machine-learning/ModelEvaluation.html",
    "title": "Key Modeling Terms",
    "section": "",
    "text": "The train-test split is a technique for evaluating the performance of a machine learning model. The dataset is divided into two parts: - Training Set: Used to train the model. - Test Set: Used to evaluate the model’s performance on unseen data to ensure it generalizes well.\nEDA and model fit should always be contained to the training data. Data leakage occurs when then test data is used in the modeling. This results in inaccurate estimates of model performance and generalizability.\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nSure! Here is a short description of data leakage and its concerns in developing models:\n\n\n\n\nData leakage occurs when information from outside the training dataset is used to create the model, giving it an unintended advantage. This typically happens when the training data includes information that will not be available when the model is making predictions in a real-world scenario.\n\n\n\nOverestimated Performance: Data leakage can cause the model to show unrealistically high performance during training and validation, as it has access to information it shouldn’t have.\nPoor Generalization: Models affected by data leakage often fail to generalize to new, unseen data because they rely on spurious patterns present only in the training data.\nMisleading Results: Data leakage can lead to misleading conclusions about the model’s effectiveness, which can be costly and time-consuming to correct.\n\n\n\n\nIncluding future information in the training data, such as using data from future time points in a time series forecast, or including a feature that directly leaks the target variable (e.g., including a feature that is derived from the target variable).\n\n\n\n\nCareful Data Splitting: Ensure that data used for training, validation, and testing are properly separated.\nFeature Engineering: Be cautious during feature engineering to avoid incorporating future information or information derived from the target variable.\nCross-Validation: Use appropriate cross-validation techniques that respect the data’s temporal or hierarchical structure to prevent leakage.\n\nBy understanding and preventing data leakage, you can develop more robust models that perform reliably in real-world scenarios.\n\n\n\n\nCross-validation is a technique for assessing how the results of a statistical analysis will generalize to an independent dataset. It involves partitioning the data into subsets, training the model on some subsets and validating it on the remaining subsets. The most common form is k-fold cross-validation.\n\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(model, X, y, cv=5)\n\n\n\n\nMetrics used to evaluate the performance of classification models include:\n\nAccuracy: The ratio of correctly predicted instances to the total instances.\nPrecision: The ratio of correctly predicted positive observations to the total predicted positives.\nRecall (Sensitivity): The ratio of correctly predicted positive observations to all observations in the actual class.\nF1 Score: The harmonic mean of precision and recall.\n\n\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred)\nrecall = recall_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred)\n\n\n\n\nMetrics used to evaluate the performance of regression models include:\n\nMean Absolute Error (MAE): The average of the absolute errors between the predicted and actual values.\nMean Squared Error (MSE): The average of the squared errors between the predicted and actual values.\nRoot Mean Squared Error (RMSE): The square root of the mean squared error.\nR-squared (R²): The proportion of the variance in the dependent variable that is predictable from the independent variables.\n\n\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nmae = mean_absolute_error(y_true, y_pred)\nmse = mean_squared_error(y_true, y_pred)\nrmse = mean_squared_error(y_true, y_pred, squared=False)\nr2 = r2_score(y_true, y_pred)\n\n\n\n\nA confusion matrix is a table used to describe the performance of a classification model. It shows the true positive, false positive, true negative, and false negative counts.\n\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_true, y_pred)\n\n\n\n\nOverfitting occurs when a model learns the training data too well, capturing noise and details that do not generalize to new data. An overfitted model performs well on training data but poorly on test data.\n\n\n\nHigh accuracy on training data.\nLow accuracy on test data.\n\n\n\n\n\nUnderfitting occurs when a model is too simple to capture the underlying patterns in the data. An underfitted model performs poorly on both training and test data.\n\n\n\nLow accuracy on training data.\nLow accuracy on test data."
  },
  {
    "objectID": "machine-learning/ModelEvaluation.html#train-test-split",
    "href": "machine-learning/ModelEvaluation.html#train-test-split",
    "title": "Key Modeling Terms",
    "section": "",
    "text": "The train-test split is a technique for evaluating the performance of a machine learning model. The dataset is divided into two parts: - Training Set: Used to train the model. - Test Set: Used to evaluate the model’s performance on unseen data to ensure it generalizes well.\nEDA and model fit should always be contained to the training data. Data leakage occurs when then test data is used in the modeling. This results in inaccurate estimates of model performance and generalizability.\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nSure! Here is a short description of data leakage and its concerns in developing models:"
  },
  {
    "objectID": "machine-learning/ModelEvaluation.html#data-leakage",
    "href": "machine-learning/ModelEvaluation.html#data-leakage",
    "title": "Key Modeling Terms",
    "section": "",
    "text": "Data leakage occurs when information from outside the training dataset is used to create the model, giving it an unintended advantage. This typically happens when the training data includes information that will not be available when the model is making predictions in a real-world scenario.\n\n\n\nOverestimated Performance: Data leakage can cause the model to show unrealistically high performance during training and validation, as it has access to information it shouldn’t have.\nPoor Generalization: Models affected by data leakage often fail to generalize to new, unseen data because they rely on spurious patterns present only in the training data.\nMisleading Results: Data leakage can lead to misleading conclusions about the model’s effectiveness, which can be costly and time-consuming to correct.\n\n\n\n\nIncluding future information in the training data, such as using data from future time points in a time series forecast, or including a feature that directly leaks the target variable (e.g., including a feature that is derived from the target variable).\n\n\n\n\nCareful Data Splitting: Ensure that data used for training, validation, and testing are properly separated.\nFeature Engineering: Be cautious during feature engineering to avoid incorporating future information or information derived from the target variable.\nCross-Validation: Use appropriate cross-validation techniques that respect the data’s temporal or hierarchical structure to prevent leakage.\n\nBy understanding and preventing data leakage, you can develop more robust models that perform reliably in real-world scenarios."
  },
  {
    "objectID": "machine-learning/ModelEvaluation.html#cross-validation",
    "href": "machine-learning/ModelEvaluation.html#cross-validation",
    "title": "Key Modeling Terms",
    "section": "",
    "text": "Cross-validation is a technique for assessing how the results of a statistical analysis will generalize to an independent dataset. It involves partitioning the data into subsets, training the model on some subsets and validating it on the remaining subsets. The most common form is k-fold cross-validation.\n\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(model, X, y, cv=5)"
  },
  {
    "objectID": "machine-learning/ModelEvaluation.html#metrics-for-classification",
    "href": "machine-learning/ModelEvaluation.html#metrics-for-classification",
    "title": "Key Modeling Terms",
    "section": "",
    "text": "Metrics used to evaluate the performance of classification models include:\n\nAccuracy: The ratio of correctly predicted instances to the total instances.\nPrecision: The ratio of correctly predicted positive observations to the total predicted positives.\nRecall (Sensitivity): The ratio of correctly predicted positive observations to all observations in the actual class.\nF1 Score: The harmonic mean of precision and recall.\n\n\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred)\nrecall = recall_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred)"
  },
  {
    "objectID": "machine-learning/ModelEvaluation.html#metrics-for-regression",
    "href": "machine-learning/ModelEvaluation.html#metrics-for-regression",
    "title": "Key Modeling Terms",
    "section": "",
    "text": "Metrics used to evaluate the performance of regression models include:\n\nMean Absolute Error (MAE): The average of the absolute errors between the predicted and actual values.\nMean Squared Error (MSE): The average of the squared errors between the predicted and actual values.\nRoot Mean Squared Error (RMSE): The square root of the mean squared error.\nR-squared (R²): The proportion of the variance in the dependent variable that is predictable from the independent variables.\n\n\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nmae = mean_absolute_error(y_true, y_pred)\nmse = mean_squared_error(y_true, y_pred)\nrmse = mean_squared_error(y_true, y_pred, squared=False)\nr2 = r2_score(y_true, y_pred)"
  },
  {
    "objectID": "machine-learning/ModelEvaluation.html#confusion-matrix",
    "href": "machine-learning/ModelEvaluation.html#confusion-matrix",
    "title": "Key Modeling Terms",
    "section": "",
    "text": "A confusion matrix is a table used to describe the performance of a classification model. It shows the true positive, false positive, true negative, and false negative counts.\n\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_true, y_pred)"
  },
  {
    "objectID": "machine-learning/ModelEvaluation.html#overfitting",
    "href": "machine-learning/ModelEvaluation.html#overfitting",
    "title": "Key Modeling Terms",
    "section": "",
    "text": "Overfitting occurs when a model learns the training data too well, capturing noise and details that do not generalize to new data. An overfitted model performs well on training data but poorly on test data.\n\n\n\nHigh accuracy on training data.\nLow accuracy on test data."
  },
  {
    "objectID": "machine-learning/ModelEvaluation.html#underfitting",
    "href": "machine-learning/ModelEvaluation.html#underfitting",
    "title": "Key Modeling Terms",
    "section": "",
    "text": "Underfitting occurs when a model is too simple to capture the underlying patterns in the data. An underfitted model performs poorly on both training and test data.\n\n\n\nLow accuracy on training data.\nLow accuracy on test data."
  },
  {
    "objectID": "machine-learning/random-forest.html",
    "href": "machine-learning/random-forest.html",
    "title": "Random Forests",
    "section": "",
    "text": "This notebook will use the Iris flower dataset from sklearn to introduce classification with Random Forest.\nFeature importance and partial dependency plots will be created once the model is trained.",
    "crumbs": [
      "Pixel Process",
      "Models & Metrics",
      "Random Forests"
    ]
  },
  {
    "objectID": "machine-learning/random-forest.html#prerequisites",
    "href": "machine-learning/random-forest.html#prerequisites",
    "title": "Random Forests",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nPython imports\nTrain-test split\nClassification metrics\nDecision Trees\nMeasures of node impurity (Shannon Entropy and Gini Index)",
    "crumbs": [
      "Pixel Process",
      "Models & Metrics",
      "Random Forests"
    ]
  },
  {
    "objectID": "machine-learning/random-forest.html#train-test-split",
    "href": "machine-learning/random-forest.html#train-test-split",
    "title": "Random Forests",
    "section": "Train-Test Split",
    "text": "Train-Test Split\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    df[data.feature_names], \n    df['label'], \n    random_state=42,\n    stratify=df['label']\n)\n\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n\n(112, 4)\n(38, 4)\n(112,)\n(38,)",
    "crumbs": [
      "Pixel Process",
      "Models & Metrics",
      "Random Forests"
    ]
  },
  {
    "objectID": "workflow/scripts-and-notebooks.html",
    "href": "workflow/scripts-and-notebooks.html",
    "title": "Scripts and Notebooks",
    "section": "",
    "text": "Code can be developed and executed in many different ways. Two primary data science options are scripts and notebooks. The right choicde depends on the task at hand and the stage of your workflow. Here are some guidelines on making the most of both.",
    "crumbs": [
      "Pixel Process",
      "Tools",
      "Scripts and Notebooks"
    ]
  },
  {
    "objectID": "workflow/scripts-and-notebooks.html#scripts",
    "href": "workflow/scripts-and-notebooks.html#scripts",
    "title": "Scripts and Notebooks",
    "section": "Scripts",
    "text": "Scripts\n\nProduction Code: more streamlined coding approach for when dependency, versioning, and integration are crucuial\nAutomation and Reproducibility: great for automated processing when no manual intervention is needed\nPerformance: reduced overhead as they avoid the interactive GUI of notebooks",
    "crumbs": [
      "Pixel Process",
      "Tools",
      "Scripts and Notebooks"
    ]
  },
  {
    "objectID": "workflow/scripts-and-notebooks.html#notebooks",
    "href": "workflow/scripts-and-notebooks.html#notebooks",
    "title": "Scripts and Notebooks",
    "section": "Notebooks",
    "text": "Notebooks\n\nPrototyping Code: ideal for rapid iteration and development work with immediate feedback on a cell-by-cell basis\nVisualization: easily test and tweak data viz to fine-tune processes\nTeaching & Tutorials: mixing code, visualizations, and markdown creates detailed explanations to help others (or future you) understand the thought process and logic behind the code",
    "crumbs": [
      "Pixel Process",
      "Tools",
      "Scripts and Notebooks"
    ]
  },
  {
    "objectID": "workflow/scripts-and-notebooks.html#best-of-both",
    "href": "workflow/scripts-and-notebooks.html#best-of-both",
    "title": "Scripts and Notebooks",
    "section": "Best of Both",
    "text": "Best of Both\nBy leveraging the strengths of both scripts and notebooks, you can create a workflow that is both efficient and easy to maintain, while also being adaptable to different stages of your projects.\n\nMachine Learning Workflow\n\nNotebook: Initial data exploration, feature engineering, model prototyping, and visualizations\nScript: Data preprocessing pipeline, model training, evaluation, and deployment\n\n\n\nData Analysis Project\n\nNotebook: EDA, hypothesis testing, and generating visualizations for reports\nScript: Data extraction, transformation, and loading (ETL) processes, as well as scheduled reports generation\n\n\n\nTutorial Development\n\nNotebook: Creating interactive tutorials with step-by-step explanations and code snippets\nScript: Including complex functions or utility scripts to keep the notebook concise and focused on the teaching content",
    "crumbs": [
      "Pixel Process",
      "Tools",
      "Scripts and Notebooks"
    ]
  },
  {
    "objectID": "workflow/package-guide.html",
    "href": "workflow/package-guide.html",
    "title": "Python Packages",
    "section": "",
    "text": "ReusabilityMaintainabilityCollaborationDistributionEncapsulation\n\n\n\n\nPackages allow developers to encapsulate functionality in reusable modules.\nOnce packaged, code can be imported and used across multiple projects without rewriting.\nExample: A utility package for data manipulation can be reused in various data science projects.\n\n\n\n\n\n\nBy organizing code into packages, it’s easier to manage, update, and fix bugs.\nModifications in a well-structured package can be reflected in all projects that depend on it.\nSeparate components of a large system can be independently maintained within packages.\n\n\n\n\n\n\nPackages make it easier for teams to work together, as code is modular and shareable.\nVersioning ensures collaborators use consistent versions, minimizing conflicts.\nA package hosted on platforms like GitHub or PyPI enables wider access and contribution from others.\n\n\n\n\n\n\nPackaging allows easy distribution of software to users.\nPublic repositories like PyPI enable anyone to install the package with a simple command (pip install).\nThis accelerates adoption and makes libraries and tools available to the entire Python community.\n\n\n\n\n\n\nPackages encapsulate specific functionalities and keep project codebases clean and modular.\nEncapsulation helps with code separation and prevents functions from different areas from conflicting with each other."
  },
  {
    "objectID": "workflow/package-guide.html#why-packaging-matters",
    "href": "workflow/package-guide.html#why-packaging-matters",
    "title": "Python Packages",
    "section": "",
    "text": "ReusabilityMaintainabilityCollaborationDistributionEncapsulation\n\n\n\n\nPackages allow developers to encapsulate functionality in reusable modules.\nOnce packaged, code can be imported and used across multiple projects without rewriting.\nExample: A utility package for data manipulation can be reused in various data science projects.\n\n\n\n\n\n\nBy organizing code into packages, it’s easier to manage, update, and fix bugs.\nModifications in a well-structured package can be reflected in all projects that depend on it.\nSeparate components of a large system can be independently maintained within packages.\n\n\n\n\n\n\nPackages make it easier for teams to work together, as code is modular and shareable.\nVersioning ensures collaborators use consistent versions, minimizing conflicts.\nA package hosted on platforms like GitHub or PyPI enables wider access and contribution from others.\n\n\n\n\n\n\nPackaging allows easy distribution of software to users.\nPublic repositories like PyPI enable anyone to install the package with a simple command (pip install).\nThis accelerates adoption and makes libraries and tools available to the entire Python community.\n\n\n\n\n\n\nPackages encapsulate specific functionalities and keep project codebases clean and modular.\nEncapsulation helps with code separation and prevents functions from different areas from conflicting with each other."
  },
  {
    "objectID": "workflow/cli-intro.html",
    "href": "workflow/cli-intro.html",
    "title": "The Command Line",
    "section": "",
    "text": "A Command-Line Interface (CLI) is a text-based way to interact with your computer. Instead of clicking through menus, you type commands into a terminal to navigate directories, manage files, run programs, and more. While it may seem intimidating at first, the CLI is powerful for automation, customization, and efficiency.\n\n\n\nEfficiency: Perform tasks faster than through graphical interfaces.\nAutomation: Combine commands into scripts for repeatable workflows.\nControl: Access deeper system functionality and options.\nRemote Work: SSH into servers and manage systems without a GUI.\n\nCommon CLIs include bash, zsh, and PowerShell. On macOS and Linux, bash or zsh are standard; on Windows, PowerShell is the default.",
    "crumbs": [
      "Pixel Process",
      "Customization",
      "The Command Line"
    ]
  },
  {
    "objectID": "workflow/cli-intro.html#command-line-interfaces-cli",
    "href": "workflow/cli-intro.html#command-line-interfaces-cli",
    "title": "The Command Line",
    "section": "",
    "text": "A Command-Line Interface (CLI) is a text-based way to interact with your computer. Instead of clicking through menus, you type commands into a terminal to navigate directories, manage files, run programs, and more. While it may seem intimidating at first, the CLI is powerful for automation, customization, and efficiency.\n\n\n\nEfficiency: Perform tasks faster than through graphical interfaces.\nAutomation: Combine commands into scripts for repeatable workflows.\nControl: Access deeper system functionality and options.\nRemote Work: SSH into servers and manage systems without a GUI.\n\nCommon CLIs include bash, zsh, and PowerShell. On macOS and Linux, bash or zsh are standard; on Windows, PowerShell is the default.",
    "crumbs": [
      "Pixel Process",
      "Customization",
      "The Command Line"
    ]
  },
  {
    "objectID": "workflow/cli-intro.html#the-shell",
    "href": "workflow/cli-intro.html#the-shell",
    "title": "The Command Line",
    "section": "The Shell",
    "text": "The Shell\nThe shell is the program that interprets commands you type and executes them.\nPopular shells:\n\nBash: Common on Linux\nZsh: Default on macOS\nPowerShell: Default on Windows\n\nThe shell supports built-in commands and lets you run programs, pipe outputs, and customize your environment.",
    "crumbs": [
      "Pixel Process",
      "Customization",
      "The Command Line"
    ]
  },
  {
    "objectID": "workflow/cli-intro.html#aliases",
    "href": "workflow/cli-intro.html#aliases",
    "title": "The Command Line",
    "section": "Aliases",
    "text": "Aliases\nAn alias is a shortcut for commands.\nOptimize your workflow by:\n\nShortening commands\nReduce errors from mistyping\nCustomize common CLI commands\n\n\nBasicsPersistent AliasesAdvanced\n\n\n\n\nWhat is an alias?\n\nAn alias is a shortcut for commands.\nalias ll='ls -lah'\n\n\n\nWhy use aliases?\n\n\nSave time by shortening commands\nReduce errors from mistyping\nCustomize your CLI workflow\n\n\n\n\n\n\nMake it permanent\n\nAdd to your shell configuration:\necho \"alias ll='ls -lah'\" &gt;&gt; ~/.zshrc && source ~/.zshrc\n\n\n\n\n\nMultiple Aliases\n\nalias gs='git status'\nalias gp='git pull'\nalias gpush='git push origin main'",
    "crumbs": [
      "Pixel Process",
      "Customization",
      "The Command Line"
    ]
  },
  {
    "objectID": "getting-started/index.html",
    "href": "getting-started/index.html",
    "title": "Getting Started",
    "section": "",
    "text": "This section focuses on core practices and packages in Python. Basic programming content from Is learning Python Worth it? to pro-tips for debugging.\nLearn the basics to see what’s possible.",
    "crumbs": [
      "Pixel Process",
      "Getting Started"
    ]
  },
  {
    "objectID": "getting-started/index.html#getting-started-topics",
    "href": "getting-started/index.html#getting-started-topics",
    "title": "Getting Started",
    "section": "Getting Started Topics",
    "text": "Getting Started Topics\n\nBrand new to programming? Jump into Python hassle free.\nBrand new to pgBuilt to explore, all notebooks in this section are interactive. Run live examples and challenges in Binder and Thebe notebooks — no setup required.\n\nPython Basics: Learn about variables, data types, and how to store and use information.\nOperators: Explore arithmetic, comparison, and logical operators to control how your code behaves.\nFlow Control: Use if-statements, loops, and logical patterns to direct your programs.\n\n\n\n\nBasic Overview\n\n\n\n FAQs → FAQs when getting started with programming\n\n\n Troubleshooting Basics → A guide to problem-solving and debugging\n\n\n How to: Jupyter Notebooks → Intro to using and navigating notebooks\n\n\n\n\n\nJump In\n\n\n\n Jump In → No downloads, no setup-jump into basic Python\n\n\n Datatypes and Operators → Jump into Python - Part 1\n\n\n Variables and Functions → Jump into Python - Part 2\n\n\n Errors and Experimentation → Jump into Python - Part 3\n\n\n\n\n\nR Introduction\n\n\n\n R Intro PDF → PDF companion to the R intro notebook series\n\n\n R: Basic Commands (NB) → Get started with R\n\n\n R: Datasets (NB) → Vectors and datasets in R\n\n\n R: Data Basics (NB) → Create, edit, and summarize vectors in R\n\n\n R: Import/Export (NB) → Loading and saving data with R\n\n\n R: Data Vizualization Basics (NB) → Basic plotting functions in R\n\n\n R: Data Analysis (NB) → Begin analyzing with correlations, ttests, and more\n\n\n\n\n\n\n\n\n\n\n\nExperience from Errors\n\n\n\nThese bugs don’t bite! Writing a complex program without errors is as probable as writing a novel without a typo. The key is test for errors, ensure things work properly, and address the issues. Understanding basic errors will allow you tackle much more complex tasks.\nCode Freely — programming is interactive, so try things, test things, experiment\nFail Safely — mistakes don’t hurt here, if something goes wrong simply reset the workspace\nLearn Deeply — examine inputs and outputs, read error messages, learn the basics to programming\n\n\n\nThe best way to learn is to break stuff — and then (try to) fix it\n\nStart small. Make mistakes. Ask questions. This is a space to experiment freely, play around, break things, and learn by doing. Nobody gets it right the first time. Just get started.",
    "crumbs": [
      "Pixel Process",
      "Getting Started"
    ]
  },
  {
    "objectID": "getting-started/r/basic-commands.html",
    "href": "getting-started/r/basic-commands.html",
    "title": "R: Basic Commands",
    "section": "",
    "text": "Operators are characters with an assigned meaning in R.\nMathematical 0perators are some of the most basic and will be explored first.\n\nAddition\nSubtraction\nMultiplication\nDivision\nExponentiation\n\n\n2 + 1\n\n3\n\n\n\n2 - 1 \n\n1\n\n\n\n2 * 1 \n\n2\n\n\n\n2 / 1 \n\n2\n\n\n\n2 ** 2\n\n4\n\n\n\n\nMultiple commands can be executed in a single cell\n\n1 + 1\n2 - 1\n2 / 2\n3 * 3\n4 ** 4\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n256\n\n\n\n\n\nVariable assignment is essential in R, as in many programming languages.\nIt can be helpful to represent objects, data, functions, and more as variables.\nThese examples show basic usage.\n\na = 1\nb = 2\nc = 3\nd = 4\n\nNo output is generated by assigning a variable.\nHowever, they are now stored in memory and can be referenced.\n\n\n\na + a\nb - a\nb / b\nc * c\nd ** d\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n256\n\n\nResults are identical to the above which just used numbers.\n\n\n\nVariables are mutable, that is they can be updated/changed.\nWe’ll also start using print, a built-in function which displays the given input. Helpful to see what is happening, since assigning variables does not produce output in the notebook.\n\nprint(\"Checking value of a...\")\nprint(a)\nprint(\"Updating value of a...\")\na = a + 1\nprint(\"Updated value of a...\")\nprint(a)\n\n[1] \"Checking value of a...\"\n[1] 1\n[1] \"Updating value of a...\"\n[1] \"Updated value of a...\"\n[1] 2\n\n\n\n\n\n\nR uses vectors to store larger amounts of data.\n\n\nc, another built-in function, stands for combine and can be used to create vectors.\n\nvector_1 = c(1, 2, 3, 4)\nprint(vector_1)\n\n[1] 1 2 3 4\n\n\n\n\n\nR makes it easy to do operations on vectors of data.\nR has built-in functions for these types of operations.\nWe’ll explore: - Sum - Mean - Min - Max\n\nsum(vector_1)\nmean(vector_1)\nmin(vector_1)\nmax(vector_1)\n\n10\n\n\n2.5\n\n\n1\n\n\n4\n\n\n\n\n\n\nvector_2 = c(a, b, c, d)\nprint(vector_2)\n\n[1] 2 2 3 4\n\n\nIt looks like vector_1 and vector_2 do not match because a was modified above.\nWe will use the setequal function to compare them. This function takes two arguments and will return TRUE if they are equal, else FALSE.\n\n\n\n\nsetequal(vector_1, vector_2)\n\nFALSE\n\n\nLet’s alter vector_2 so that they match.\nThis step uses indexing to get the desired value from the vector and then assigns it a value of 1.\nRemember vectors are mutable.\n\n\n\n\nvector_2[1]=1\nprint(vector_2)\n\n[1] 1 2 3 4\n\n\nWe can see in the output above that the first item in the vector was set to 1.\nTo compare two vectors in R, we again use setequal.\n\n\n\n\nsetequal(vector_1, vector_2)\n\nTRUE\n\n\nHere we can see that the two vectors are equal.\nR can perform vector math as well.\nHere we create a third vector by adding vector_1 and vector_2.\n\n\n\n\nvector_3 = vector_1 + vector_2\n\nNotice, there is no output! This is because we assigned the result into a new variable.\nLet’s print out vector_3 and see the result.\n\nprint(vector_3)\n\n[1] 2 4 6 8\n\n\n\n\n\n\nvector_1 - vector_2\nvector_1 * vector_2\nvector_1 / vector_2\nvector_1 ** vector_2\n\n\n0000\n\n\n\n14916\n\n\n\n1111\n\n\n\n1427256\n\n\n\n\n\n\n\n\nVectors can be combined to create longer vectors using the c command.\n\nvector_4 = c(vector_1, vector_2, vector_3)\nprint(vector_4)\n\n [1] 1 2 3 4 1 2 3 4 2 4 6 8\n\n\n\n\n\nMultiple vectors can be combined into data frames.\nUse the command rbind to combine vectors as rows.\n\ndf1 = rbind(vector_1, vector_2, vector_3)\nprint(df1)\n\n         [,1] [,2] [,3] [,4]\nvector_1    1    2    3    4\nvector_2    1    2    3    4\nvector_3    2    4    6    8\n\n\nUse the command cbind to combine vectors as columns.\n\ndf2 = cbind(vector_1, vector_2, vector_3)\nprint(df2)\n\n     vector_1 vector_2 vector_3\n[1,]        1        1        2\n[2,]        2        2        4\n[3,]        3        3        6\n[4,]        4        4        8\n\n\n\n\n\nNow that the data is in a multidimensional array, colnames and rownames may be assigned.\nWe can access those values using colnames() and rownames().\n\ncolnames(df2)\nrownames(df2)\n\n\n'vector_1''vector_2''vector_3'\n\n\nNULL\n\n\nNotice, NULL is returned as the row names. This is because they have not been set yet.\nTo get R to automatically assign row names add the argument do.NULL=FALSE.\nTo learn more about this argument you can run ?rownames.\n\ncolnames(df2)\nrownames(df2, do.NULL=FALSE)\n\n\n'vector_1''vector_2''vector_3'\n\n\n\n'row1''row2''row3''row4'\n\n\nIt can often be helpful to supply row and column names when creating data frames.\nThis next chunk shows how to do just that!\n\ncolnames(df2) = c(\"Col1\", \"Col2\", \"Col3\")\nrownames(df2) = c(\"Row1\", \"Row2\", \"Row3\", \"Row4\")\n\nNotice, no output is produced. Lets try printing the data frame to see if our changes worked.\n\nprint(df2)\ncolnames(df2)\nrownames(df2)\n\n     Col1 Col2 Col3\nRow1    1    1    2\nRow2    2    2    4\nRow3    3    3    6\nRow4    4    4    8\n\n\n\n'Col1''Col2''Col3'\n\n\n\n'Row1''Row2''Row3''Row4'\n\n\nThis concludes the introduction to basic commands in R!"
  },
  {
    "objectID": "getting-started/r/basic-commands.html#variable-assignment",
    "href": "getting-started/r/basic-commands.html#variable-assignment",
    "title": "R: Basic Commands",
    "section": "",
    "text": "Variable assignment is essential in R, as in many programming languages.\nIt can be helpful to represent objects, data, functions, and more as variables.\nThese examples show basic usage.\n\na = 1\nb = 2\nc = 3\nd = 4\n\nNo output is generated by assigning a variable.\nHowever, they are now stored in memory and can be referenced.\n\n\n\na + a\nb - a\nb / b\nc * c\nd ** d\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n256\n\n\nResults are identical to the above which just used numbers.\n\n\n\nVariables are mutable, that is they can be updated/changed.\nWe’ll also start using print, a built-in function which displays the given input. Helpful to see what is happening, since assigning variables does not produce output in the notebook.\n\nprint(\"Checking value of a...\")\nprint(a)\nprint(\"Updating value of a...\")\na = a + 1\nprint(\"Updated value of a...\")\nprint(a)\n\n[1] \"Checking value of a...\"\n[1] 1\n[1] \"Updating value of a...\"\n[1] \"Updated value of a...\"\n[1] 2"
  },
  {
    "objectID": "getting-started/r/basic-commands.html#vectors",
    "href": "getting-started/r/basic-commands.html#vectors",
    "title": "R: Basic Commands",
    "section": "",
    "text": "R uses vectors to store larger amounts of data.\n\n\nc, another built-in function, stands for combine and can be used to create vectors.\n\nvector_1 = c(1, 2, 3, 4)\nprint(vector_1)\n\n[1] 1 2 3 4\n\n\n\n\n\nR makes it easy to do operations on vectors of data.\nR has built-in functions for these types of operations.\nWe’ll explore: - Sum - Mean - Min - Max\n\nsum(vector_1)\nmean(vector_1)\nmin(vector_1)\nmax(vector_1)\n\n10\n\n\n2.5\n\n\n1\n\n\n4\n\n\n\n\n\n\nvector_2 = c(a, b, c, d)\nprint(vector_2)\n\n[1] 2 2 3 4\n\n\nIt looks like vector_1 and vector_2 do not match because a was modified above.\nWe will use the setequal function to compare them. This function takes two arguments and will return TRUE if they are equal, else FALSE.\n\n\n\n\nsetequal(vector_1, vector_2)\n\nFALSE\n\n\nLet’s alter vector_2 so that they match.\nThis step uses indexing to get the desired value from the vector and then assigns it a value of 1.\nRemember vectors are mutable.\n\n\n\n\nvector_2[1]=1\nprint(vector_2)\n\n[1] 1 2 3 4\n\n\nWe can see in the output above that the first item in the vector was set to 1.\nTo compare two vectors in R, we again use setequal.\n\n\n\n\nsetequal(vector_1, vector_2)\n\nTRUE\n\n\nHere we can see that the two vectors are equal.\nR can perform vector math as well.\nHere we create a third vector by adding vector_1 and vector_2.\n\n\n\n\nvector_3 = vector_1 + vector_2\n\nNotice, there is no output! This is because we assigned the result into a new variable.\nLet’s print out vector_3 and see the result.\n\nprint(vector_3)\n\n[1] 2 4 6 8\n\n\n\n\n\n\nvector_1 - vector_2\nvector_1 * vector_2\nvector_1 / vector_2\nvector_1 ** vector_2\n\n\n0000\n\n\n\n14916\n\n\n\n1111\n\n\n\n1427256"
  },
  {
    "objectID": "getting-started/r/basic-commands.html#manipulating-multiple-vectors",
    "href": "getting-started/r/basic-commands.html#manipulating-multiple-vectors",
    "title": "R: Basic Commands",
    "section": "",
    "text": "Vectors can be combined to create longer vectors using the c command.\n\nvector_4 = c(vector_1, vector_2, vector_3)\nprint(vector_4)\n\n [1] 1 2 3 4 1 2 3 4 2 4 6 8\n\n\n\n\n\nMultiple vectors can be combined into data frames.\nUse the command rbind to combine vectors as rows.\n\ndf1 = rbind(vector_1, vector_2, vector_3)\nprint(df1)\n\n         [,1] [,2] [,3] [,4]\nvector_1    1    2    3    4\nvector_2    1    2    3    4\nvector_3    2    4    6    8\n\n\nUse the command cbind to combine vectors as columns.\n\ndf2 = cbind(vector_1, vector_2, vector_3)\nprint(df2)\n\n     vector_1 vector_2 vector_3\n[1,]        1        1        2\n[2,]        2        2        4\n[3,]        3        3        6\n[4,]        4        4        8\n\n\n\n\n\nNow that the data is in a multidimensional array, colnames and rownames may be assigned.\nWe can access those values using colnames() and rownames().\n\ncolnames(df2)\nrownames(df2)\n\n\n'vector_1''vector_2''vector_3'\n\n\nNULL\n\n\nNotice, NULL is returned as the row names. This is because they have not been set yet.\nTo get R to automatically assign row names add the argument do.NULL=FALSE.\nTo learn more about this argument you can run ?rownames.\n\ncolnames(df2)\nrownames(df2, do.NULL=FALSE)\n\n\n'vector_1''vector_2''vector_3'\n\n\n\n'row1''row2''row3''row4'\n\n\nIt can often be helpful to supply row and column names when creating data frames.\nThis next chunk shows how to do just that!\n\ncolnames(df2) = c(\"Col1\", \"Col2\", \"Col3\")\nrownames(df2) = c(\"Row1\", \"Row2\", \"Row3\", \"Row4\")\n\nNotice, no output is produced. Lets try printing the data frame to see if our changes worked.\n\nprint(df2)\ncolnames(df2)\nrownames(df2)\n\n     Col1 Col2 Col3\nRow1    1    1    2\nRow2    2    2    4\nRow3    3    3    6\nRow4    4    4    8\n\n\n\n'Col1''Col2''Col3'\n\n\n\n'Row1''Row2''Row3''Row4'\n\n\nThis concludes the introduction to basic commands in R!"
  },
  {
    "objectID": "getting-started/r/data-manipulation.html",
    "href": "getting-started/r/data-manipulation.html",
    "title": "Data_Manipulation",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\nRunning this in a new project or on your own computer for the fist time you would need to run install.packages(\"tidyverse\"). Thereafter you only need to turn on the package.\n\ninstall.packages(\"tidyverse\")\n\n\nThe downloaded binary packages are in\n    /var/folders/dz/k8c4rxzs27v3q7ly5w31c95m0000gn/T//Rtmpu46fQx/downloaded_packages\n\n\n\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n\n✔ purrr     1.1.0     \n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n\n✖ dplyr::filter() masks stats::filter()\n\n✖ dplyr::lag()    masks stats::lag()\n\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "getting-started/r/data-manipulation.html#r-markdown",
    "href": "getting-started/r/data-manipulation.html#r-markdown",
    "title": "Data_Manipulation",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\nRunning this in a new project or on your own computer for the fist time you would need to run install.packages(\"tidyverse\"). Thereafter you only need to turn on the package.\n\ninstall.packages(\"tidyverse\")\n\n\nThe downloaded binary packages are in\n    /var/folders/dz/k8c4rxzs27v3q7ly5w31c95m0000gn/T//Rtmpu46fQx/downloaded_packages\n\n\n\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n\n✔ purrr     1.1.0     \n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n\n✖ dplyr::filter() masks stats::filter()\n\n✖ dplyr::lag()    masks stats::lag()\n\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "getting-started/r/data-manipulation.html#iris-dataset",
    "href": "getting-started/r/data-manipulation.html#iris-dataset",
    "title": "Data_Manipulation",
    "section": "Iris Dataset",
    "text": "Iris Dataset\nYou will read in the pbc data set from the survival package. To find more information on the pbc data set you can use ?pbc or use the help tab in the right hand side and search pbc. This gives more information on how the data was created and what is in each column.\n\niris\n\n\nA data.frame: 150 × 5\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.1\nsetosa\n\n\n5.4\n3.7\n1.5\n0.2\nsetosa\n\n\n4.8\n3.4\n1.6\n0.2\nsetosa\n\n\n4.8\n3.0\n1.4\n0.1\nsetosa\n\n\n4.3\n3.0\n1.1\n0.1\nsetosa\n\n\n5.8\n4.0\n1.2\n0.2\nsetosa\n\n\n5.7\n4.4\n1.5\n0.4\nsetosa\n\n\n5.4\n3.9\n1.3\n0.4\nsetosa\n\n\n5.1\n3.5\n1.4\n0.3\nsetosa\n\n\n5.7\n3.8\n1.7\n0.3\nsetosa\n\n\n5.1\n3.8\n1.5\n0.3\nsetosa\n\n\n5.4\n3.4\n1.7\n0.2\nsetosa\n\n\n5.1\n3.7\n1.5\n0.4\nsetosa\n\n\n4.6\n3.6\n1.0\n0.2\nsetosa\n\n\n5.1\n3.3\n1.7\n0.5\nsetosa\n\n\n4.8\n3.4\n1.9\n0.2\nsetosa\n\n\n5.0\n3.0\n1.6\n0.2\nsetosa\n\n\n5.0\n3.4\n1.6\n0.4\nsetosa\n\n\n5.2\n3.5\n1.5\n0.2\nsetosa\n\n\n5.2\n3.4\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.6\n0.2\nsetosa\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n6.9\n3.2\n5.7\n2.3\nvirginica\n\n\n5.6\n2.8\n4.9\n2.0\nvirginica\n\n\n7.7\n2.8\n6.7\n2.0\nvirginica\n\n\n6.3\n2.7\n4.9\n1.8\nvirginica\n\n\n6.7\n3.3\n5.7\n2.1\nvirginica\n\n\n7.2\n3.2\n6.0\n1.8\nvirginica\n\n\n6.2\n2.8\n4.8\n1.8\nvirginica\n\n\n6.1\n3.0\n4.9\n1.8\nvirginica\n\n\n6.4\n2.8\n5.6\n2.1\nvirginica\n\n\n7.2\n3.0\n5.8\n1.6\nvirginica\n\n\n7.4\n2.8\n6.1\n1.9\nvirginica\n\n\n7.9\n3.8\n6.4\n2.0\nvirginica\n\n\n6.4\n2.8\n5.6\n2.2\nvirginica\n\n\n6.3\n2.8\n5.1\n1.5\nvirginica\n\n\n6.1\n2.6\n5.6\n1.4\nvirginica\n\n\n7.7\n3.0\n6.1\n2.3\nvirginica\n\n\n6.3\n3.4\n5.6\n2.4\nvirginica\n\n\n6.4\n3.1\n5.5\n1.8\nvirginica\n\n\n6.0\n3.0\n4.8\n1.8\nvirginica\n\n\n6.9\n3.1\n5.4\n2.1\nvirginica\n\n\n6.7\n3.1\n5.6\n2.4\nvirginica\n\n\n6.9\n3.1\n5.1\n2.3\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n6.8\n3.2\n5.9\n2.3\nvirginica\n\n\n6.7\n3.3\n5.7\n2.5\nvirginica\n\n\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n\nYou can sort your data using the arrange function. This will sort the data based on age in ascending order with the youngest patient being the first of the dataset.\n\niris_arrange &lt;- arrange(iris, Sepal.Length) \niris_arrange\n\n\nA data.frame: 150 × 5\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n\n\n\n\n4.3\n3.0\n1.1\n0.1\nsetosa\n\n\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n4.4\n3.0\n1.3\n0.2\nsetosa\n\n\n4.4\n3.2\n1.3\n0.2\nsetosa\n\n\n4.5\n2.3\n1.3\n0.3\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n4.6\n3.6\n1.0\n0.2\nsetosa\n\n\n4.6\n3.2\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.7\n3.2\n1.6\n0.2\nsetosa\n\n\n4.8\n3.4\n1.6\n0.2\nsetosa\n\n\n4.8\n3.0\n1.4\n0.1\nsetosa\n\n\n4.8\n3.4\n1.9\n0.2\nsetosa\n\n\n4.8\n3.1\n1.6\n0.2\nsetosa\n\n\n4.8\n3.0\n1.4\n0.3\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.1\nsetosa\n\n\n4.9\n3.1\n1.5\n0.2\nsetosa\n\n\n4.9\n3.6\n1.4\n0.1\nsetosa\n\n\n4.9\n2.4\n3.3\n1.0\nversicolor\n\n\n4.9\n2.5\n4.5\n1.7\nvirginica\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n5.0\n3.0\n1.6\n0.2\nsetosa\n\n\n5.0\n3.4\n1.6\n0.4\nsetosa\n\n\n5.0\n3.2\n1.2\n0.2\nsetosa\n\n\n5.0\n3.5\n1.3\n0.3\nsetosa\n\n\n5.0\n3.5\n1.6\n0.6\nsetosa\n\n\n5.0\n3.3\n1.4\n0.2\nsetosa\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n6.6\n2.9\n4.6\n1.3\nversicolor\n\n\n6.6\n3.0\n4.4\n1.4\nversicolor\n\n\n6.7\n3.1\n4.4\n1.4\nversicolor\n\n\n6.7\n3.0\n5.0\n1.7\nversicolor\n\n\n6.7\n3.1\n4.7\n1.5\nversicolor\n\n\n6.7\n2.5\n5.8\n1.8\nvirginica\n\n\n6.7\n3.3\n5.7\n2.1\nvirginica\n\n\n6.7\n3.1\n5.6\n2.4\nvirginica\n\n\n6.7\n3.3\n5.7\n2.5\nvirginica\n\n\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n6.8\n2.8\n4.8\n1.4\nversicolor\n\n\n6.8\n3.0\n5.5\n2.1\nvirginica\n\n\n6.8\n3.2\n5.9\n2.3\nvirginica\n\n\n6.9\n3.1\n4.9\n1.5\nversicolor\n\n\n6.9\n3.2\n5.7\n2.3\nvirginica\n\n\n6.9\n3.1\n5.4\n2.1\nvirginica\n\n\n6.9\n3.1\n5.1\n2.3\nvirginica\n\n\n7.0\n3.2\n4.7\n1.4\nversicolor\n\n\n7.1\n3.0\n5.9\n2.1\nvirginica\n\n\n7.2\n3.6\n6.1\n2.5\nvirginica\n\n\n7.2\n3.2\n6.0\n1.8\nvirginica\n\n\n7.2\n3.0\n5.8\n1.6\nvirginica\n\n\n7.3\n2.9\n6.3\n1.8\nvirginica\n\n\n7.4\n2.8\n6.1\n1.9\nvirginica\n\n\n7.6\n3.0\n6.6\n2.1\nvirginica\n\n\n7.7\n3.8\n6.7\n2.2\nvirginica\n\n\n7.7\n2.6\n6.9\n2.3\nvirginica\n\n\n7.7\n2.8\n6.7\n2.0\nvirginica\n\n\n7.7\n3.0\n6.1\n2.3\nvirginica\n\n\n7.9\n3.8\n6.4\n2.0\nvirginica\n\n\n\n\n\nYou can also sort on descending order. You will again use age and this time the oldest patient will be the first.\n\niris_arrange &lt;- arrange(iris, desc(Sepal.Length))\niris_arrange\n\n\nA data.frame: 150 × 5\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n\n\n\n\n7.9\n3.8\n6.4\n2.0\nvirginica\n\n\n7.7\n3.8\n6.7\n2.2\nvirginica\n\n\n7.7\n2.6\n6.9\n2.3\nvirginica\n\n\n7.7\n2.8\n6.7\n2.0\nvirginica\n\n\n7.7\n3.0\n6.1\n2.3\nvirginica\n\n\n7.6\n3.0\n6.6\n2.1\nvirginica\n\n\n7.4\n2.8\n6.1\n1.9\nvirginica\n\n\n7.3\n2.9\n6.3\n1.8\nvirginica\n\n\n7.2\n3.6\n6.1\n2.5\nvirginica\n\n\n7.2\n3.2\n6.0\n1.8\nvirginica\n\n\n7.2\n3.0\n5.8\n1.6\nvirginica\n\n\n7.1\n3.0\n5.9\n2.1\nvirginica\n\n\n7.0\n3.2\n4.7\n1.4\nversicolor\n\n\n6.9\n3.1\n4.9\n1.5\nversicolor\n\n\n6.9\n3.2\n5.7\n2.3\nvirginica\n\n\n6.9\n3.1\n5.4\n2.1\nvirginica\n\n\n6.9\n3.1\n5.1\n2.3\nvirginica\n\n\n6.8\n2.8\n4.8\n1.4\nversicolor\n\n\n6.8\n3.0\n5.5\n2.1\nvirginica\n\n\n6.8\n3.2\n5.9\n2.3\nvirginica\n\n\n6.7\n3.1\n4.4\n1.4\nversicolor\n\n\n6.7\n3.0\n5.0\n1.7\nversicolor\n\n\n6.7\n3.1\n4.7\n1.5\nversicolor\n\n\n6.7\n2.5\n5.8\n1.8\nvirginica\n\n\n6.7\n3.3\n5.7\n2.1\nvirginica\n\n\n6.7\n3.1\n5.6\n2.4\nvirginica\n\n\n6.7\n3.3\n5.7\n2.5\nvirginica\n\n\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n6.6\n2.9\n4.6\n1.3\nversicolor\n\n\n6.6\n3.0\n4.4\n1.4\nversicolor\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n5.0\n3.0\n1.6\n0.2\nsetosa\n\n\n5.0\n3.4\n1.6\n0.4\nsetosa\n\n\n5.0\n3.2\n1.2\n0.2\nsetosa\n\n\n5.0\n3.5\n1.3\n0.3\nsetosa\n\n\n5.0\n3.5\n1.6\n0.6\nsetosa\n\n\n5.0\n3.3\n1.4\n0.2\nsetosa\n\n\n5.0\n2.0\n3.5\n1.0\nversicolor\n\n\n5.0\n2.3\n3.3\n1.0\nversicolor\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.1\nsetosa\n\n\n4.9\n3.1\n1.5\n0.2\nsetosa\n\n\n4.9\n3.6\n1.4\n0.1\nsetosa\n\n\n4.9\n2.4\n3.3\n1.0\nversicolor\n\n\n4.9\n2.5\n4.5\n1.7\nvirginica\n\n\n4.8\n3.4\n1.6\n0.2\nsetosa\n\n\n4.8\n3.0\n1.4\n0.1\nsetosa\n\n\n4.8\n3.4\n1.9\n0.2\nsetosa\n\n\n4.8\n3.1\n1.6\n0.2\nsetosa\n\n\n4.8\n3.0\n1.4\n0.3\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.7\n3.2\n1.6\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n4.6\n3.6\n1.0\n0.2\nsetosa\n\n\n4.6\n3.2\n1.4\n0.2\nsetosa\n\n\n4.5\n2.3\n1.3\n0.3\nsetosa\n\n\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n4.4\n3.0\n1.3\n0.2\nsetosa\n\n\n4.4\n3.2\n1.3\n0.2\nsetosa\n\n\n4.3\n3.0\n1.1\n0.1\nsetosa\n\n\n\n\n\nThis is a large data set, let’s say you want to subset the data to just the columns of information you need for your analysis. You can use the select function. Let’s say you are interested in just the patient id, sex and age.\n\niris_select &lt;- select(iris, Sepal.Length, Sepal.Width, Species)\niris_select\n\n\nA data.frame: 150 × 3\n\n\nSepal.Length\nSepal.Width\nSpecies\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n\n\n\n\n5.1\n3.5\nsetosa\n\n\n4.9\n3.0\nsetosa\n\n\n4.7\n3.2\nsetosa\n\n\n4.6\n3.1\nsetosa\n\n\n5.0\n3.6\nsetosa\n\n\n5.4\n3.9\nsetosa\n\n\n4.6\n3.4\nsetosa\n\n\n5.0\n3.4\nsetosa\n\n\n4.4\n2.9\nsetosa\n\n\n4.9\n3.1\nsetosa\n\n\n5.4\n3.7\nsetosa\n\n\n4.8\n3.4\nsetosa\n\n\n4.8\n3.0\nsetosa\n\n\n4.3\n3.0\nsetosa\n\n\n5.8\n4.0\nsetosa\n\n\n5.7\n4.4\nsetosa\n\n\n5.4\n3.9\nsetosa\n\n\n5.1\n3.5\nsetosa\n\n\n5.7\n3.8\nsetosa\n\n\n5.1\n3.8\nsetosa\n\n\n5.4\n3.4\nsetosa\n\n\n5.1\n3.7\nsetosa\n\n\n4.6\n3.6\nsetosa\n\n\n5.1\n3.3\nsetosa\n\n\n4.8\n3.4\nsetosa\n\n\n5.0\n3.0\nsetosa\n\n\n5.0\n3.4\nsetosa\n\n\n5.2\n3.5\nsetosa\n\n\n5.2\n3.4\nsetosa\n\n\n4.7\n3.2\nsetosa\n\n\n⋮\n⋮\n⋮\n\n\n6.9\n3.2\nvirginica\n\n\n5.6\n2.8\nvirginica\n\n\n7.7\n2.8\nvirginica\n\n\n6.3\n2.7\nvirginica\n\n\n6.7\n3.3\nvirginica\n\n\n7.2\n3.2\nvirginica\n\n\n6.2\n2.8\nvirginica\n\n\n6.1\n3.0\nvirginica\n\n\n6.4\n2.8\nvirginica\n\n\n7.2\n3.0\nvirginica\n\n\n7.4\n2.8\nvirginica\n\n\n7.9\n3.8\nvirginica\n\n\n6.4\n2.8\nvirginica\n\n\n6.3\n2.8\nvirginica\n\n\n6.1\n2.6\nvirginica\n\n\n7.7\n3.0\nvirginica\n\n\n6.3\n3.4\nvirginica\n\n\n6.4\n3.1\nvirginica\n\n\n6.0\n3.0\nvirginica\n\n\n6.9\n3.1\nvirginica\n\n\n6.7\n3.1\nvirginica\n\n\n6.9\n3.1\nvirginica\n\n\n5.8\n2.7\nvirginica\n\n\n6.8\n3.2\nvirginica\n\n\n6.7\n3.3\nvirginica\n\n\n6.7\n3.0\nvirginica\n\n\n6.3\n2.5\nvirginica\n\n\n6.5\n3.0\nvirginica\n\n\n6.2\n3.4\nvirginica\n\n\n5.9\n3.0\nvirginica\n\n\n\n\n\nFor reference you can use the “-” to select every column but the ones listed\n\niris_select &lt;- select(iris, -Sepal.Length, -Sepal.Width)\niris_select\n\n\nA data.frame: 150 × 3\n\n\nPetal.Length\nPetal.Width\nSpecies\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n\n\n\n\n1.4\n0.2\nsetosa\n\n\n1.4\n0.2\nsetosa\n\n\n1.3\n0.2\nsetosa\n\n\n1.5\n0.2\nsetosa\n\n\n1.4\n0.2\nsetosa\n\n\n1.7\n0.4\nsetosa\n\n\n1.4\n0.3\nsetosa\n\n\n1.5\n0.2\nsetosa\n\n\n1.4\n0.2\nsetosa\n\n\n1.5\n0.1\nsetosa\n\n\n1.5\n0.2\nsetosa\n\n\n1.6\n0.2\nsetosa\n\n\n1.4\n0.1\nsetosa\n\n\n1.1\n0.1\nsetosa\n\n\n1.2\n0.2\nsetosa\n\n\n1.5\n0.4\nsetosa\n\n\n1.3\n0.4\nsetosa\n\n\n1.4\n0.3\nsetosa\n\n\n1.7\n0.3\nsetosa\n\n\n1.5\n0.3\nsetosa\n\n\n1.7\n0.2\nsetosa\n\n\n1.5\n0.4\nsetosa\n\n\n1.0\n0.2\nsetosa\n\n\n1.7\n0.5\nsetosa\n\n\n1.9\n0.2\nsetosa\n\n\n1.6\n0.2\nsetosa\n\n\n1.6\n0.4\nsetosa\n\n\n1.5\n0.2\nsetosa\n\n\n1.4\n0.2\nsetosa\n\n\n1.6\n0.2\nsetosa\n\n\n⋮\n⋮\n⋮\n\n\n5.7\n2.3\nvirginica\n\n\n4.9\n2.0\nvirginica\n\n\n6.7\n2.0\nvirginica\n\n\n4.9\n1.8\nvirginica\n\n\n5.7\n2.1\nvirginica\n\n\n6.0\n1.8\nvirginica\n\n\n4.8\n1.8\nvirginica\n\n\n4.9\n1.8\nvirginica\n\n\n5.6\n2.1\nvirginica\n\n\n5.8\n1.6\nvirginica\n\n\n6.1\n1.9\nvirginica\n\n\n6.4\n2.0\nvirginica\n\n\n5.6\n2.2\nvirginica\n\n\n5.1\n1.5\nvirginica\n\n\n5.6\n1.4\nvirginica\n\n\n6.1\n2.3\nvirginica\n\n\n5.6\n2.4\nvirginica\n\n\n5.5\n1.8\nvirginica\n\n\n4.8\n1.8\nvirginica\n\n\n5.4\n2.1\nvirginica\n\n\n5.6\n2.4\nvirginica\n\n\n5.1\n2.3\nvirginica\n\n\n5.1\n1.9\nvirginica\n\n\n5.9\n2.3\nvirginica\n\n\n5.7\n2.5\nvirginica\n\n\n5.2\n2.3\nvirginica\n\n\n5.0\n1.9\nvirginica\n\n\n5.2\n2.0\nvirginica\n\n\n5.4\n2.3\nvirginica\n\n\n5.1\n1.8\nvirginica\n\n\n\n\n\nYou may only be interested in the setosa species for this data set. You can use the filter function to select only rows where species is equal to “setosa”.\n\niris_filter &lt;- filter(iris, Species==\"setosa\") \niris_filter\n\n\nA data.frame: 50 × 5\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.1\nsetosa\n\n\n5.4\n3.7\n1.5\n0.2\nsetosa\n\n\n4.8\n3.4\n1.6\n0.2\nsetosa\n\n\n4.8\n3.0\n1.4\n0.1\nsetosa\n\n\n4.3\n3.0\n1.1\n0.1\nsetosa\n\n\n5.8\n4.0\n1.2\n0.2\nsetosa\n\n\n5.7\n4.4\n1.5\n0.4\nsetosa\n\n\n5.4\n3.9\n1.3\n0.4\nsetosa\n\n\n5.1\n3.5\n1.4\n0.3\nsetosa\n\n\n5.7\n3.8\n1.7\n0.3\nsetosa\n\n\n5.1\n3.8\n1.5\n0.3\nsetosa\n\n\n5.4\n3.4\n1.7\n0.2\nsetosa\n\n\n5.1\n3.7\n1.5\n0.4\nsetosa\n\n\n4.6\n3.6\n1.0\n0.2\nsetosa\n\n\n5.1\n3.3\n1.7\n0.5\nsetosa\n\n\n4.8\n3.4\n1.9\n0.2\nsetosa\n\n\n5.0\n3.0\n1.6\n0.2\nsetosa\n\n\n5.0\n3.4\n1.6\n0.4\nsetosa\n\n\n5.2\n3.5\n1.5\n0.2\nsetosa\n\n\n5.2\n3.4\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.6\n0.2\nsetosa\n\n\n4.8\n3.1\n1.6\n0.2\nsetosa\n\n\n5.4\n3.4\n1.5\n0.4\nsetosa\n\n\n5.2\n4.1\n1.5\n0.1\nsetosa\n\n\n5.5\n4.2\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.2\n1.2\n0.2\nsetosa\n\n\n5.5\n3.5\n1.3\n0.2\nsetosa\n\n\n4.9\n3.6\n1.4\n0.1\nsetosa\n\n\n4.4\n3.0\n1.3\n0.2\nsetosa\n\n\n5.1\n3.4\n1.5\n0.2\nsetosa\n\n\n5.0\n3.5\n1.3\n0.3\nsetosa\n\n\n4.5\n2.3\n1.3\n0.3\nsetosa\n\n\n4.4\n3.2\n1.3\n0.2\nsetosa\n\n\n5.0\n3.5\n1.6\n0.6\nsetosa\n\n\n5.1\n3.8\n1.9\n0.4\nsetosa\n\n\n4.8\n3.0\n1.4\n0.3\nsetosa\n\n\n5.1\n3.8\n1.6\n0.2\nsetosa\n\n\n4.6\n3.2\n1.4\n0.2\nsetosa\n\n\n5.3\n3.7\n1.5\n0.2\nsetosa\n\n\n5.0\n3.3\n1.4\n0.2\nsetosa\n\n\n\n\n\nYou can also filter based on if septal length is greater than the mean of septal length.\n\niris_filter &lt;- filter(iris, Sepal.Length &gt; mean(iris$Sepal.Length))\niris_filter\n\n\nA data.frame: 70 × 5\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n\n\n\n\n7.0\n3.2\n4.7\n1.4\nversicolor\n\n\n6.4\n3.2\n4.5\n1.5\nversicolor\n\n\n6.9\n3.1\n4.9\n1.5\nversicolor\n\n\n6.5\n2.8\n4.6\n1.5\nversicolor\n\n\n6.3\n3.3\n4.7\n1.6\nversicolor\n\n\n6.6\n2.9\n4.6\n1.3\nversicolor\n\n\n5.9\n3.0\n4.2\n1.5\nversicolor\n\n\n6.0\n2.2\n4.0\n1.0\nversicolor\n\n\n6.1\n2.9\n4.7\n1.4\nversicolor\n\n\n6.7\n3.1\n4.4\n1.4\nversicolor\n\n\n6.2\n2.2\n4.5\n1.5\nversicolor\n\n\n5.9\n3.2\n4.8\n1.8\nversicolor\n\n\n6.1\n2.8\n4.0\n1.3\nversicolor\n\n\n6.3\n2.5\n4.9\n1.5\nversicolor\n\n\n6.1\n2.8\n4.7\n1.2\nversicolor\n\n\n6.4\n2.9\n4.3\n1.3\nversicolor\n\n\n6.6\n3.0\n4.4\n1.4\nversicolor\n\n\n6.8\n2.8\n4.8\n1.4\nversicolor\n\n\n6.7\n3.0\n5.0\n1.7\nversicolor\n\n\n6.0\n2.9\n4.5\n1.5\nversicolor\n\n\n6.0\n2.7\n5.1\n1.6\nversicolor\n\n\n6.0\n3.4\n4.5\n1.6\nversicolor\n\n\n6.7\n3.1\n4.7\n1.5\nversicolor\n\n\n6.3\n2.3\n4.4\n1.3\nversicolor\n\n\n6.1\n3.0\n4.6\n1.4\nversicolor\n\n\n6.2\n2.9\n4.3\n1.3\nversicolor\n\n\n6.3\n3.3\n6.0\n2.5\nvirginica\n\n\n7.1\n3.0\n5.9\n2.1\nvirginica\n\n\n6.3\n2.9\n5.6\n1.8\nvirginica\n\n\n6.5\n3.0\n5.8\n2.2\nvirginica\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n7.7\n2.6\n6.9\n2.3\nvirginica\n\n\n6.0\n2.2\n5.0\n1.5\nvirginica\n\n\n6.9\n3.2\n5.7\n2.3\nvirginica\n\n\n7.7\n2.8\n6.7\n2.0\nvirginica\n\n\n6.3\n2.7\n4.9\n1.8\nvirginica\n\n\n6.7\n3.3\n5.7\n2.1\nvirginica\n\n\n7.2\n3.2\n6.0\n1.8\nvirginica\n\n\n6.2\n2.8\n4.8\n1.8\nvirginica\n\n\n6.1\n3.0\n4.9\n1.8\nvirginica\n\n\n6.4\n2.8\n5.6\n2.1\nvirginica\n\n\n7.2\n3.0\n5.8\n1.6\nvirginica\n\n\n7.4\n2.8\n6.1\n1.9\nvirginica\n\n\n7.9\n3.8\n6.4\n2.0\nvirginica\n\n\n6.4\n2.8\n5.6\n2.2\nvirginica\n\n\n6.3\n2.8\n5.1\n1.5\nvirginica\n\n\n6.1\n2.6\n5.6\n1.4\nvirginica\n\n\n7.7\n3.0\n6.1\n2.3\nvirginica\n\n\n6.3\n3.4\n5.6\n2.4\nvirginica\n\n\n6.4\n3.1\n5.5\n1.8\nvirginica\n\n\n6.0\n3.0\n4.8\n1.8\nvirginica\n\n\n6.9\n3.1\n5.4\n2.1\nvirginica\n\n\n6.7\n3.1\n5.6\n2.4\nvirginica\n\n\n6.9\n3.1\n5.1\n2.3\nvirginica\n\n\n6.8\n3.2\n5.9\n2.3\nvirginica\n\n\n6.7\n3.3\n5.7\n2.5\nvirginica\n\n\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n\nYou may be interested in computing new information from your data.\nFor instance, you may want to calculate the ratio of sepal width to sepal length.\n\niris_mutate &lt;- mutate(iris, Sepal.Ratio=Sepal.Width/Sepal.Length) \niris_mutate\n\n\nA data.frame: 150 × 6\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\nSepal.Ratio\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n&lt;dbl&gt;\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n0.6862745\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n0.6122449\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n0.6808511\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n0.6739130\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n0.7200000\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n0.7222222\n\n\n4.6\n3.4\n1.4\n0.3\nsetosa\n0.7391304\n\n\n5.0\n3.4\n1.5\n0.2\nsetosa\n0.6800000\n\n\n4.4\n2.9\n1.4\n0.2\nsetosa\n0.6590909\n\n\n4.9\n3.1\n1.5\n0.1\nsetosa\n0.6326531\n\n\n5.4\n3.7\n1.5\n0.2\nsetosa\n0.6851852\n\n\n4.8\n3.4\n1.6\n0.2\nsetosa\n0.7083333\n\n\n4.8\n3.0\n1.4\n0.1\nsetosa\n0.6250000\n\n\n4.3\n3.0\n1.1\n0.1\nsetosa\n0.6976744\n\n\n5.8\n4.0\n1.2\n0.2\nsetosa\n0.6896552\n\n\n5.7\n4.4\n1.5\n0.4\nsetosa\n0.7719298\n\n\n5.4\n3.9\n1.3\n0.4\nsetosa\n0.7222222\n\n\n5.1\n3.5\n1.4\n0.3\nsetosa\n0.6862745\n\n\n5.7\n3.8\n1.7\n0.3\nsetosa\n0.6666667\n\n\n5.1\n3.8\n1.5\n0.3\nsetosa\n0.7450980\n\n\n5.4\n3.4\n1.7\n0.2\nsetosa\n0.6296296\n\n\n5.1\n3.7\n1.5\n0.4\nsetosa\n0.7254902\n\n\n4.6\n3.6\n1.0\n0.2\nsetosa\n0.7826087\n\n\n5.1\n3.3\n1.7\n0.5\nsetosa\n0.6470588\n\n\n4.8\n3.4\n1.9\n0.2\nsetosa\n0.7083333\n\n\n5.0\n3.0\n1.6\n0.2\nsetosa\n0.6000000\n\n\n5.0\n3.4\n1.6\n0.4\nsetosa\n0.6800000\n\n\n5.2\n3.5\n1.5\n0.2\nsetosa\n0.6730769\n\n\n5.2\n3.4\n1.4\n0.2\nsetosa\n0.6538462\n\n\n4.7\n3.2\n1.6\n0.2\nsetosa\n0.6808511\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n6.9\n3.2\n5.7\n2.3\nvirginica\n0.4637681\n\n\n5.6\n2.8\n4.9\n2.0\nvirginica\n0.5000000\n\n\n7.7\n2.8\n6.7\n2.0\nvirginica\n0.3636364\n\n\n6.3\n2.7\n4.9\n1.8\nvirginica\n0.4285714\n\n\n6.7\n3.3\n5.7\n2.1\nvirginica\n0.4925373\n\n\n7.2\n3.2\n6.0\n1.8\nvirginica\n0.4444444\n\n\n6.2\n2.8\n4.8\n1.8\nvirginica\n0.4516129\n\n\n6.1\n3.0\n4.9\n1.8\nvirginica\n0.4918033\n\n\n6.4\n2.8\n5.6\n2.1\nvirginica\n0.4375000\n\n\n7.2\n3.0\n5.8\n1.6\nvirginica\n0.4166667\n\n\n7.4\n2.8\n6.1\n1.9\nvirginica\n0.3783784\n\n\n7.9\n3.8\n6.4\n2.0\nvirginica\n0.4810127\n\n\n6.4\n2.8\n5.6\n2.2\nvirginica\n0.4375000\n\n\n6.3\n2.8\n5.1\n1.5\nvirginica\n0.4444444\n\n\n6.1\n2.6\n5.6\n1.4\nvirginica\n0.4262295\n\n\n7.7\n3.0\n6.1\n2.3\nvirginica\n0.3896104\n\n\n6.3\n3.4\n5.6\n2.4\nvirginica\n0.5396825\n\n\n6.4\n3.1\n5.5\n1.8\nvirginica\n0.4843750\n\n\n6.0\n3.0\n4.8\n1.8\nvirginica\n0.5000000\n\n\n6.9\n3.1\n5.4\n2.1\nvirginica\n0.4492754\n\n\n6.7\n3.1\n5.6\n2.4\nvirginica\n0.4626866\n\n\n6.9\n3.1\n5.1\n2.3\nvirginica\n0.4492754\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n0.4655172\n\n\n6.8\n3.2\n5.9\n2.3\nvirginica\n0.4705882\n\n\n6.7\n3.3\n5.7\n2.5\nvirginica\n0.4925373\n\n\n6.7\n3.0\n5.2\n2.3\nvirginica\n0.4477612\n\n\n6.3\n2.5\n5.0\n1.9\nvirginica\n0.3968254\n\n\n6.5\n3.0\n5.2\n2.0\nvirginica\n0.4615385\n\n\n6.2\n3.4\n5.4\n2.3\nvirginica\n0.5483871\n\n\n5.9\n3.0\n5.1\n1.8\nvirginica\n0.5084746\n\n\n\n\n\nYou can use the summarize function to summarize the data in specific ways. Here you are outputting a table with mean sepal length of the dataset.\n\navg_sepal_length &lt;-summarize(iris, mean_val = mean(Sepal.Length))\navg_sepal_length\n\n\nA data.frame: 1 × 1\n\n\nmean_val\n\n\n&lt;dbl&gt;\n\n\n\n\n5.843333\n\n\n\n\n\nYou can also group the data based on a specified variable or group of variables.\n\ngroup_by_species &lt;- group_by(iris,Species)\ngroup_by_species\n\n\nA grouped_df: 150 × 5\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.1\nsetosa\n\n\n5.4\n3.7\n1.5\n0.2\nsetosa\n\n\n4.8\n3.4\n1.6\n0.2\nsetosa\n\n\n4.8\n3.0\n1.4\n0.1\nsetosa\n\n\n4.3\n3.0\n1.1\n0.1\nsetosa\n\n\n5.8\n4.0\n1.2\n0.2\nsetosa\n\n\n5.7\n4.4\n1.5\n0.4\nsetosa\n\n\n5.4\n3.9\n1.3\n0.4\nsetosa\n\n\n5.1\n3.5\n1.4\n0.3\nsetosa\n\n\n5.7\n3.8\n1.7\n0.3\nsetosa\n\n\n5.1\n3.8\n1.5\n0.3\nsetosa\n\n\n5.4\n3.4\n1.7\n0.2\nsetosa\n\n\n5.1\n3.7\n1.5\n0.4\nsetosa\n\n\n4.6\n3.6\n1.0\n0.2\nsetosa\n\n\n5.1\n3.3\n1.7\n0.5\nsetosa\n\n\n4.8\n3.4\n1.9\n0.2\nsetosa\n\n\n5.0\n3.0\n1.6\n0.2\nsetosa\n\n\n5.0\n3.4\n1.6\n0.4\nsetosa\n\n\n5.2\n3.5\n1.5\n0.2\nsetosa\n\n\n5.2\n3.4\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.6\n0.2\nsetosa\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n6.9\n3.2\n5.7\n2.3\nvirginica\n\n\n5.6\n2.8\n4.9\n2.0\nvirginica\n\n\n7.7\n2.8\n6.7\n2.0\nvirginica\n\n\n6.3\n2.7\n4.9\n1.8\nvirginica\n\n\n6.7\n3.3\n5.7\n2.1\nvirginica\n\n\n7.2\n3.2\n6.0\n1.8\nvirginica\n\n\n6.2\n2.8\n4.8\n1.8\nvirginica\n\n\n6.1\n3.0\n4.9\n1.8\nvirginica\n\n\n6.4\n2.8\n5.6\n2.1\nvirginica\n\n\n7.2\n3.0\n5.8\n1.6\nvirginica\n\n\n7.4\n2.8\n6.1\n1.9\nvirginica\n\n\n7.9\n3.8\n6.4\n2.0\nvirginica\n\n\n6.4\n2.8\n5.6\n2.2\nvirginica\n\n\n6.3\n2.8\n5.1\n1.5\nvirginica\n\n\n6.1\n2.6\n5.6\n1.4\nvirginica\n\n\n7.7\n3.0\n6.1\n2.3\nvirginica\n\n\n6.3\n3.4\n5.6\n2.4\nvirginica\n\n\n6.4\n3.1\n5.5\n1.8\nvirginica\n\n\n6.0\n3.0\n4.8\n1.8\nvirginica\n\n\n6.9\n3.1\n5.4\n2.1\nvirginica\n\n\n6.7\n3.1\n5.6\n2.4\nvirginica\n\n\n6.9\n3.1\n5.1\n2.3\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n6.8\n3.2\n5.9\n2.3\nvirginica\n\n\n6.7\n3.3\n5.7\n2.5\nvirginica\n\n\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n\nInterestingly nothing changes about the data that can be seen in the table. You can use the groups function to look at how the data is. You can always use View(iris) to look at the original data set.\n\ngroups(group_by_species)\ngroups(iris)\n\n[[1]]\nSpecies\n\n\n\n\n\n\nGroup by is particularly helpful when used in conjunction other functions such as the previously used summarize function. You can combine the group by and summarize function to calculate the mean age of males versus females. You could do this in two steps but you don’t need the intermediate data so a pipe “%&gt;%” can be used. A pipe is like saying do this then immediately follow with this next function.\n\niris_final &lt;- iris %&gt;% group_by(Species) %&gt;% summarize(new_col = mean(Sepal.Length))\niris_final\n\n\nA tibble: 3 × 2\n\n\nSpecies\nnew_col\n\n\n&lt;fct&gt;\n&lt;dbl&gt;\n\n\n\n\nsetosa\n5.006\n\n\nversicolor\n5.936\n\n\nvirginica\n6.588\n\n\n\n\n\nThis concludes the introduction to data manipulation!"
  },
  {
    "objectID": "getting-started/r/datasets.html",
    "href": "getting-started/r/datasets.html",
    "title": "Packages_and_Datasets",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nprint('Hello, World!')\n\n[1] \"Hello, World!\""
  },
  {
    "objectID": "getting-started/r/datasets.html#r-markdown",
    "href": "getting-started/r/datasets.html#r-markdown",
    "title": "Packages_and_Datasets",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nprint('Hello, World!')\n\n[1] \"Hello, World!\""
  },
  {
    "objectID": "getting-started/r/datasets.html#packages-and-datasets",
    "href": "getting-started/r/datasets.html#packages-and-datasets",
    "title": "Packages_and_Datasets",
    "section": "Packages and Datasets",
    "text": "Packages and Datasets\nThere are many built-in functions and data sets include in R, which is known as base R.\nIn addition to base R, additional packages can be downloaded, imported and used.\nThis markdown file will cover how to work with built-in data sets and imported packages.\nThe following chuck will open a window with a list of built-in data sets you can experiment with.\n\ndata()\n\nData sets\n\n\nA data.frame: 108 × 3\n\n\nPackage\nItem\nTitle\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\ndatasets\nAirPassengers\nMonthly Airline Passenger Numbers 1949-1960\n\n\ndatasets\nBJsales\nSales Data with Leading Indicator\n\n\ndatasets\nBJsales.lead (BJsales)\nSales Data with Leading Indicator\n\n\ndatasets\nBOD\nBiochemical Oxygen Demand\n\n\ndatasets\nCO2\nCarbon Dioxide Uptake in Grass Plants\n\n\ndatasets\nChickWeight\nWeight versus age of chicks on different diets\n\n\ndatasets\nDNase\nElisa assay of DNase\n\n\ndatasets\nEuStockMarkets\nDaily Closing Prices of Major European Stock Indices, 1991-1998\n\n\ndatasets\nFormaldehyde\nDetermination of Formaldehyde\n\n\ndatasets\nHairEyeColor\nHair and Eye Color of Statistics Students\n\n\ndatasets\nHarman23.cor\nHarman Example 2.3\n\n\ndatasets\nHarman74.cor\nHarman Example 7.4\n\n\ndatasets\nIndometh\nPharmacokinetics of Indomethacin\n\n\ndatasets\nInsectSprays\nEffectiveness of Insect Sprays\n\n\ndatasets\nJohnsonJohnson \nQuarterly Earnings per Johnson & Johnson Share \n\n\ndatasets\nLakeHuron\nLevel of Lake Huron 1875-1972\n\n\ndatasets\nLifeCycleSavings\nIntercountry Life-Cycle Savings Data\n\n\ndatasets\nLoblolly\nGrowth of Loblolly Pine Trees\n\n\ndatasets\nNile\nFlow of the River Nile\n\n\ndatasets\nOrange\nGrowth of Orange Trees\n\n\ndatasets\nOrchardSprays\nPotency of Orchard Sprays\n\n\ndatasets\nPlantGrowth\nResults from an Experiment on Plant Growth\n\n\ndatasets\nPuromycin\nReaction Velocity of an Enzymatic Reaction\n\n\ndatasets\nSeatbelts\nRoad Casualties in Great Britain 1969-84\n\n\ndatasets\nTheoph\nPharmacokinetics of Theophylline\n\n\ndatasets\nTitanic\nSurvival of passengers on the Titanic\n\n\ndatasets\nToothGrowth\nThe Effect of Vitamin C on Tooth Growth in Guinea Pigs\n\n\ndatasets\nUCBAdmissions\nStudent Admissions at UC Berkeley\n\n\ndatasets\nUKDriverDeaths\nRoad Casualties in Great Britain 1969-84\n\n\ndatasets\nUKgas\nUK Quarterly Gas Consumption\n\n\ndatasets\nUSAccDeaths\nAccidental Deaths in the US 1973-1978\n\n\ndatasets\nUSArrests\nViolent Crime Rates by US State\n\n\ndatasets\nUSJudgeRatings\nLawyers' Ratings of State Judges in the US Superior Court\n\n\ndatasets\nUSPersonalExpenditure\nPersonal Expenditure Data\n\n\ndatasets\nUScitiesD\nDistances Between European Cities and Between US Cities\n\n\ndatasets\nVADeaths\nDeath Rates in Virginia (1940)\n\n\ndatasets\nWWWusage\nInternet Usage per Minute\n\n\ndatasets\nWorldPhones\nThe World's Telephones\n\n\ndatasets\nability.cov\nAbility and Intelligence Tests\n\n\ndatasets\nairmiles\nPassenger Miles on Commercial US Airlines, 1937-1960\n\n\ndatasets\nairquality\nNew York Air Quality Measurements\n\n\ndatasets\nanscombe\nAnscombe's Quartet of 'Identical' Simple Linear Regressions\n\n\ndatasets\nattenu\nThe Joyner-Boore Attenuation Data\n\n\ndatasets\nattitude\nThe Chatterjee-Price Attitude Data\n\n\ndatasets\naustres\nQuarterly Time Series of the Number of Australian Residents\n\n\ndatasets\nbeaver1 (beavers)\nBody Temperature Series of Two Beavers\n\n\ndatasets\nbeaver2 (beavers)\nBody Temperature Series of Two Beavers\n\n\ndatasets\ncars\nSpeed and Stopping Distances of Cars\n\n\ndatasets\nchickwts\nChicken Weights by Feed Type\n\n\ndatasets\nco2\nMauna Loa Atmospheric CO2 Concentration\n\n\ndatasets\ncrimtab\nStudent's 3000 Criminals Data\n\n\ndatasets\ndiscoveries\nYearly Numbers of Important Discoveries\n\n\ndatasets\nesoph\nSmoking, Alcohol and (O)esophageal Cancer\n\n\ndatasets\neuro\nConversion Rates of Euro Currencies\n\n\ndatasets\neuro.cross (euro)\nConversion Rates of Euro Currencies\n\n\ndatasets\neurodist\nDistances Between European Cities and Between US Cities\n\n\ndatasets\nfaithful\nOld Faithful Geyser Data\n\n\ndatasets\nfdeaths (UKLungDeaths)\nMonthly Deaths from Lung Diseases in the UK\n\n\ndatasets\nfreeny\nFreeny's Revenue Data\n\n\ndatasets\nfreeny.x (freeny)\nFreeny's Revenue Data\n\n\ndatasets\nfreeny.y (freeny)\nFreeny's Revenue Data\n\n\ndatasets\ngait\nHip and Knee Angle while Walking\n\n\ndatasets\ninfert\nInfertility after Spontaneous and Induced Abortion\n\n\ndatasets\niris\nEdgar Anderson's Iris Data\n\n\ndatasets\niris3\nEdgar Anderson's Iris Data\n\n\ndatasets\nislands\nAreas of the World's Major Landmasses\n\n\ndatasets\nldeaths (UKLungDeaths)\nMonthly Deaths from Lung Diseases in the UK\n\n\ndatasets\nlh\nLuteinizing Hormone in Blood Samples\n\n\ndatasets\nlongley\nLongley's Economic Regression Data\n\n\ndatasets\nlynx\nAnnual Canadian Lynx trappings 1821-1934\n\n\ndatasets\nmdeaths (UKLungDeaths)\nMonthly Deaths from Lung Diseases in the UK\n\n\ndatasets\nmorley\nMichelson Speed of Light Data\n\n\ndatasets\nmtcars\nMotor Trend Car Road Tests\n\n\ndatasets\nnhtemp\nAverage Yearly Temperatures in New Haven\n\n\ndatasets\nnottem\nAverage Monthly Temperatures at Nottingham, 1920-1939\n\n\ndatasets\nnpk\nClassical N, P, K Factorial Experiment\n\n\ndatasets\noccupationalStatus\nOccupational Status of Fathers and their Sons\n\n\ndatasets\npenguins\nMeasurements of Penguins near Palmer Station, Antarctica\n\n\ndatasets\npenguins_raw (penguins)\nMeasurements of Penguins near Palmer Station, Antarctica\n\n\ndatasets\nprecip\nAnnual Precipitation in Selected US Cities\n\n\ndatasets\npresidents\nQuarterly Approval Ratings of US Presidents\n\n\ndatasets\npressure\nVapor Pressure of Mercury as a Function of Temperature\n\n\ndatasets\nquakes\nLocations of Earthquakes off Fiji\n\n\ndatasets\nrandu\nRandom Numbers from Congruential Generator RANDU\n\n\ndatasets\nrivers\nLengths of Major North American Rivers\n\n\ndatasets\nrock\nMeasurements on Petroleum Rock Samples\n\n\ndatasets\nsleep\nStudent's Sleep Data\n\n\ndatasets\nstack.loss (stackloss)\nBrownlee's Stack Loss Plant Data\n\n\ndatasets\nstack.x (stackloss)\nBrownlee's Stack Loss Plant Data\n\n\ndatasets\nstackloss\nBrownlee's Stack Loss Plant Data\n\n\ndatasets\nstate.abb (state)\nUS State Facts and Figures\n\n\ndatasets\nstate.area (state)\nUS State Facts and Figures\n\n\ndatasets\nstate.center (state)\nUS State Facts and Figures\n\n\ndatasets\nstate.division (state)\nUS State Facts and Figures\n\n\ndatasets\nstate.name (state)\nUS State Facts and Figures\n\n\ndatasets\nstate.region (state)\nUS State Facts and Figures\n\n\ndatasets\nstate.x77 (state)\nUS State Facts and Figures\n\n\ndatasets\nsunspot.m2014 (sunspot.month)\nMonthly Sunspot Data, from 1749 to \"Present\"\n\n\ndatasets\nsunspot.month\nMonthly Sunspot Data, from 1749 to \"Present\"\n\n\ndatasets\nsunspot.year\nYearly Sunspot Data, 1700-1988\n\n\ndatasets\nsunspots\nMonthly Sunspot Numbers, 1749-1983\n\n\ndatasets\nswiss\nSwiss Fertility and Socioeconomic Indicators (1888) Data\n\n\ndatasets\ntreering\nYearly Tree-Ring Data, -6000-1979\n\n\ndatasets\ntrees\nDiameter, Height and Volume for Black Cherry Trees\n\n\ndatasets\nuspop\nPopulations Recorded by the US Census\n\n\ndatasets\nvolcano\nTopographic Information on Auckland's Maunga Whau Volcano\n\n\ndatasets\nwarpbreaks\nThe Number of Breaks in Yarn during Weaving\n\n\ndatasets\nwomen\nAverage Heights and Weights for American Women\n\n\n\n\n\nUse ‘data(package = .packages(all.available = TRUE))’\nto list the data sets in all *available* packages."
  },
  {
    "objectID": "getting-started/r/datasets.html#iris-dataset",
    "href": "getting-started/r/datasets.html#iris-dataset",
    "title": "Packages_and_Datasets",
    "section": "Iris Dataset",
    "text": "Iris Dataset\nFor today’s session we will begin by looking at a built-in data set, known as iris.\nTo see the data, simply run the command iris in a code chunk.\n\niris\n\n\nA data.frame: 150 × 5\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.1\nsetosa\n\n\n5.4\n3.7\n1.5\n0.2\nsetosa\n\n\n4.8\n3.4\n1.6\n0.2\nsetosa\n\n\n4.8\n3.0\n1.4\n0.1\nsetosa\n\n\n4.3\n3.0\n1.1\n0.1\nsetosa\n\n\n5.8\n4.0\n1.2\n0.2\nsetosa\n\n\n5.7\n4.4\n1.5\n0.4\nsetosa\n\n\n5.4\n3.9\n1.3\n0.4\nsetosa\n\n\n5.1\n3.5\n1.4\n0.3\nsetosa\n\n\n5.7\n3.8\n1.7\n0.3\nsetosa\n\n\n5.1\n3.8\n1.5\n0.3\nsetosa\n\n\n5.4\n3.4\n1.7\n0.2\nsetosa\n\n\n5.1\n3.7\n1.5\n0.4\nsetosa\n\n\n4.6\n3.6\n1.0\n0.2\nsetosa\n\n\n5.1\n3.3\n1.7\n0.5\nsetosa\n\n\n4.8\n3.4\n1.9\n0.2\nsetosa\n\n\n5.0\n3.0\n1.6\n0.2\nsetosa\n\n\n5.0\n3.4\n1.6\n0.4\nsetosa\n\n\n5.2\n3.5\n1.5\n0.2\nsetosa\n\n\n5.2\n3.4\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.6\n0.2\nsetosa\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n6.9\n3.2\n5.7\n2.3\nvirginica\n\n\n5.6\n2.8\n4.9\n2.0\nvirginica\n\n\n7.7\n2.8\n6.7\n2.0\nvirginica\n\n\n6.3\n2.7\n4.9\n1.8\nvirginica\n\n\n6.7\n3.3\n5.7\n2.1\nvirginica\n\n\n7.2\n3.2\n6.0\n1.8\nvirginica\n\n\n6.2\n2.8\n4.8\n1.8\nvirginica\n\n\n6.1\n3.0\n4.9\n1.8\nvirginica\n\n\n6.4\n2.8\n5.6\n2.1\nvirginica\n\n\n7.2\n3.0\n5.8\n1.6\nvirginica\n\n\n7.4\n2.8\n6.1\n1.9\nvirginica\n\n\n7.9\n3.8\n6.4\n2.0\nvirginica\n\n\n6.4\n2.8\n5.6\n2.2\nvirginica\n\n\n6.3\n2.8\n5.1\n1.5\nvirginica\n\n\n6.1\n2.6\n5.6\n1.4\nvirginica\n\n\n7.7\n3.0\n6.1\n2.3\nvirginica\n\n\n6.3\n3.4\n5.6\n2.4\nvirginica\n\n\n6.4\n3.1\n5.5\n1.8\nvirginica\n\n\n6.0\n3.0\n4.8\n1.8\nvirginica\n\n\n6.9\n3.1\n5.4\n2.1\nvirginica\n\n\n6.7\n3.1\n5.6\n2.4\nvirginica\n\n\n6.9\n3.1\n5.1\n2.3\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n6.8\n3.2\n5.9\n2.3\nvirginica\n\n\n6.7\n3.3\n5.7\n2.5\nvirginica\n\n\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n\nAs shown, there are 5 columns and 150 rows.\nWhen wrangling data, it is often best to start by getting a summary of the data.\nTo do so, run the next chunk.\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\nR conveniently offers descriptive statistics for each numerical column.\nFor the Species column, a count of the categorical variables is returned.\nFrom this, we can tell that the data represents 3 species of iris, with 50 examples of each.\n\nSelecting Data\nTo select a single row of data from the data set, use a square bracket[rows, columns] and specify the row number followed by a ,.\n\niris[1,]\n\n\nA data.frame: 1 × 5\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n\n\n\nTo select a single column of data from the dataset, use the $ symbol followed by the name of the column of interest.\n\niris$Petal.Length\n\n\n1.41.41.31.51.41.71.41.51.41.51.51.61.41.11.21.51.31.41.71.51.71.511.71.91.61.61.51.41.61.61.51.51.41.51.21.31.41.31.51.31.31.31.61.91.41.61.41.51.44.74.54.944.64.54.73.34.63.93.54.244.73.64.44.54.14.53.94.844.94.74.34.44.854.53.53.83.73.95.14.54.54.74.44.144.44.643.34.24.24.24.334.165.15.95.65.86.64.56.35.86.15.15.35.555.15.35.56.76.955.74.96.74.95.764.84.95.65.86.16.45.65.15.66.15.65.54.85.45.65.15.15.95.75.255.25.45.1\n\n\nAlternatively, you can use a square bracket and specify the column number after a ,.\n\niris[,3]\n\n\n1.41.41.31.51.41.71.41.51.41.51.51.61.41.11.21.51.31.41.71.51.71.511.71.91.61.61.51.41.61.61.51.51.41.51.21.31.41.31.51.31.31.31.61.91.41.61.41.51.44.74.54.944.64.54.73.34.63.93.54.244.73.64.44.54.14.53.94.844.94.74.34.44.854.53.53.83.73.95.14.54.54.74.44.144.44.643.34.24.24.24.334.165.15.95.65.86.64.56.35.86.15.15.35.555.15.35.56.76.955.74.96.74.95.764.84.95.65.86.16.45.65.15.66.15.65.54.85.45.65.15.15.95.75.255.25.45.1\n\n\nYou can also combine these methods to specify rows and columns simultaneously.\n\niris[130:150, 1:3]\n\n\nA data.frame: 21 × 3\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n130\n7.2\n3.0\n5.8\n\n\n131\n7.4\n2.8\n6.1\n\n\n132\n7.9\n3.8\n6.4\n\n\n133\n6.4\n2.8\n5.6\n\n\n134\n6.3\n2.8\n5.1\n\n\n135\n6.1\n2.6\n5.6\n\n\n136\n7.7\n3.0\n6.1\n\n\n137\n6.3\n3.4\n5.6\n\n\n138\n6.4\n3.1\n5.5\n\n\n139\n6.0\n3.0\n4.8\n\n\n140\n6.9\n3.1\n5.4\n\n\n141\n6.7\n3.1\n5.6\n\n\n142\n6.9\n3.1\n5.1\n\n\n143\n5.8\n2.7\n5.1\n\n\n144\n6.8\n3.2\n5.9\n\n\n145\n6.7\n3.3\n5.7\n\n\n146\n6.7\n3.0\n5.2\n\n\n147\n6.3\n2.5\n5.0\n\n\n148\n6.5\n3.0\n5.2\n\n\n149\n6.2\n3.4\n5.4\n\n\n150\n5.9\n3.0\n5.1\n\n\n\n\n\nSelecting rows and columns by index (number) can be useful, but sometimes it more convenient to use labels. Conditions can also be applied when sub-setting data.\n\niris[iris$Species==\"setosa\", ]\n\n\nA data.frame: 50 × 5\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n6\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n7\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n8\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n9\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n10\n4.9\n3.1\n1.5\n0.1\nsetosa\n\n\n11\n5.4\n3.7\n1.5\n0.2\nsetosa\n\n\n12\n4.8\n3.4\n1.6\n0.2\nsetosa\n\n\n13\n4.8\n3.0\n1.4\n0.1\nsetosa\n\n\n14\n4.3\n3.0\n1.1\n0.1\nsetosa\n\n\n15\n5.8\n4.0\n1.2\n0.2\nsetosa\n\n\n16\n5.7\n4.4\n1.5\n0.4\nsetosa\n\n\n17\n5.4\n3.9\n1.3\n0.4\nsetosa\n\n\n18\n5.1\n3.5\n1.4\n0.3\nsetosa\n\n\n19\n5.7\n3.8\n1.7\n0.3\nsetosa\n\n\n20\n5.1\n3.8\n1.5\n0.3\nsetosa\n\n\n21\n5.4\n3.4\n1.7\n0.2\nsetosa\n\n\n22\n5.1\n3.7\n1.5\n0.4\nsetosa\n\n\n23\n4.6\n3.6\n1.0\n0.2\nsetosa\n\n\n24\n5.1\n3.3\n1.7\n0.5\nsetosa\n\n\n25\n4.8\n3.4\n1.9\n0.2\nsetosa\n\n\n26\n5.0\n3.0\n1.6\n0.2\nsetosa\n\n\n27\n5.0\n3.4\n1.6\n0.4\nsetosa\n\n\n28\n5.2\n3.5\n1.5\n0.2\nsetosa\n\n\n29\n5.2\n3.4\n1.4\n0.2\nsetosa\n\n\n30\n4.7\n3.2\n1.6\n0.2\nsetosa\n\n\n31\n4.8\n3.1\n1.6\n0.2\nsetosa\n\n\n32\n5.4\n3.4\n1.5\n0.4\nsetosa\n\n\n33\n5.2\n4.1\n1.5\n0.1\nsetosa\n\n\n34\n5.5\n4.2\n1.4\n0.2\nsetosa\n\n\n35\n4.9\n3.1\n1.5\n0.2\nsetosa\n\n\n36\n5.0\n3.2\n1.2\n0.2\nsetosa\n\n\n37\n5.5\n3.5\n1.3\n0.2\nsetosa\n\n\n38\n4.9\n3.6\n1.4\n0.1\nsetosa\n\n\n39\n4.4\n3.0\n1.3\n0.2\nsetosa\n\n\n40\n5.1\n3.4\n1.5\n0.2\nsetosa\n\n\n41\n5.0\n3.5\n1.3\n0.3\nsetosa\n\n\n42\n4.5\n2.3\n1.3\n0.3\nsetosa\n\n\n43\n4.4\n3.2\n1.3\n0.2\nsetosa\n\n\n44\n5.0\n3.5\n1.6\n0.6\nsetosa\n\n\n45\n5.1\n3.8\n1.9\n0.4\nsetosa\n\n\n46\n4.8\n3.0\n1.4\n0.3\nsetosa\n\n\n47\n5.1\n3.8\n1.6\n0.2\nsetosa\n\n\n48\n4.6\n3.2\n1.4\n0.2\nsetosa\n\n\n49\n5.3\n3.7\n1.5\n0.2\nsetosa\n\n\n50\n5.0\n3.3\n1.4\n0.2\nsetosa\n\n\n\n\n\nIn the above example, by leaving the space following the comma blank, it will return all columns of data.\nYou can specify columns to be returned as well. For example,\n\niris[iris$Species==\"setosa\", c(\"Sepal.Length\", \"Sepal.Width\")]\n\n\nA data.frame: 50 × 2\n\n\n\nSepal.Length\nSepal.Width\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n5.1\n3.5\n\n\n2\n4.9\n3.0\n\n\n3\n4.7\n3.2\n\n\n4\n4.6\n3.1\n\n\n5\n5.0\n3.6\n\n\n6\n5.4\n3.9\n\n\n7\n4.6\n3.4\n\n\n8\n5.0\n3.4\n\n\n9\n4.4\n2.9\n\n\n10\n4.9\n3.1\n\n\n11\n5.4\n3.7\n\n\n12\n4.8\n3.4\n\n\n13\n4.8\n3.0\n\n\n14\n4.3\n3.0\n\n\n15\n5.8\n4.0\n\n\n16\n5.7\n4.4\n\n\n17\n5.4\n3.9\n\n\n18\n5.1\n3.5\n\n\n19\n5.7\n3.8\n\n\n20\n5.1\n3.8\n\n\n21\n5.4\n3.4\n\n\n22\n5.1\n3.7\n\n\n23\n4.6\n3.6\n\n\n24\n5.1\n3.3\n\n\n25\n4.8\n3.4\n\n\n26\n5.0\n3.0\n\n\n27\n5.0\n3.4\n\n\n28\n5.2\n3.5\n\n\n29\n5.2\n3.4\n\n\n30\n4.7\n3.2\n\n\n31\n4.8\n3.1\n\n\n32\n5.4\n3.4\n\n\n33\n5.2\n4.1\n\n\n34\n5.5\n4.2\n\n\n35\n4.9\n3.1\n\n\n36\n5.0\n3.2\n\n\n37\n5.5\n3.5\n\n\n38\n4.9\n3.6\n\n\n39\n4.4\n3.0\n\n\n40\n5.1\n3.4\n\n\n41\n5.0\n3.5\n\n\n42\n4.5\n2.3\n\n\n43\n4.4\n3.2\n\n\n44\n5.0\n3.5\n\n\n45\n5.1\n3.8\n\n\n46\n4.8\n3.0\n\n\n47\n5.1\n3.8\n\n\n48\n4.6\n3.2\n\n\n49\n5.3\n3.7\n\n\n50\n5.0\n3.3\n\n\n\n\n\nIf you need a list of column names run the following command:\n\ncolnames(iris)\n\n\n'Sepal.Length''Sepal.Width''Petal.Length''Petal.Width''Species'"
  },
  {
    "objectID": "getting-started/r/datasets.html#vector-math-on-datasets",
    "href": "getting-started/r/datasets.html#vector-math-on-datasets",
    "title": "Packages_and_Datasets",
    "section": "Vector math on datasets",
    "text": "Vector math on datasets\nThe Basic_Commands.Rmd tutorial covered vector operations. You can apply those examples to columns or rows of a data set.\nLet’s also introduce the cat command here. cat is used to concatenate info together.\nHere we will concat our output to ensure the meaning is clear.\n\ncolumn_of_interest = \"Sepal.Length\"\ncol_mean = mean(iris[,column_of_interest])\ncat(\"The mean of the columm\", column_of_interest, \"is: \", col_mean)\n\nThe mean of the columm Sepal.Length is:  5.843333\n\n\nTo test, your knowledge, try the following: 1. Update the above code to get the mean of another column. 2. Apply a different function than the mean. 3. Can you get summary stats for multiple columns at once?"
  },
  {
    "objectID": "getting-started/r/datasets.html#base-r-visualizations",
    "href": "getting-started/r/datasets.html#base-r-visualizations",
    "title": "Packages_and_Datasets",
    "section": "Base R Visualizations",
    "text": "Base R Visualizations\nLet’s start with a histogram showing the values of Sepal Length in the Iris dataset.\n\nhist(iris$Sepal.Length)\n\n\n\n\n\n\n\n\nNext, lets do a boxplot of Sepal Length by Iris type.\n\nboxplot(Sepal.Length ~ Species, iris, xlab = \"Species\", ylab = \"Sepal Length\")\n\n\n\n\n\n\n\n\nThe command plot will produce a scatterplot when given an two vectors (x and y).\n\nwith(iris, plot(Sepal.Length, Sepal.Width))\n\n\n\n\n\n\n\n\nWe can expand on this basic graph by adding colors for each species of iris in the dataset.\n\nwith(iris, plot(Sepal.Length, Sepal.Width, main = \"Sepal Length and Width by Species\", type = \"n\"))\nwith(subset(iris, Species == \"setosa\"), points(Sepal.Length, Sepal.Width, col = \"blue\"))\nwith(subset(iris, Species == \"virginica\"), points(Sepal.Length, Sepal.Width, col = \"red\"))\nwith(subset(iris, Species == \"versicolor\"), points(Sepal.Length, Sepal.Width, col = \"green\"))\nlegend(\"topright\", pch = 1, col = c(\"blue\", \"red\", \"green\"), legend = c(\"Setosa\", \"Virginica\", \"Versicolor\"))\n\n\n\n\n\n\n\n\nThis concludes your introduction to using built-in data sets."
  },
  {
    "objectID": "getting-started/jupyter-notebooks.html",
    "href": "getting-started/jupyter-notebooks.html",
    "title": "Jupyter Notebooks",
    "section": "",
    "text": "Jupyter Notebooks have become a staple in the data science community due to their flexibility and powerful features. Notebooks are ideal for EDA, tutorials, and teaching. By seperating code into cells and combining code, documentation, and visualizations notebooks allow for rapid iteration and interactive learning.\nJupyter Notebook Documentation\n\n\n\nInteractive Environment: Jupyter Notebooks provide an interactive environment where you can write and execute code in a step-by-step manner. This is particularly useful for data analysis and exploration.\nDocumentation and Code Together: You can include markdown cells for documentation alongside your code cells, making it easier to explain your thought process and findings.\nVisualization Integration: Jupyter Notebooks support rich outputs such as charts and graphs, which can be rendered directly within the notebook.\nReproducibility: Notebooks can be shared easily, and others can rerun the code to reproduce the results, ensuring transparency and reproducibility in research.",
    "crumbs": [
      "Pixel Process",
      "Basics",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "getting-started/jupyter-notebooks.html#jupyter-notebooks",
    "href": "getting-started/jupyter-notebooks.html#jupyter-notebooks",
    "title": "Jupyter Notebooks",
    "section": "",
    "text": "Jupyter Notebooks have become a staple in the data science community due to their flexibility and powerful features. Notebooks are ideal for EDA, tutorials, and teaching. By seperating code into cells and combining code, documentation, and visualizations notebooks allow for rapid iteration and interactive learning.\nJupyter Notebook Documentation\n\n\n\nInteractive Environment: Jupyter Notebooks provide an interactive environment where you can write and execute code in a step-by-step manner. This is particularly useful for data analysis and exploration.\nDocumentation and Code Together: You can include markdown cells for documentation alongside your code cells, making it easier to explain your thought process and findings.\nVisualization Integration: Jupyter Notebooks support rich outputs such as charts and graphs, which can be rendered directly within the notebook.\nReproducibility: Notebooks can be shared easily, and others can rerun the code to reproduce the results, ensuring transparency and reproducibility in research.",
    "crumbs": [
      "Pixel Process",
      "Basics",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "getting-started/jupyter-notebooks.html#visual-overview",
    "href": "getting-started/jupyter-notebooks.html#visual-overview",
    "title": "Jupyter Notebooks",
    "section": "Visual Overview",
    "text": "Visual Overview\n\n\n  \n    \n      \n    \n    \n    ← Previous\n    Next →",
    "crumbs": [
      "Pixel Process",
      "Basics",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "getting-started/jupyter-notebooks.html#cell-types",
    "href": "getting-started/jupyter-notebooks.html#cell-types",
    "title": "Jupyter Notebooks",
    "section": "Cell Types",
    "text": "Cell Types\nJupyter Notebooks consist of cells, which are the building blocks of the notebook. There are three main types of cells:\n\nCode Cells: Used to write and execute Python code. The output of the code, such as results or visualizations, appears directly below the cell.\nMarkdown Cells: Used to write formatted text using Markdown syntax. These cells are perfect for adding explanations, headers, lists, links, and other documentation.\nRaw Cells: Used to write plain text that is not processed by the notebook. These cells are rarely used and are generally for specific formats or custom content.",
    "crumbs": [
      "Pixel Process",
      "Basics",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "getting-started/jupyter-notebooks.html#executing-code",
    "href": "getting-started/jupyter-notebooks.html#executing-code",
    "title": "Jupyter Notebooks",
    "section": "Executing Code",
    "text": "Executing Code\nTo execute a code cell, you can use one of the following methods:\n\nKeyboard Shortcuts: Press Shift + Enter to run the current cell and move to the next cell. Press Ctrl + Enter to run the current cell and stay in the same cell.\nToolbar Buttons: Click the Run button in the toolbar to execute the selected cell.\n\nThe output of the code, whether it’s text, tables, or visualizations, will be displayed directly below the cell.",
    "crumbs": [
      "Pixel Process",
      "Basics",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "getting-started/jupyter-notebooks.html#kernels",
    "href": "getting-started/jupyter-notebooks.html#kernels",
    "title": "Jupyter Notebooks",
    "section": "Kernels",
    "text": "Kernels\nA kernel is a computational engine that executes the code contained in the notebook. Here’s how to work with kernels in Jupyter Notebooks:\n\nStarting a Kernel: When you open a Jupyter Notebook, a kernel is automatically started. You can see the kernel status in the top-right corner of the notebook interface.\nChanging Kernels: If you want to change the kernel (e.g., switch from Python 3 to another kernel), you can do so from the Kernel menu. Select Change kernel and choose the desired kernel from the list.\nRestarting a Kernel: Sometimes you may need to restart the kernel, especially if it becomes unresponsive or if you want to clear all outputs and start fresh. You can restart the kernel from the Kernel menu by selecting Restart.",
    "crumbs": [
      "Pixel Process",
      "Basics",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "getting-started/jupyter-notebooks.html#additional-features",
    "href": "getting-started/jupyter-notebooks.html#additional-features",
    "title": "Jupyter Notebooks",
    "section": "Additional Features",
    "text": "Additional Features\nJupyter Notebooks offer several other features that enhance the data science workflow:\n\nExtensions and Widgets: There are many extensions and widgets available to extend the functionality of Jupyter Notebooks, such as interactive widgets (ipywidgets), and extensions for improved usability (JupyterLab extensions).\nVersion Control: You can use version control systems like Git to track changes in your notebooks, making it easier to collaborate and maintain a history of your work.\nExport Options: Notebooks can be exported to various formats, including HTML, PDF, and slideshows, making it easy to share your work with others.\n\nJupyter Notebooks provide an excellent environment for data scientists to perform exploratory data analysis, visualize data, and document their findings. By combining code, documentation, and visualizations in a single document, Jupyter Notebooks enhance the efficiency and reproducibility of data science projects.",
    "crumbs": [
      "Pixel Process",
      "Basics",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "foundations/data-viz/data-viz-building-blocks.html",
    "href": "foundations/data-viz/data-viz-building-blocks.html",
    "title": "Visualization Building Blocks",
    "section": "",
    "text": "Establish terminology for building figures with comparisons of matplotlib, seaborn, and plotly.\nBonus: Aviod common pitfalls\n\nFigure\nFigure: The overall container that holds everything in a visualization, including one or more plots/axes.\n\n\n\nmatplotlib\n\nCreated with plt.figure() or implicitly with plt.subplots(). Acts as the top-level container for all axes and elements.\n\n\n\nseaborn\n\nWraps matplotlib; figure objects are matplotlib figures. Many seaborn functions automatically create a figure.\n\n\n\nplotly\n\nCreated with go.Figure() or make_subplots(). Serves as the top-level object to which traces and layout are added.\n\n\n\nPitfalls\nIn matplotlib/seaborn, you usually need plt.show() in scripts to render; in notebooks it auto-displays. In plotly, you must call fig.show(). Seaborn may create its own figure silently if you don’t pass ax=.... Plotly figures are interactive by default, while matplotlib’s are static unless combined with widgets.\n\n\n\nAxes\nAxes: A single plotting area within a figure (what most people think of as ‘the plot’).\n\n\n\nmatplotlib\n\nCreated with fig.add_subplot() or plt.subplots(). Each Axes has its own coordinate system, ticks, and labels.\n\n\n\nseaborn\n\nAlways returns a matplotlib Axes (or FacetGrid/PairGrid, which manages multiple Axes). Seaborn’s high-level API plots directly onto Axes objects.\n\n\n\nplotly\n\nDoesn’t use an explicit ‘Axes’ object. Instead, axes are defined by layout (fig.update_xaxes, fig.update_yaxes). Multiple subplots are managed through layout specs.\n\n\n\nPitfalls\nTerminology confusion: ‘Axes’ (plural of ‘Axis’) vs ‘Axis’ (x/y scale). In seaborn, calling multiple functions on the same Axes requires passing ax=...; otherwise each call may create a new figure. In plotly, axis limits and labels are adjusted via layout updates, not per-object methods.\n\n\n\nColor Palettes\nColor Palette: Collection of colors used to distinguish categories or continuous values.\n\n\n\nmatplotlib\n\nUses ‘colormaps’ (cmap) for continuous data (e.g., viridis, plasma) and ‘color cycles’ for categorical data.\n\n\n\nseaborn\n\nBuilt-in support for categorical palettes (color_palette('deep')), sequential/diverging palettes (cubehelix_palette, light_palette), and wrappers for matplotlib colormaps.\n\n\n\nplotly\n\nSupports color_discrete_sequence for categorical colors and colorscale for continuous values. Many named scales available (Viridis, Cividis, Plasma).\n\n\n\nPitfalls\nIn matplotlib, cmap is only for continuous values; using it for categories requires manual mapping. Seaborn will override matplotlib’s default color cycle when you call sns.set_theme(). Plotly distinguishes between discrete (color_discrete_sequence) and continuous (colorscale); passing the wrong one has no effect.\n\n\n\nTemplates / Styles\nTemplates / Styles: Preset styling that controls background, grids, font, and overall look of plots.\n\n\n\nmatplotlib\n\nUses ‘colormaps’ (cmap) for continuous data (e.g., viridis, plasma) and ‘color cycles’ for categorical data.\n\n\n\nseaborn\n\nThemes via sns.set_theme(style='whitegrid'). Controls background, ticks, grid, font scale, etc.\n\n\n\nplotly\n\nTemplates applied with fig.update_layout(template='plotly_dark'). Includes built-in templates (plotly, ggplot2, seaborn) or user-defined ones.\n\n\n\nPitfalls\nMatplotlib stylesheets affect all following plots unless reset — beginners often forget to reset. Seaborn’s set_theme changes matplotlib defaults globally (grids, ticks, etc.), which may surprise users mixing raw matplotlib code. Plotly templates override both colors and layout; you may need to re-apply custom settings after applying a template."
  },
  {
    "objectID": "foundations/data-viz/data-viz-building-blocks-nb.html",
    "href": "foundations/data-viz/data-viz-building-blocks-nb.html",
    "title": "Data Visualization Building Blocks",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n\n# Load built-in tips dataset\ntips = sns.load_dataset(\"tips\")\ntips.head()\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\n\nagg = tips.groupby(\"day\", as_index=False)[\"total_bill\"].mean()\n\n\nMatplotlib\n\nplt.bar(tips['day'], tips['total_bill'])\nplt.title(\"Total Bill by Day\")\nplt.xlabel(\"Day\")\nplt.ylabel(\"Total Bill ($)\")\nplt.tight_layout()\nplt.savefig(\"matplotlib.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSeaborn\n\nfig, ax = plt.subplots()\nsns.barplot(data=tips, x=\"day\", y=\"total_bill\", ax=ax)\nax.set_title(\"Average Bill by Day\", fontsize=14)\nax.set_xlabel(\"Day\", fontsize=12)\nax.set_ylabel(\"Total Bill\", fontsize=12)\nplt.tight_layout()\nplt.savefig(\"seaborn_chart.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPlotly\n\nfig = px.bar(agg, x=\"day\", y=\"total_bill\", title=\"Total Bill by Day\")\nfig.update_layout(title_font_size=18, xaxis_title=\"Day\", yaxis_title=\"Total Bill\")\nfig.write_image(\"plotly_chart.png\")  # Requires kaleido\nfig.show()"
  },
  {
    "objectID": "foundations/index.html",
    "href": "foundations/index.html",
    "title": "Foundations",
    "section": "",
    "text": "Every good project rests on strong foundations — and in coding, that means understanding the core tools and patterns that show up everywhere. This section focuses on the building blocks of real-world programming: from reading and writing data to structuring reusable code and visualizing results clearly. You’ll work with files, packages, functions, and figures — the essential tools that help you do more with less.\nBuild strong. Build smart.",
    "crumbs": [
      "Pixel Process",
      "Foundations"
    ]
  },
  {
    "objectID": "foundations/index.html#foundations-topics",
    "href": "foundations/index.html#foundations-topics",
    "title": "Foundations",
    "section": "Foundations Topics",
    "text": "Foundations Topics\n\nDatasets: Read, write, and manage data in common tabluar with tools like pandas.\nData Visualization: Communicate results through charts, plots, and custom visuals.\nAdvanced Data: Learn the basics of working with complex data such as images.\n\nNeed a place to start? Check out the FAQs!\n\n\n\nDatasets\n\n\n\n Dataset Exploration → Guide to evaluating tabluar datasets\n\n\n Dataset Exploration (NB) → Verify data dimensions, datatypes, and quality\n\n\n\n\n\nData Visualization\n\n\n\n Data Visualization Basics → Learn about turning data into stories\n\n\n Data Visualization Basics (NB) → Start turning data into stories\n\n\n Data Viz Building Blocks → Core features behind data viz\n\n\n Data Viz Building Blocks (NB) → Code example for core features behind data viz\n\n\n\n\n\nAdvanced Data\n\n\n\n Image Basics (NB) → Notebook on basic image properties\n\n\n\n\n\n\n\n\n\n\n\nStart Your Journey\n\n\n\nI used to think I was  because my code ran and I (usually) understood it…then I got a job. Most real-world projects depend on shared understanding: conventions, documentation, and structure that make your code easy to follow, reuse, and extend — it’s about collaboration.\nLearn patterns — resolving the same issues wastes time, solve recurring issues in systematic ways\nBuild habits — implementing workflows, tools, and standards that work reduces headaches and elevates returns\nGrow confident — solid practices and foundations make projects extendible, code resuable, and enthusiasts experts\n\n\n\nCode is read much more often than it is written\n\nIt’s easy to dabble in code — to test the waters, try a few tricks, move on. But learning best practices is how you go from dipping your toes to diving deep. Professionals do not just code for themselves.",
    "crumbs": [
      "Pixel Process",
      "Foundations"
    ]
  },
  {
    "objectID": "foundations/datasets/dataset-exploration-nb.html",
    "href": "foundations/datasets/dataset-exploration-nb.html",
    "title": "Dataset Exploration",
    "section": "",
    "text": "Notebook goals:",
    "crumbs": [
      "Pixel Process",
      "Datasets",
      "Dataset Exploration"
    ]
  },
  {
    "objectID": "foundations/datasets/dataset-exploration-nb.html#deeper-dive-skim",
    "href": "foundations/datasets/dataset-exploration-nb.html#deeper-dive-skim",
    "title": "Dataset Exploration",
    "section": "Deeper Dive: Skim",
    "text": "Deeper Dive: Skim\nSkimpy is a package that profiles a df with visually digestible information.\n\nfrom skimpy import skim\n\n\nskim(iris)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ Dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 150    │ │ float64     │ 4     │                                                          │\n│ │ Number of columns │ 5      │ │ string      │ 1     │                                                          │\n│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━━━━┓  │\n│ ┃ column            ┃ NA   ┃ NA %   ┃ mean     ┃ sd        ┃ p0    ┃ p25   ┃ p50    ┃ p75  ┃ p100  ┃ hist    ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━━━━┩  │\n│ │ sepal_length      │    0 │      0 │    5.843 │    0.8281 │   4.3 │   5.1 │    5.8 │  6.4 │   7.9 │ ▃██▇▅▂  │  │\n│ │ sepal_width       │    0 │      0 │    3.057 │    0.4359 │     2 │   2.8 │      3 │  3.3 │   4.4 │ ▁▇█▇▂▁  │  │\n│ │ petal_length      │    0 │      0 │    3.758 │     1.765 │     1 │   1.6 │   4.35 │  5.1 │   6.9 │ █ ▂▇▆▂  │  │\n│ │ petal_width       │    0 │      0 │    1.199 │    0.7622 │   0.1 │   0.3 │    1.3 │  1.8 │   2.5 │ █ ▂▆▄▄  │  │\n│ └───────────────────┴──────┴────────┴──────────┴───────────┴───────┴───────┴────────┴──────┴───────┴─────────┘  │\n│                                                     string                                                      │\n│ ┏━━━━━━━━━┳━━━━┳━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓  │\n│ ┃         ┃    ┃      ┃          ┃            ┃        ┃           ┃ chars per   ┃ words per   ┃             ┃  │\n│ ┃ column  ┃ NA ┃ NA % ┃ shortest ┃ longest    ┃ min    ┃ max       ┃ row         ┃ row         ┃ total words ┃  │\n│ ┡━━━━━━━━━╇━━━━╇━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩  │\n│ │ species │  0 │    0 │ setosa   │ versicolor │ setosa │ virginica │        8.33 │           1 │         150 │  │\n│ └─────────┴────┴──────┴──────────┴────────────┴────────┴───────────┴─────────────┴─────────────┴─────────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\n\nskim(titanic)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types               Categories                                        │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓ ┏━━━━━━━━━━━━━━━━━━━━━━━┓                                │\n│ ┃ Dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃ ┃ Categorical Variables ┃                                │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩ ┡━━━━━━━━━━━━━━━━━━━━━━━┩                                │\n│ │ Number of rows    │ 891    │ │ string      │ 5     │ │ class                 │                                │\n│ │ Number of columns │ 15     │ │ int64       │ 4     │ │ deck                  │                                │\n│ └───────────────────┴────────┘ │ float64     │ 2     │ └───────────────────────┘                                │\n│                                │ category    │ 2     │                                                          │\n│                                │ bool        │ 2     │                                                          │\n│                                └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━┳━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━┳━━━━━━━┳━━━━━━━━┓  │\n│ ┃ column    ┃ NA   ┃ NA %                 ┃ mean    ┃ sd      ┃ p0    ┃ p25   ┃ p50   ┃ p75 ┃ p100  ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━╇━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━╇━━━━━━━╇━━━━━━━━┩  │\n│ │ survived  │    0 │                    0 │  0.3838 │  0.4866 │     0 │     0 │     0 │   1 │     1 │ █    ▅ │  │\n│ │ pclass    │    0 │                    0 │   2.309 │  0.8361 │     1 │     2 │     3 │   3 │     3 │ ▄  ▃ █ │  │\n│ │ age       │  177 │   19.865319865319865 │    29.7 │   14.53 │  0.42 │ 20.12 │    28 │  38 │    80 │ ▂██▃▁  │  │\n│ │ sibsp     │    0 │                    0 │   0.523 │   1.103 │     0 │     0 │     0 │   1 │     8 │   █    │  │\n│ │ parch     │    0 │                    0 │  0.3816 │  0.8061 │     0 │     0 │     0 │   0 │     6 │  █▁▁   │  │\n│ │ fare      │    0 │                    0 │    32.2 │   49.69 │     0 │  7.91 │ 14.45 │  31 │ 512.3 │   █    │  │\n│ └───────────┴──────┴──────────────────────┴─────────┴─────────┴───────┴───────┴───────┴─────┴───────┴────────┘  │\n│                                                    category                                                     │\n│ ┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓  │\n│ ┃ column          ┃ NA        ┃ NA %                                    ┃ ordered           ┃ unique         ┃  │\n│ ┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩  │\n│ │ class           │         0 │                                       0 │ False             │              3 │  │\n│ │ deck            │       688 │                       77.21661054994388 │ False             │              8 │  │\n│ └─────────────────┴───────────┴─────────────────────────────────────────┴───────────────────┴────────────────┘  │\n│                                                      bool                                                       │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column                            ┃ true            ┃ true rate                     ┃ hist                 ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩  │\n│ │ adult_male                        │             537 │                           0.6 │        ▅    █        │  │\n│ │ alone                             │             537 │                           0.6 │        ▅    █        │  │\n│ └───────────────────────────────────┴─────────────────┴───────────────────────────────┴──────────────────────┘  │\n│                                                     string                                                      │\n│ ┏━━━━━━━━━━━┳━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┓  │\n│ ┃           ┃    ┃           ┃          ┃           ┃          ┃           ┃ chars    ┃ words per ┃ total    ┃  │\n│ ┃ column    ┃ NA ┃ NA %      ┃ shortest ┃ longest   ┃ min      ┃ max       ┃ per row  ┃ row       ┃ words    ┃  │\n│ ┡━━━━━━━━━━━╇━━━━╇━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━┩  │\n│ │ sex       │  0 │         0 │ male     │ female    │ female   │ male      │      4.7 │         1 │      891 │  │\n│ │ embarked  │  2 │ 0.2244668 │ S        │ S         │ C        │ S         │        1 │         1 │      889 │  │\n│ │           │    │ 911335578 │          │           │          │           │          │           │          │  │\n│ │ who       │  0 │         0 │ man      │ woman     │ child    │ woman     │     3.79 │         1 │      891 │  │\n│ │ embark_to │  2 │ 0.2244668 │ Cherbour │ Southampt │ Cherbour │ Southampt │     10.5 │         1 │      889 │  │\n│ │ wn        │    │ 911335578 │ g        │ on        │ g        │ on        │          │           │          │  │\n│ │ alive     │  0 │         0 │ no       │ yes       │ no       │ yes       │     2.38 │         1 │      891 │  │\n│ └───────────┴────┴───────────┴──────────┴───────────┴──────────┴───────────┴──────────┴───────────┴──────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯",
    "crumbs": [
      "Pixel Process",
      "Datasets",
      "Dataset Exploration"
    ]
  },
  {
    "objectID": "foundations/datasets/dataset-exploration-nb.html#deeper-dive-missingno",
    "href": "foundations/datasets/dataset-exploration-nb.html#deeper-dive-missingno",
    "title": "Dataset Exploration",
    "section": "Deeper Dive: missingno",
    "text": "Deeper Dive: missingno\nMissing data matters, and just if it exists. Relationships between missing data can heavily impact analysis results.\nmissingno provides insightful and easy insight into missing data.\nAll below examples focus on the titantic df. Iris has no missing values.\n\nimport missingno as msno\n\n\nmsno.matrix(titanic)\n\n\n\n\n\n\n\n\n\nmsno.bar(titanic);",
    "crumbs": [
      "Pixel Process",
      "Datasets",
      "Dataset Exploration"
    ]
  },
  {
    "objectID": "foundations/advanced-data/image-basics.html",
    "href": "foundations/advanced-data/image-basics.html",
    "title": "Image Basics",
    "section": "",
    "text": "Notebook goals:\nNote: PIL is better for working with image data in many cases, however the use of np arrays can faciiliate querying the data and understanding what makes an image.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n# Set display size for all images in the notebook\nfrom IPython.display import display, HTML\ndisplay(HTML(\"&lt;style&gt;.jp-RenderedImage { max-width: 400px; max-height: 400px; }&lt;/style&gt;\"))",
    "crumbs": [
      "Pixel Process",
      "Advanced Data",
      "Image Basics"
    ]
  },
  {
    "objectID": "foundations/advanced-data/image-basics.html#meet-pixpix",
    "href": "foundations/advanced-data/image-basics.html#meet-pixpix",
    "title": "Image Basics",
    "section": "Meet Pixpix",
    "text": "Meet Pixpix\nThis little pixie contains over 4 million values!\nWe’ll load the image and begin exploring the properties the create this “simple” image.\n\nLoad Image into Multiple Formats\nVariable names will indicate the data format, and numeric values the steps along the way.\nRemember the type command in Python can always confirm the data type.\n\npwd\n\n'/Users/dsl/Documents/Work/pixel-process/foundations'\n\n\n\nimage_path = ('../assets/images/pixpix.png')\n\n\n# Load image as a PIL Image\nimg_0 = Image.open(image_path)\n\n\n# Display image\nimg_0\n\n\n\n\n\n\n\n\n\ntype(img_0)\n\nPIL.PngImagePlugin.PngImageFile\n\n\n\n\nCreate an Array Version\n\narr_0 = np.array(img_0)\n\n\ntype(arr_0)\n\nnumpy.ndarray\n\n\n\n# np.size will return the total number of elements in an array\nnp.size(arr_0)\n\n4194304\n\n\n\nplt.imshow(arr_0)",
    "crumbs": [
      "Pixel Process",
      "Advanced Data",
      "Image Basics"
    ]
  },
  {
    "objectID": "foundations/advanced-data/image-basics.html#image-properties",
    "href": "foundations/advanced-data/image-basics.html#image-properties",
    "title": "Image Basics",
    "section": "Image Properties",
    "text": "Image Properties\nLet’s see what data exists for Pixpix in these two formats.\n\nNumpy Array\nShape provides the number of dimensions and element count for each.\nThis image consists of a 1024x1024 pixel grid, with 4 values per pixel.\n\narr_0.shape\n\n(1024, 1024, 4)\n\n\nArrays dimensions and elements can be selected with [] notation.\nFor long arrays, the head and tail will be shown.\nAlso note, a shape and data type is shown for each.\n\narr_0[0]\n\narray([[20, 16, 17,  0],\n       [19, 16, 16,  0],\n       [19, 16, 16,  0],\n       ...,\n       [10,  9, 11,  0],\n       [11,  9, 11,  0],\n       [ 9,  9, 10,  0]], shape=(1024, 4), dtype=uint8)\n\n\n\narr_0[1]\n\narray([[19, 16, 16,  0],\n       [18, 16, 15,  0],\n       [20, 16, 18,  0],\n       ...,\n       [ 8,  8,  9,  0],\n       [ 9, 10, 11,  0],\n       [10, 11, 11,  0]], shape=(1024, 4), dtype=uint8)\n\n\n\n# Remember, Python uses a 0 based indexing system\n# So arr_0[0] is valid, but the below fails\narr_0[1024]\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[15], line 3\n      1 # Remember, Python uses a 0 based indexing system\n      2 # So arr_0[0] is valid, but the below fails\n----&gt; 3 arr_0[1024]\n\nIndexError: index 1024 is out of bounds for axis 0 with size 1024\n\n\n\n\n\nChained Indexing\nIndexing can be used to create new subset arrays, to assign values, or chained to drill down into data.\n\n# These 4 values create the first pixel (top left corner)\narr_0[0][0]\n\narray([20, 16, 17,  0], dtype=uint8)\n\n\n\narr_0[0][0][0]\n\nnp.uint8(20)\n\n\n\n\nSubset Arrays\nThis is functionally equivalent to the above.\nNot the proper approach depends on goals.\n\narr_0[0]\n\narray([[20, 16, 17,  0],\n       [19, 16, 16,  0],\n       [19, 16, 16,  0],\n       ...,\n       [10,  9, 11,  0],\n       [11,  9, 11,  0],\n       [ 9,  9, 10,  0]], shape=(1024, 4), dtype=uint8)\n\n\n\narr_0_0 = arr_0[0]\n\n\narr_0_0\n\narray([[20, 16, 17,  0],\n       [19, 16, 16,  0],\n       [19, 16, 16,  0],\n       ...,\n       [10,  9, 11,  0],\n       [11,  9, 11,  0],\n       [ 9,  9, 10,  0]], shape=(1024, 4), dtype=uint8)\n\n\n\nprint(f\"Shape of arr_0: {arr_0.shape}\")\nprint(f\"Shape of arr_0[0]: {arr_0[0].shape}\")\nprint(f\"Shape of arr_0_0: {arr_0_0.shape}\")\n\nShape of arr_0: (1024, 1024, 4)\nShape of arr_0[0]: (1024, 4)\nShape of arr_0_0: (1024, 4)\n\n\n\n\nPIL Image\nThe img_0 stores the same pixel information, but in a different format.\nSee the PIL Docs for more details!\n\ntype(img_0)\n\nPIL.PngImagePlugin.PngImageFile\n\n\n\nprint(f\"Format: {img_0.format}\")\nprint(f\"Mode: {img_0.mode}\")\nprint(f\"Size (WxH): {img_0.size}\")\nprint(f\"Total Pixels: {img_0.size[0] * img_0.size[1]}\")\n\nFormat: PNG\nMode: RGBA\nSize (WxH): (1024, 1024)\nTotal Pixels: 1048576\n\n\nHere we see that the image is a png, matching the input file type.\nThe mode is red, green, blue, alpha (RGBA). These are the 4 values that define each pixel-more on this later.\nThe pixel grid is 1024x1024, meaning over a million total pixels.",
    "crumbs": [
      "Pixel Process",
      "Advanced Data",
      "Image Basics"
    ]
  },
  {
    "objectID": "foundations/advanced-data/image-basics.html#reducing-complexity",
    "href": "foundations/advanced-data/image-basics.html#reducing-complexity",
    "title": "Image Basics",
    "section": "Reducing Complexity",
    "text": "Reducing Complexity\n\nCreate a greyscale version\nDrop pixels\nFilter values\n\n\n# Convert to greyscale\nimg_1 = img_0.convert(\"L\")\n\n\ntype(img_1)\n\nPIL.Image.Image\n\n\n\nimg_1\n\n\n\n\n\n\n\n\n\nConvert to Array\nShow with plt.imshow\n\narr_1 = np.array(img_1)\n\n\nplt.imshow(arr_1)\n\n\n\n\n\n\n\n\nWhy isn’t the image in greyscale?\nBy default, imshow does not use a greyscale to map values.\nBelow we see how the colors map and then apply the proper color scale.\n\nplt.imshow(arr_1)\nplt.colorbar()\n\n\n\n\n\n\n\n\n\nplt.imshow(arr_1, cmap='grey')\n\n\n\n\n\n\n\n\n\n\n25% Less Data\nLet’s inspect how this transformation changed the data.\nWe’ll compare arr_0 (original) and arr_1 (greyscale).\n\nprint(f\"Original shape: {arr_0.shape}\")\nprint(f\"Original size: {arr_0.size}\")\nprint(f\"Greyscale shape: {arr_1.shape}\")\nprint(f\"Greyscale shape: {arr_1.size}\")\n\nOriginal shape: (1024, 1024, 4)\nOriginal size: 4194304\nGreyscale shape: (1024, 1024)\nGreyscale shape: 1048576\n\n\n\n100*(arr_1.size/arr_0.size)\n\n25.0\n\n\n\n\nColor Channel Manipulation\nThe original image is in RGBA, providing 4 values/pixel.\nRed, Green, Blue, and Alpha.\nAlpha controls transparency.\n\nRemove Alpha\n\narr_2 = arr_0[:, :, :3]\n\n\nprint(f\"RGBA shape: {arr_0.shape}\")\nprint(f\"RGBA size: {arr_0.size}\")\nprint(f\"RGB shape: {arr_2.shape}\")\nprint(f\"RGB shape: {arr_2.size}\")\n\nRGBA shape: (1024, 1024, 4)\nRGBA size: 4194304\nRGB shape: (1024, 1024, 3)\nRGB shape: 3145728\n\n\n\nplt.imshow(arr_0)\nplt.show()\nplt.imshow(arr_2)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWithout alpha to control transparency, all RGB colors are solid\n\nred_only = arr_2.copy()\nred_only[:, :, 1] = 0   # zero green\nred_only[:, :, 2] = 0   # zero blue\n\n\ngreen_only = arr_2.copy()\ngreen_only[:, :, 0] = 0 # zero red\ngreen_only[:, :, 2] = 0 # zero blue\n\n\nblue_only = arr_2.copy()\nblue_only[:, :, 0] = 0  # zero red\nblue_only[:, :, 1] = 0  # zero green\n\n\n# Make copies for maxed-out channels\nred_max = arr_2.copy()\nred_max[:, :, 0] = 255\n\ngreen_max = arr_2.copy()\ngreen_max[:, :, 1] = 255\n\nblue_max = arr_2.copy()\nblue_max[:, :, 2] = 255\n\n\n# Plot results\nfig, axes = plt.subplots(2, 4, figsize=(9, 6))\n\naxes[0, 0].imshow(arr_2);   axes[0, 0].set_title(\"RGB\")\naxes[0, 1].imshow(red_only);   axes[0, 1].set_title(\"Red only\")\naxes[0, 2].imshow(green_only); axes[0, 2].set_title(\"Green only\")\naxes[0, 3].imshow(blue_only);  axes[0, 3].set_title(\"Blue only\")\n\naxes[1, 0].imshow(arr_2);   axes[1, 0].set_title(\"RGB\")\naxes[1, 1].imshow(red_max);    axes[1, 1].set_title(\"Red maxed\")\naxes[1, 2].imshow(green_max);  axes[1, 2].set_title(\"Green maxed\")\naxes[1, 3].imshow(blue_max);   axes[1, 3].set_title(\"Blue maxed\")\n\nfor ax in axes.flat:\n    ax.axis(\"off\")\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Pixel Process",
      "Advanced Data",
      "Image Basics"
    ]
  },
  {
    "objectID": "jump-in/jl-notebooks/jl-build/notebooks/jupyter-lite.html",
    "href": "jump-in/jl-notebooks/jl-build/notebooks/jupyter-lite.html",
    "title": "jupyter-lite.ipynb",
    "section": "",
    "text": "This notebook is the preferred source of site-specific runtime configuration for a JupyterLite app, and will override any configuration in jupyter-lite.json."
  },
  {
    "objectID": "jump-in/jl-notebooks/jl-build/notebooks/jupyter-lite.html#editing-configuration",
    "href": "jump-in/jl-notebooks/jl-build/notebooks/jupyter-lite.html#editing-configuration",
    "title": "jupyter-lite.ipynb",
    "section": "Editing Configuration",
    "text": "Editing Configuration\nThe configuration is stored in this Notebook’s metadata under the jupyter-lite key. To edit the configuration in JupyterLab.\n\nopen the Property Inspector sidebar\nexpand the Advanced Tools section\nedit the jupyter-lite metadata sub-key\npress the “check” icon\nsave the notebook"
  },
  {
    "objectID": "jump-in/jl-notebooks/jl-build/files/datatypes-and-operators-nb.html",
    "href": "jump-in/jl-notebooks/jl-build/files/datatypes-and-operators-nb.html",
    "title": "Datatypes and Operators - Notebook",
    "section": "",
    "text": "4+5\n\n9\n\n\n\nprint('Hello')\n\nHello",
    "crumbs": [
      "Pixel Process",
      "Datatypes and Operators - Notebook"
    ]
  },
  {
    "objectID": "jump-in/jl-notebooks/jl-build/edit/jupyter-lite.html",
    "href": "jump-in/jl-notebooks/jl-build/edit/jupyter-lite.html",
    "title": "jupyter-lite.ipynb",
    "section": "",
    "text": "This notebook is the preferred source of site-specific runtime configuration for a JupyterLite app, and will override any configuration in jupyter-lite.json."
  },
  {
    "objectID": "jump-in/jl-notebooks/jl-build/edit/jupyter-lite.html#editing-configuration",
    "href": "jump-in/jl-notebooks/jl-build/edit/jupyter-lite.html#editing-configuration",
    "title": "jupyter-lite.ipynb",
    "section": "Editing Configuration",
    "text": "Editing Configuration\nThe configuration is stored in this Notebook’s metadata under the jupyter-lite key. To edit the configuration in JupyterLab.\n\nopen the Property Inspector sidebar\nexpand the Advanced Tools section\nedit the jupyter-lite metadata sub-key\npress the “check” icon\nsave the notebook"
  },
  {
    "objectID": "jump-in/jl-notebooks/jl-build/repl/jupyter-lite.html",
    "href": "jump-in/jl-notebooks/jl-build/repl/jupyter-lite.html",
    "title": "jupyter-lite.ipynb",
    "section": "",
    "text": "This notebook is the preferred source of site-specific runtime configuration for a JupyterLite app, and will override any configuration in jupyter-lite.json."
  },
  {
    "objectID": "jump-in/jl-notebooks/jl-build/repl/jupyter-lite.html#editing-configuration",
    "href": "jump-in/jl-notebooks/jl-build/repl/jupyter-lite.html#editing-configuration",
    "title": "jupyter-lite.ipynb",
    "section": "Editing Configuration",
    "text": "Editing Configuration\nThe configuration is stored in this Notebook’s metadata under the jupyter-lite key. To edit the configuration in JupyterLab.\n\nopen the Property Inspector sidebar\nexpand the Advanced Tools section\nedit the jupyter-lite metadata sub-key\npress the “check” icon\nsave the notebook"
  },
  {
    "objectID": "jump-in/jl-notebooks/jl-build/tree/jupyter-lite.html",
    "href": "jump-in/jl-notebooks/jl-build/tree/jupyter-lite.html",
    "title": "jupyter-lite.ipynb",
    "section": "",
    "text": "This notebook is the preferred source of site-specific runtime configuration for a JupyterLite app, and will override any configuration in jupyter-lite.json."
  },
  {
    "objectID": "jump-in/jl-notebooks/jl-build/tree/jupyter-lite.html#editing-configuration",
    "href": "jump-in/jl-notebooks/jl-build/tree/jupyter-lite.html#editing-configuration",
    "title": "jupyter-lite.ipynb",
    "section": "Editing Configuration",
    "text": "Editing Configuration\nThe configuration is stored in this Notebook’s metadata under the jupyter-lite key. To edit the configuration in JupyterLab.\n\nopen the Property Inspector sidebar\nexpand the Advanced Tools section\nedit the jupyter-lite metadata sub-key\npress the “check” icon\nsave the notebook"
  },
  {
    "objectID": "jump-in/jl-notebooks/files/variables-and-functions-nb.html",
    "href": "jump-in/jl-notebooks/files/variables-and-functions-nb.html",
    "title": "Variables and Functions - Notebook",
    "section": "",
    "text": "Any programmer knows, the first test to show if you know a language it printing Hello, World!\nThis interactive notebook uses only pure Python, no external resources to get started.\nTopics covered:\n\nHello World\nTypes (int, float, str, bool, list, dict, set, tuple)\nVariables & assignment\nOperators (arithmetic, comparison, logical)\n\n\nprint(\"Hello, World!\")\n\nHello, World!\n\n\nThere are two parts to the syntax above: - print is a builtin function - “Hello, World!” is a string\nWhen print is given a single, string argument like above, it displays that input.\nNext, give yourself a little praise, you’re on your way to mastering Python and more.\nSet up two variables to work with: - compliment - name\n\ncompliment = \"Great work\"\n\n\nname = \"Your name here\"\n\n\nprint(compliment)\nprint(name)\n\nGreat work\nYour name here\n\n\n\n\nVariables are essential to programming. A variable creates a named container for information.\nVariable names can only contain letters, digits, and underscores, must start with a letter or underscore, and can’t be a reserved word (e.g., if, def, return).\nIn Python, use = to assign values: x = 5.\nCreating and working with variables makes programming much easier!\nIt creates a reusable, custom name.\n\nprint(compliment)\nprint(name)\n\nGreat work\nYour name here\n\n\nSomething about the above looks off…\nIt should be on one line.\n\ncompliment_and_name = compliment + name\n\n\nprint(compliment_and_name)\n\nGreat workYour name here\n\n\nStill not quite right…\n\ncompliment_and_name_2 = compliment +', '+ name\n\n\nprint(compliment_and_name_2)\n\nGreat work, Your name here\n\n\nWith feeling!\n\ncompliment_and_name_3 = compliment_and_name_2 +'!'\n\n\nprint(compliment_and_name_3)\n\nGreat work, Your name here!\n\n\n\n\n\nNow you have not only created variables you concatenated them with an operator +.\nKnowing how to manipulate variables is key to understanding programming.\n\n\n\n\n\n\n\n\nExample\nValid?\nReason\n\n\n\n\nage\n✅ Yes\nStarts with a letter, letters only\n\n\nuser_name\n✅ Yes\nLetters + underscore are allowed\n\n\ntemperature2\n✅ Yes\nDigits allowed, but not at start\n\n\n_hidden\n✅ Yes\nLeading underscore allowed (often used for “private” values)\n\n\nMAX_VALUE\n✅ Yes\nUppercase is allowed, often used for constants\n\n\n2nd_value\n❌ No\nCannot start with a digit\n\n\nuser-name\n❌ No\nDash (-) not allowed\n\n\nfile.path\n❌ No\nDot (.) means “attribute,” not part of a name\n\n\nmy var\n❌ No\nSpaces not allowed\n\n\nreturn\n❌ No\nreturn is a reserved Python keyword"
  },
  {
    "objectID": "jump-in/jl-notebooks/files/variables-and-functions-nb.html#hello-world",
    "href": "jump-in/jl-notebooks/files/variables-and-functions-nb.html#hello-world",
    "title": "Variables and Functions - Notebook",
    "section": "",
    "text": "Any programmer knows, the first test to show if you know a language it printing Hello, World!\nThis interactive notebook uses only pure Python, no external resources to get started.\nTopics covered:\n\nHello World\nTypes (int, float, str, bool, list, dict, set, tuple)\nVariables & assignment\nOperators (arithmetic, comparison, logical)\n\n\nprint(\"Hello, World!\")\n\nHello, World!\n\n\nThere are two parts to the syntax above: - print is a builtin function - “Hello, World!” is a string\nWhen print is given a single, string argument like above, it displays that input.\nNext, give yourself a little praise, you’re on your way to mastering Python and more.\nSet up two variables to work with: - compliment - name\n\ncompliment = \"Great work\"\n\n\nname = \"Your name here\"\n\n\nprint(compliment)\nprint(name)\n\nGreat work\nYour name here\n\n\n\n\nVariables are essential to programming. A variable creates a named container for information.\nVariable names can only contain letters, digits, and underscores, must start with a letter or underscore, and can’t be a reserved word (e.g., if, def, return).\nIn Python, use = to assign values: x = 5.\nCreating and working with variables makes programming much easier!\nIt creates a reusable, custom name.\n\nprint(compliment)\nprint(name)\n\nGreat work\nYour name here\n\n\nSomething about the above looks off…\nIt should be on one line.\n\ncompliment_and_name = compliment + name\n\n\nprint(compliment_and_name)\n\nGreat workYour name here\n\n\nStill not quite right…\n\ncompliment_and_name_2 = compliment +', '+ name\n\n\nprint(compliment_and_name_2)\n\nGreat work, Your name here\n\n\nWith feeling!\n\ncompliment_and_name_3 = compliment_and_name_2 +'!'\n\n\nprint(compliment_and_name_3)\n\nGreat work, Your name here!\n\n\n\n\n\nNow you have not only created variables you concatenated them with an operator +.\nKnowing how to manipulate variables is key to understanding programming.\n\n\n\n\n\n\n\n\nExample\nValid?\nReason\n\n\n\n\nage\n✅ Yes\nStarts with a letter, letters only\n\n\nuser_name\n✅ Yes\nLetters + underscore are allowed\n\n\ntemperature2\n✅ Yes\nDigits allowed, but not at start\n\n\n_hidden\n✅ Yes\nLeading underscore allowed (often used for “private” values)\n\n\nMAX_VALUE\n✅ Yes\nUppercase is allowed, often used for constants\n\n\n2nd_value\n❌ No\nCannot start with a digit\n\n\nuser-name\n❌ No\nDash (-) not allowed\n\n\nfile.path\n❌ No\nDot (.) means “attribute,” not part of a name\n\n\nmy var\n❌ No\nSpaces not allowed\n\n\nreturn\n❌ No\nreturn is a reserved Python keyword"
  },
  {
    "objectID": "jump-in/jl-notebooks/files/variables-and-functions-nb.html#datatypes",
    "href": "jump-in/jl-notebooks/files/variables-and-functions-nb.html#datatypes",
    "title": "Variables and Functions - Notebook",
    "section": "Datatypes",
    "text": "Datatypes\n\n\n\nCategory\nType\nExample\n\n\n\n\nNumbers\nint\n42\n\n\n\nfloat\n3.14\n\n\n\ncomplex\n2 + 3j\n\n\nText\nstr\n\"hello\"\n\n\nSequences\nlist\n[1, 2, 3]\n\n\n\ntuple\n(1, 2, 3)\n\n\n\nrange\nrange(5)\n\n\nSets\nset\n{1, 2, 3}\n\n\n\nfrozenset\nfrozenset([1, 2, 3])\n\n\nMappings\ndict\n{\"a\": 1, \"b\": 2}\n\n\nBoolean\nbool\nTrue, False\n\n\nSpecial\nNoneType\nNone"
  },
  {
    "objectID": "jump-in/variables-and-functions.html",
    "href": "jump-in/variables-and-functions.html",
    "title": "Variables and Functions",
    "section": "",
    "text": "This is the second part of the Jump In guide introducing core Python concepts in a hands-on way, using Jupyter notebooks as the primary environment. This page will cover variables and built-in functions. Be sure to check out the accompanying notebook for more hands-on experience.\n\n\n\n\n\n\nReusability\n\n\n\nEffective programming is built around reusability. Creating automated, reproducible code starts with variables and functions.",
    "crumbs": [
      "Pixel Process",
      "Variables and Functions"
    ]
  },
  {
    "objectID": "jump-in/variables-and-functions.html#get-started-with-python---part-2",
    "href": "jump-in/variables-and-functions.html#get-started-with-python---part-2",
    "title": "Variables and Functions",
    "section": "",
    "text": "This is the second part of the Jump In guide introducing core Python concepts in a hands-on way, using Jupyter notebooks as the primary environment. This page will cover variables and built-in functions. Be sure to check out the accompanying notebook for more hands-on experience.\n\n\n\n\n\n\nReusability\n\n\n\nEffective programming is built around reusability. Creating automated, reproducible code starts with variables and functions.",
    "crumbs": [
      "Pixel Process",
      "Variables and Functions"
    ]
  },
  {
    "objectID": "jump-in/variables-and-functions.html#variables",
    "href": "jump-in/variables-and-functions.html#variables",
    "title": "Variables and Functions",
    "section": "Variables",
    "text": "Variables\nVariable assignment stores data in a named container. In Python, use = to assign values: x = 5. Variables are case-sensitive and can store numbers, strings, lists, and other datatypes. Variables in Python are mutable meaning the value can be modified.\n\n\n\n\n\n\n\n\n\nExample\nValid?\nReason\n\n\n\n\nage = 25\n✅ Yes\nStarts with a letter, letters only\n\n\nuser_name = “A”\n✅ Yes\nLetters + underscore are allowed\n\n\ntemperature2\n✅ Yes\nDigits allowed, but not at start\n\n\n_hidden\n✅ Yes\nLeading underscore allowed (often used for “private” values)\n\n\nMAX_VALUE\n✅ Yes\nUppercase is allowed, often used for constants\n\n\n2nd_value\n❌ No\nCannot start with a digit\n\n\nuser-name\n❌ No\nDash (-) not allowed\n\n\nfile.path\n❌ No\nDot (.) means “attribute,” not part of a name\n\n\nmy var\n❌ No\nSpaces not allowed\n\n\nreturn = 5\n❌ No\nreturn is a reserved Python keyword",
    "crumbs": [
      "Pixel Process",
      "Variables and Functions"
    ]
  },
  {
    "objectID": "jump-in/variables-and-functions.html#functions",
    "href": "jump-in/variables-and-functions.html#functions",
    "title": "Variables and Functions",
    "section": "Functions",
    "text": "Functions\nFunctions create reusable code which can simplify tasks, allow iterative runs, and can make your code much more powerful. Python comes with some builtin functions which will help demonstrate. The table below outlines so of the most commonly used ones and their usage.\n\n\n\n\n\n\n\n\n\n\nFunction\nExample\nMeaning\nResult\n\n\n\n\nhelp\nhelp(len)\nDisplays documentation about an object or function\nShows usage info in console\n\n\nlen\nlen([1, 2, 3])\nReturns the number of items in an object\n3\n\n\nprint\nprint(‘Hello’)\nOutputs text or variables to the console\nHello\n\n\ntype\ntype(3.14)\nReturns the data type of an object\n&lt;class ‘float’&gt;\n\n\nmin\nmin(4, 7, 2)\nReturns the smallest of the given values\n2\n\n\nmax\nmax(4, 7, 2)\nReturns the largest of the given values\n7\n\n\nstr\nstr(123)\nConverts a value to a string\n‘123’\n\n\nint\nint(3.9)\nConverts a value to an integer (truncates decimals)\n3\n\n\nfloat\nfloat(‘3.14’)\nConverts a value to a floating-point number\n3.14",
    "crumbs": [
      "Pixel Process",
      "Variables and Functions"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pixel Process",
    "section": "",
    "text": "Welcome to PxP!\nCurious about how it all fits together? Start anywhere, follow your path, and discover how small pieces become powerful systems."
  },
  {
    "objectID": "index.html#paths-not-courses",
    "href": "index.html#paths-not-courses",
    "title": "Pixel Process",
    "section": "Paths, Not Courses",
    "text": "Paths, Not Courses\nThis isn’t a course — it’s a toolkit.\nPixel Process is designed for exploration, not checklists. You won’t find step-by-step lessons or a strict progression. Instead, start by choosing a goal that excites you — maybe you want to:\n\nAutomate a repetitive task with Python\nBuild a portfolio-worthy recommender system\nStandardize your projects with consistent structure and tools\n\nFrom there, explore freely. Each section of the site offers building blocks: beginner to advanced, theory to implementation. You’ll need to spot your own gaps, follow your curiosity, and piece together what matters most to your workflow.\nThis path isn’t about completing a syllabus — it’s about creating momentum."
  },
  {
    "objectID": "index.html#jump-into-hot-topics",
    "href": "index.html#jump-into-hot-topics",
    "title": "Pixel Process",
    "section": "Jump into Hot Topics",
    "text": "Jump into Hot Topics\n\n\n\n About → PxP mission, values, and team\n\n\n Jump In → No downloads, no setup-jump into basic Python\n\n\n Machine Learning → A guide to machine learning\n\n\n Random Forest (NB) → Deep dive into decision trees and random forest models\n\n\n Perspective: Pixels & Pictures → Context dependent perception\n\n\n Tools Overview → Editors, linters, and formatters, oh my!"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Pixel Process",
    "section": "Getting Started",
    "text": "Getting Started\nInstall tools, launch notebooks, and get productive in minutes.\n\n\n\nBasic Overview\n\n\n\n FAQs → FAQs when getting started with programming\n\n\n Troubleshooting Basics → A guide to problem-solving and debugging\n\n\n How to: Jupyter Notebooks → Intro to using and navigating notebooks\n\n\n\n\n\nJump In\n\n\n\n Jump In → No downloads, no setup-jump into basic Python\n\n\n Datatypes and Operators → Jump into Python - Part 1\n\n\n Variables and Functions → Jump into Python - Part 2\n\n\n Errors and Experimentation → Jump into Python - Part 3\n\n\n\n\n\nR Introduction\n\n\n\n R Intro PDF → PDF companion to the R intro notebook series\n\n\n R: Basic Commands (NB) → Get started with R\n\n\n R: Datasets (NB) → Vectors and datasets in R\n\n\n R: Data Basics (NB) → Create, edit, and summarize vectors in R\n\n\n R: Import/Export (NB) → Loading and saving data with R\n\n\n R: Data Vizualization Basics (NB) → Basic plotting functions in R\n\n\n R: Data Analysis (NB) → Begin analyzing with correlations, ttests, and more"
  },
  {
    "objectID": "index.html#foundations",
    "href": "index.html#foundations",
    "title": "Pixel Process",
    "section": "Foundations",
    "text": "Foundations\nCore Python, data loading/cleaning, and visualization basics.\n\n\n\nDatasets\n\n\n\n Dataset Exploration → Guide to evaluating tabluar datasets\n\n\n Dataset Exploration (NB) → Verify data dimensions, datatypes, and quality\n\n\n\n\n\nData Visualization\n\n\n\n Data Visualization Basics → Learn about turning data into stories\n\n\n Data Visualization Basics (NB) → Start turning data into stories\n\n\n Data Viz Building Blocks → Core features behind data viz\n\n\n Data Viz Building Blocks (NB) → Code example for core features behind data viz\n\n\n\n\n\nImage Analysis"
  },
  {
    "objectID": "index.html#machine-learning",
    "href": "index.html#machine-learning",
    "title": "Pixel Process",
    "section": "Machine Learning",
    "text": "Machine Learning\nWorked notebooks and concepts—from regression and classification to trees and evaluation.\n\n\n\nModels & Metrics\n\n\n\n What can ML do? → Overview of data and algorithms for ML\n\n\n Random Forest (NB) → Deep dive into decision trees and random forest models\n\n\n\n\n\nClassification\n\n\n\n Iris Classification (NB) → Complete classification pipeline based on Iris dataset\n\n\n\n\n\nRegression\n\n\n\n Housing Regression (NB) → Complete regression pipeline based on housing dataset"
  },
  {
    "objectID": "index.html#workflow",
    "href": "index.html#workflow",
    "title": "Pixel Process",
    "section": "Workflow",
    "text": "Workflow\nEnvironments, linters, automation, and project structure for calm, repeatable work.\n\n\n\nTools\n\n\n\n Tools Overview → Editors, linters, and formatters, oh my!\n\n\n Scripts and Notebooks → When and why to use notebooks vs scripts\n\n\n\n\n\nCustomization\n\n\n\n Command Line Interface → Don’t just learn CLI, optimize it"
  },
  {
    "objectID": "index.html#perspectives",
    "href": "index.html#perspectives",
    "title": "Pixel Process",
    "section": "Perspectives",
    "text": "Perspectives\nReflections and insights into tech and trends\n\n\n\nAND\n\n\n\n Perspective: Pixels & Pictures → Context dependent perception\n\n\n\n\n\n\n\n\n\n\n\nStart Your Journey\n\n\n\nFrom experience, I can attest learning everything is overwhelming, exhausting, and unnecessary. Focus on what you want to build or understand. Learn what you need as you go. And don’t be afraid to change course — that’s part of the process, too.\nStart small — expertise doesn’t happen in a day\nIterate often — progress comes through refactoring, revisiting, and rethinking\nImprove always — focus on what matters now, and let your skills grow with your goals\n\n\n\nStart where you are. Use what you have. Learn what you need\n\nEach section of this project is a stepping stone. Start where it makes sense for you — revisit what you need, skip what you don’t. The best path is the one that fits your needs."
  },
  {
    "objectID": "perspectives/posts/not/mistakes-feature-not-flaw.html",
    "href": "perspectives/posts/not/mistakes-feature-not-flaw.html",
    "title": "Mistakes: Feature ~ Flaw",
    "section": "",
    "text": "Open with a story or example of a mistake that turned into a breakthrough.\n\nContrast the instinct to hide mistakes with the idea that they can be intentional signals.\n\nFrame the post around the theme: mistakes are not flaws, but features of the process."
  },
  {
    "objectID": "perspectives/posts/not/mistakes-feature-not-flaw.html#introduction",
    "href": "perspectives/posts/not/mistakes-feature-not-flaw.html#introduction",
    "title": "Mistakes: Feature ~ Flaw",
    "section": "",
    "text": "Open with a story or example of a mistake that turned into a breakthrough.\n\nContrast the instinct to hide mistakes with the idea that they can be intentional signals.\n\nFrame the post around the theme: mistakes are not flaws, but features of the process."
  },
  {
    "objectID": "perspectives/posts/not/mistakes-feature-not-flaw.html#mistakes-in-coding",
    "href": "perspectives/posts/not/mistakes-feature-not-flaw.html#mistakes-in-coding",
    "title": "Mistakes: Feature ~ Flaw",
    "section": "Mistakes in Coding",
    "text": "Mistakes in Coding\n\nErrors as feedback: how error messages in Python (or any language) aren’t failures but guides.\n\nDebugging as discovery: often reveals hidden assumptions.\n\nNotebooks vs. production: space where mistakes are safe and exploratory."
  },
  {
    "objectID": "perspectives/posts/not/mistakes-feature-not-flaw.html#mistakes-in-data-science",
    "href": "perspectives/posts/not/mistakes-feature-not-flaw.html#mistakes-in-data-science",
    "title": "Mistakes: Feature ~ Flaw",
    "section": "Mistakes in Data Science",
    "text": "Mistakes in Data Science\n\nMessy data: outliers, missing values, and misformatted entries push us to think critically.\n\nModel overfitting/underfitting: mistakes that illuminate model limitations.\n\nExploratory analysis: the “wrong turns” often suggest better hypotheses."
  },
  {
    "objectID": "perspectives/posts/not/mistakes-feature-not-flaw.html#mistakes-in-learning",
    "href": "perspectives/posts/not/mistakes-feature-not-flaw.html#mistakes-in-learning",
    "title": "Mistakes: Feature ~ Flaw",
    "section": "Mistakes in Learning",
    "text": "Mistakes in Learning\n\nNot knowing is part of learning: growth mindset.\n\nIteration and revision: how “failed” attempts are necessary steps toward mastery.\n\nPatterns in mistakes: noticing recurring errors shows where deeper understanding is needed."
  },
  {
    "objectID": "perspectives/posts/not/mistakes-feature-not-flaw.html#mistakes-in-creativity",
    "href": "perspectives/posts/not/mistakes-feature-not-flaw.html#mistakes-in-creativity",
    "title": "Mistakes: Feature ~ Flaw",
    "section": "Mistakes in Creativity",
    "text": "Mistakes in Creativity\n\nHappy accidents: parallels with art, design, or photography.\n\nUnexpected outcomes: sometimes better than the original plan.\n\nReframing flaws as style: imperfections that make work unique."
  },
  {
    "objectID": "perspectives/posts/not/mistakes-feature-not-flaw.html#from-flaw-to-feature",
    "href": "perspectives/posts/not/mistakes-feature-not-flaw.html#from-flaw-to-feature",
    "title": "Mistakes: Feature ~ Flaw",
    "section": "From Flaw to Feature",
    "text": "From Flaw to Feature\n\nHow to document and share mistakes productively (e.g., code comments, blog posts, talks).\n\nBuilding workflows that expect and embrace iteration.\n\nEncouraging collaboration by normalizing error-sharing."
  },
  {
    "objectID": "perspectives/posts/not/mistakes-feature-not-flaw.html#closing-reflection",
    "href": "perspectives/posts/not/mistakes-feature-not-flaw.html#closing-reflection",
    "title": "Mistakes: Feature ~ Flaw",
    "section": "Closing Reflection",
    "text": "Closing Reflection\n\nCircle back to the post title — mistakes are not flaws.\n\nMistakes mark progress, reveal features, and are signs of growth.\n\nInvite readers to reflect: What’s the last mistake you learned from that improved your process?"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About",
    "section": "",
    "text": "The pixel process is a powerful method for learning, and just in data and computer science.\nTo work effectively with image data, you need to understand what’s in a single pixel: its value range, color representation (grayscale, RGB, RGBA), and image grid. Understanding a pixel is easy, while understanding an image is much more difficult. Even the most advanced computer vision techniques rely on pixel level data.\nPixels may seem small and uninteresting, but they are the foundation for creating stunning visuals and impactful analytics.\n\n\nComplex systems often follow the principle: understand the fundamentals before combining them into something greater.\n\nPixels → Images → Videos\nWords → Sentences → Books\nNumbers → Geometry → Calculus\nNucleotides → Genes → DNA → Chromosomes\n\nUnderstanding higher level processes often rely on an indepth understanding of lower ones.\n\n\n\nThe pixel process is about breaking the complex into the manageable. Create a roadmap with achievable goals and notable milestones to master complex systems.\nEqually important, is understanding that mastering everything is untenable. Get better resultss by having a focused objective, so you can efficiently build the skills you need.\nSmall steps, big impact—that’s the PixelProcess."
  },
  {
    "objectID": "about/index.html#what-is-the-pixel-process",
    "href": "about/index.html#what-is-the-pixel-process",
    "title": "About",
    "section": "",
    "text": "The pixel process is a powerful method for learning, and just in data and computer science.\nTo work effectively with image data, you need to understand what’s in a single pixel: its value range, color representation (grayscale, RGB, RGBA), and image grid. Understanding a pixel is easy, while understanding an image is much more difficult. Even the most advanced computer vision techniques rely on pixel level data.\nPixels may seem small and uninteresting, but they are the foundation for creating stunning visuals and impactful analytics.\n\n\nComplex systems often follow the principle: understand the fundamentals before combining them into something greater.\n\nPixels → Images → Videos\nWords → Sentences → Books\nNumbers → Geometry → Calculus\nNucleotides → Genes → DNA → Chromosomes\n\nUnderstanding higher level processes often rely on an indepth understanding of lower ones.\n\n\n\nThe pixel process is about breaking the complex into the manageable. Create a roadmap with achievable goals and notable milestones to master complex systems.\nEqually important, is understanding that mastering everything is untenable. Get better resultss by having a focused objective, so you can efficiently build the skills you need.\nSmall steps, big impact—that’s the PixelProcess."
  },
  {
    "objectID": "about/index.html#values",
    "href": "about/index.html#values",
    "title": "About",
    "section": "Values",
    "text": "Values\n\nOpen License, Non-Commercial Use\nThis project is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License (CC BY-NC-SA 4.0).\nWhat does this mean in plain language?\n\n✅ You may: Share, copy, and adapt this content for any non-commercial purpose.\n✅ You must: Give appropriate credit, provide a link to the license, and indicate if changes were made.\n🔄 If you remix/adapt it: You must distribute your version under the same license (CC BY-NC-SA).\n🚫 You may not: Use this content for commercial purposes.\n\nIn short: learn from it, remix it, improve it—and share it back, but not for profit.\n\n\n\nPrivacy First\nYour privacy matters here. Tracking and analytics are disabled as much as possible.\nThis means:\n\nNo automatic progress tracking right now.\nNo hidden data collection or ad targeting.\n\nIf this changes, it will be clearly documented.\n\n\n\nAd-Free and Hassle-Free\nThis project exists to provide value, not generate income.\n\nNo ads.\nNo pop-ups.\nNo paywalls.\n\n\n\n\nMaintained by One Person\nThis is a personal project, designed and maintained by one human.\nWhat that means:\n\nPlease be patient with updates or fixes.\nMinor quirks may exist—thank you for understanding.\nNo direct moderation or code review is available right now.\n\nIf you want to improve or extend the project:\n➡️ Fork the repository and build your version!\n\nThank you for being here and supporting the vision of accessible, thoughtful content without unnecessary friction."
  },
  {
    "objectID": "about/index.html#meet-the-dev",
    "href": "about/index.html#meet-the-dev",
    "title": "About",
    "section": "Meet the Dev",
    "text": "Meet the Dev\n\n\n\nD. Bugz, PhD\n\nDeveloper\n\n\n\nData scientist with years of experience and a PhD in Psychology. He loves to share knowledge and specializes in automation, reproducibility, and troubleshooting. More than a decade of scientific research and programming with Python, R, and SQL, applied across diverse data types including neuroimaging, biological samples, surveys, and financial data. Skilled in methodologies including computer vision, natural language processing, classification/regression, time-series analysis, and both supervised and unsupervised learning. Proficient with cloud services (AWS, Azure), Git, GitHub, and Jupyter, and dedicated to optimizing workflows for research and industry applications.\n\n\n\n🛠 Tech Stack\n\n\n\nPython\n\n\nR\n\n\nSQL\n\n\nGit\n\n\nSPSS\n\n\nMATLAB\n\n\nJupyter\n\n\n\n\n\n🧪 Methods\n\n\n\nPredictive Modeling\n\n\nComputer Vision\n\n\nNatural Language Processing (NLP)\n\n\nExperimental Design\n\n\nExploratory Data Analysis (EDA)\n\n\n\n\n\n⛰ Best Practices\n\n\n\nProduction\n\n\nStardard Operating Procedures (SOPs)\n\n\nPackaging\n\n\nDocumentation\n\n\nVersion Control\n\n\n\n\n\n🗂 Data Types\n\n\n\nTabular\n\n\nTime Series\n\n\nImages\n\n\nText\n\n\n\n\n\n👯 Extracurricular\n\n\n\nReading Fantasy and Sci-Fi\n\n\nWalking my Teammates\n\n\nCooking\n\n\nVideo Games\n\n\n\n\n\n\n\n\n\n\n\nMy Learning Project, Too\n\n\n\nThis site isn’t just for sharing what I know — it’s also where I figure things out. Every post reflects a step in my own process: experiments, revisions, and occasional wrong turns.\nBuild in public — showing the process helps solidify it\nLearn by doing — every bug, every fix, every refactor teaches something\nKeep growing — the goal isn’t mastery, it’s momentum\n\n\n\nWelcome to PixelProcess\n\nHope you find something useful here — or learn from my mistakes and improve from that!"
  },
  {
    "objectID": "about/index.html#d.-bugz-phd",
    "href": "about/index.html#d.-bugz-phd",
    "title": "About",
    "section": "D. Bugz, PhD",
    "text": "D. Bugz, PhD\n\nDeveloper"
  },
  {
    "objectID": "perspectives/posts/and/perspective-pixels-and-pictures.html",
    "href": "perspectives/posts/and/perspective-pixels-and-pictures.html",
    "title": "Perspective: Pixels & Pictures",
    "section": "",
    "text": "Pixpix is a standard png image with 1024x1024 pixels defined by a RGBA channel layout (Red, Green, Blue, and Alpha which handles transparency).\nPixpix is made up of over 4 million values.\nBy extension, a dataset with 10,000 images contains over 40 billion values.\nThe amount of data encountered and processed on daily basis is staggering.\nWith so much information, how do we decide what to focus on?",
    "crumbs": [
      "Pixel Process",
      "AND",
      "Perspective: Pixels & Pictures"
    ]
  },
  {
    "objectID": "perspectives/posts/and/perspective-pixels-and-pictures.html#pixie-png-or-pixels",
    "href": "perspectives/posts/and/perspective-pixels-and-pictures.html#pixie-png-or-pixels",
    "title": "Perspective: Pixels & Pictures",
    "section": "Pixie, PNG, or Pixels?",
    "text": "Pixie, PNG, or Pixels?\nPerspective determines if you see a pixie, an image, or a set of pixels.\n\nA Pixie\nBy default, most people would see the above as an entity. This is a very functional perspective, as more details are extraneous most of the time.\n\n\nA PNG\nAlternatively, some may think of it as an image or png. This is more likely, if say, you adding the file to a website or using it as an example image for a tutorial.\nThese perspective is helpful when determing the size for image display or figuring out why the image isn’t showing on the page. It is a more-detailed approach.\n\n\nPixels\nUndestanding individual pixel attributes and values is really only helpful in very specific contexts. For instance, it helps if you are trying to analyze the image with others, or edit the picture. Then this very detailed approach makes sense.\n\n\nWhich is it? All 3, Of Course!\nPerception depends on goals and available infomation. Whether you see a pixie, an image, or 4+ million values depends on how you look at it.\nBy understanding the basics of data arrays, pixel properties, and image color channels, those 4+ million values can be easily organized and managed. Those skills also easily translate to other images, videos, and more.\nAdditionally, if you understand pixel properties for one image, it extends to many others!",
    "crumbs": [
      "Pixel Process",
      "AND",
      "Perspective: Pixels & Pictures"
    ]
  },
  {
    "objectID": "perspectives/posts/and/perspective-pixels-and-pictures.html#increasing-complexity",
    "href": "perspectives/posts/and/perspective-pixels-and-pictures.html#increasing-complexity",
    "title": "Perspective: Pixels & Pictures",
    "section": "Increasing Complexity",
    "text": "Increasing Complexity\nOur simple image, has at least 3 distinct perceptions. All of which are functional depending on goals and usage.\nThis sort of appreciate for depth and level of analysis applies to most any field of study or skill.\n\n\nArt\n\nAppreciate art\nMake art\nCurate exhibits\nVerify authenticity of newly discovered Renaissance pieces\n\n\n\nCulinary\n\nEat\nCook\nChief\nGovernment contamination expert\n\n\n\nData Science\n\nSee charts/plots/stats/figures everywhere\nUnderstand data origin/sources\nEvaluated information critically\nCreate your own models and figures\n\n\n\nPerspecive is based on information and goals. Gaining perspective requires understanding your current perspective and focusing on skills to move to a more detailed approach.",
    "crumbs": [
      "Pixel Process",
      "AND",
      "Perspective: Pixels & Pictures"
    ]
  },
  {
    "objectID": "perspectives/posts/and/perspective-pixels-and-pictures.html#pick-a-perspective",
    "href": "perspectives/posts/and/perspective-pixels-and-pictures.html#pick-a-perspective",
    "title": "Perspective: Pixels & Pictures",
    "section": "Pick a Perspective",
    "text": "Pick a Perspective\nDesign your learning based on current skills, opportunities available, and the level of detail at which you want to be engaged.\nSome level of data skill and technical proficiency helps with all of those paths. The detail and level of the perspective varies considerably though.\nBasic Considerations\n- Do I want to work with code all day?\n- Do I want to design the questions?\n- Do I like to automate or prototype?\n- What does impact mean for me?",
    "crumbs": [
      "Pixel Process",
      "AND",
      "Perspective: Pixels & Pictures"
    ]
  },
  {
    "objectID": "perspectives/posts/and/perspective-pixels-and-pictures.html#understand-the-basics-focus-your-efforts",
    "href": "perspectives/posts/and/perspective-pixels-and-pictures.html#understand-the-basics-focus-your-efforts",
    "title": "Perspective: Pixels & Pictures",
    "section": "Understand the Basics, Focus your Efforts",
    "text": "Understand the Basics, Focus your Efforts\nFoundational knowledge is your anchor. When you understand the basics of how data works—you gain confidence. Taking the time assess your interests, skills, knowledge-gaps, goals, and learning style pays off in the long run.\nYou won’t be an expert in every model, every tool, every new technology—and that’s okay. If you chase everything, you will catch nothing.",
    "crumbs": [
      "Pixel Process",
      "AND",
      "Perspective: Pixels & Pictures"
    ]
  },
  {
    "objectID": "perspectives/posts/and/perspective-pixels-and-pictures.html#specialize-to-succeed",
    "href": "perspectives/posts/and/perspective-pixels-and-pictures.html#specialize-to-succeed",
    "title": "Perspective: Pixels & Pictures",
    "section": "Specialize to Succeed",
    "text": "Specialize to Succeed\nSpecialization is an edge. Teams thrive on a mix of deep specialists and broad generalists.\nThink of it like building a house:\n- The architect plans the structure.\n- The electrician ensures the lights turn on.\n- The plumber keeps the water flowing.\nIn data science:\n- An ML engineer focuses on model deployment.\n- A data analyst handles visualization and business context.\n- A data engineer controls access and automation.\nHaving an area of focus makes you valuable. Knowing how your efforts fit into the broader picture, even more so.",
    "crumbs": [
      "Pixel Process",
      "AND",
      "Perspective: Pixels & Pictures"
    ]
  },
  {
    "objectID": "perspectives/posts/and/perspective-pixels-and-pictures.html#perspecive-matters",
    "href": "perspectives/posts/and/perspective-pixels-and-pictures.html#perspecive-matters",
    "title": "Perspective: Pixels & Pictures",
    "section": "Perspecive Matters",
    "text": "Perspecive Matters\nPixpix is a character, an image, a file, and millions of numbers.\nBut if you understand pixels, you can apply processes to the whole image and many others!\nUnderstanding stems from perspective and effort is optimized when focused.",
    "crumbs": [
      "Pixel Process",
      "AND",
      "Perspective: Pixels & Pictures"
    ]
  },
  {
    "objectID": "perspectives/index.html",
    "href": "perspectives/index.html",
    "title": "Perspectives",
    "section": "",
    "text": "Learning doesn’t happen all at once—it evolves. Concepts we once held as certain may shift with new context, experience, or tools. The point of learning isn’t to lock in static knowledge, but to stay open to rethinking and reworking. Like refactoring code, insight grows from iteration.\nThis space is for exactly that: ideas revisited, connections redrawn, and assumptions gently shaken. Sometimes playful, sometimes practical, always curious.",
    "crumbs": [
      "Pixel Process",
      "Perspectives"
    ]
  },
  {
    "objectID": "perspectives/index.html#rethinking-what-we-know",
    "href": "perspectives/index.html#rethinking-what-we-know",
    "title": "Perspectives",
    "section": "",
    "text": "Learning doesn’t happen all at once—it evolves. Concepts we once held as certain may shift with new context, experience, or tools. The point of learning isn’t to lock in static knowledge, but to stay open to rethinking and reworking. Like refactoring code, insight grows from iteration.\nThis space is for exactly that: ideas revisited, connections redrawn, and assumptions gently shaken. Sometimes playful, sometimes practical, always curious.",
    "crumbs": [
      "Pixel Process",
      "Perspectives"
    ]
  },
  {
    "objectID": "perspectives/index.html#and-or-not",
    "href": "perspectives/index.html#and-or-not",
    "title": "Perspectives",
    "section": "And, Or, Not",
    "text": "And, Or, Not\nPosts are organized by the bitwise operators you might recognize from programming—but here they represent how ideas relate to one another:\n\n& (AND) — Complementary or intersecting concepts\nExamples: Information: Continuous & Categorical\n\n| (OR) — Contrasting or competing ideas\nExamples: Design: Structure | Spontaneity\n\n~ (NOT) — Reframed, nuanced, or subverted concepts\nExamples: Mistakes: Feature ~ Flaw\n\n\n\n🔍 Recent Posts\n\nPerspective: Pixels & Pictures\nMistakes: Feature ~ Flaw\nData: Spun ~ Spontaneous\nInformation: Continuous & Categorical",
    "crumbs": [
      "Pixel Process",
      "Perspectives"
    ]
  },
  {
    "objectID": "jump-in/errors-and-experimentation.html",
    "href": "jump-in/errors-and-experimentation.html",
    "title": "Errors and Experimentation",
    "section": "",
    "text": "This guide introduces core Python concepts in a hands-on way, using Jupyter notebooks as the primary environment. You’ll learn about variables, data types, basic syntax, error messages, and more. All in base Python, package use will come later.\n\n\n\n\n\n\nJupyter Notebooks\n\n\n\nNotebooks are a great way to learn and collaborate with code. Be sure to check out Jupyter Notebooks if you are unfamiliar with them, as they are heavily used in this project.",
    "crumbs": [
      "Pixel Process",
      "Errors and Experimentation"
    ]
  },
  {
    "objectID": "jump-in/errors-and-experimentation.html#getting-started-with-python",
    "href": "jump-in/errors-and-experimentation.html#getting-started-with-python",
    "title": "Errors and Experimentation",
    "section": "",
    "text": "This guide introduces core Python concepts in a hands-on way, using Jupyter notebooks as the primary environment. You’ll learn about variables, data types, basic syntax, error messages, and more. All in base Python, package use will come later.\n\n\n\n\n\n\nJupyter Notebooks\n\n\n\nNotebooks are a great way to learn and collaborate with code. Be sure to check out Jupyter Notebooks if you are unfamiliar with them, as they are heavily used in this project.",
    "crumbs": [
      "Pixel Process",
      "Errors and Experimentation"
    ]
  },
  {
    "objectID": "jump-in/errors-and-experimentation.html#python-parts",
    "href": "jump-in/errors-and-experimentation.html#python-parts",
    "title": "Errors and Experimentation",
    "section": "Python Parts",
    "text": "Python Parts\nBefore jumping into the code, we’ll cover the core pieces, pixels if you will, that Python is built upon. Many of these concepts are similar in other languages as well.\n\nDatatypes\nOperators\nReserved Words\nBuiltin Functions\nVariables\nFlow Controls\n\n\nDatatypes\nPython uses datatypes to organize and process information. The datatype can determine how much data is stored, used, and displayed. Picking the right datatype for a use-case makes coding much smoother. Key catergories include numeric, strings, and sequences.\n\n\n\n\nCategory\nType\nExample\n\n\n\n\nNumbers\nint\n42\n\n\nNumbers\nfloat\n3.14\n\n\nNumbers\ncomplex\n2 + 3j\n\n\nText\nstr\n“hello”\n\n\nSequences\nlist\n[1, 2, 3]\n\n\nSequences\ntuple\n(1, 2, 3)\n\n\nSequences\nrange\nrange(5)\n\n\nSets\nset\n{1, 2, 3}\n\n\nSets\nfrozenset\nfrozenset([1, 2, 3])\n\n\nMappings\ndict\n{“a”: 1, “b”: 2}\n\n\nBoolean\nbool\nTrue, False\n\n\nSpecial\nNoneType\nNone\n\n\n\n\n\n\nOperators\nOperations in programming are used to manipulate and evaluate values. These allow programs to perform calculations, make decisions, and control the flow of execution. Types include:\n\nMathematical\nLogical\nComparison\nAssignment\n\n\nMathematical\nMathematical operators perform arithmetic calculations. In Python, these include + (addition), - (subtraction), * (multiplication), / (division), ** or ^ (exponentiation), and % or %% (modulus). Python uses // for integer division.\n\n\n\n\nOperator\nExample\nMeaning\nResult\n\n\n\n\n+\n3 + 2\nAddition\n5\n\n\n-\n7 - 4\nSubtraction\n3\n\n\n*\n6 * 2\nMultiplication\n12\n\n\n/\n7 / 2\nDivision (always float)\n3.5\n\n\n//\n7 // 2\nFloor (integer) division\n3\n\n\n%\n7 % 2\nModulus (remainder)\n1\n\n\n**\n2 ** 3\nExponentiation\n8\n\n\n\n\n\n\nComparison\nComparison operators evaluate relationships between values. Common operators in Python include == (equal), != (not equal), &lt; (less than), &lt;= (less than or equal to), &gt; (greater than), and &gt;= (greater than or equal to).\nThese return boolean values of True/False.\n\n\n\n\nOperator\nExample\nMeaning\nResult\n\n\n\n\n==\n3 == 3\nEqual to\nTrue\n\n\n!=\n3 != 4\nNot equal to\nTrue\n\n\n&gt;\n5 &gt; 2\nGreater than\nTrue\n\n\n&lt;\n2 &lt; 5\nLess than\nTrue\n\n\n&gt;=\n5 &gt;= 5\nGreater than or equal to\nTrue\n\n\n&lt;=\n4 &lt;= 5\nLess than or equal to\nTrue\n\n\n\n\n\n\nLogical\nLogical operators allow for boolean logic. In Python: and, or, not. These are used in conditionals and loops to combine multiple logical expressions.\n\n\n\n\nOperator\nExample\nMeaning\nResult\n\n\n\n\nand\n(3 &gt; 1) and (2 &lt; 5)\nTrue if both are true\nTrue\n\n\nor\n(3 &gt; 1) or (2 &gt; 5)\nTrue if at least one is true\nTrue\n\n\nnot\nnot (3 &gt; 1)\nNegates (True → False)\nFalse\n\n\n\n\n\n\n\n\n\n\nAssignment and Equality\n\n\n\nAssignment uses = to assign a value to a variable. x = 10 creates or updates the variable x to be 10.\nEquality checking uses == and returns a Boolean (True, or False) depending on if the values are equal. x == 10 will return True if x is set as above.\n\n\n\n\n\nVariables and Functions\nEffective programming is built around reusable variables and code snippets like functions, classes, and methods. We’ll explore this more in-depth throughout the project.\n\nVariables\nVariable assignment stores data in a named container. In Python, use = to assign values: x = 5. Variables are case-sensitive and can store numbers, strings, lists, and other datatypes. Variables in Python are mutable meaning the value can be modified.\n\n\n\n\n\n\n\n\n\nExample\nValid?\nReason\n\n\n\n\nage = 25\n✅ Yes\nStarts with a letter, letters only\n\n\nuser_name = “A”\n✅ Yes\nLetters + underscore are allowed\n\n\ntemperature2\n✅ Yes\nDigits allowed, but not at start\n\n\n_hidden\n✅ Yes\nLeading underscore allowed (often used for “private” values)\n\n\nMAX_VALUE\n✅ Yes\nUppercase is allowed, often used for constants\n\n\n2nd_value\n❌ No\nCannot start with a digit\n\n\nuser-name\n❌ No\nDash (-) not allowed\n\n\nfile.path\n❌ No\nDot (.) means “attribute,” not part of a name\n\n\nmy var\n❌ No\nSpaces not allowed\n\n\nreturn = 5\n❌ No\nreturn is a reserved Python keyword\n\n\n\n\n\n\nFunctions\nFunctions create reusable code which can simplify tasks, allow iterative runs, and can make your code much more powerful. Python comes with some builtin functions which will help demonstrate. To begin, let’s focus on: - print: display the passed argument - type: returns the type of the passed argument - help: returns documentation and help files of passed argument - Assignment",
    "crumbs": [
      "Pixel Process",
      "Errors and Experimentation"
    ]
  },
  {
    "objectID": "jump-in/errors-and-experimentation.html#control-flow",
    "href": "jump-in/errors-and-experimentation.html#control-flow",
    "title": "Errors and Experimentation",
    "section": "Control Flow",
    "text": "Control Flow\nUse if, elif, and else for conditional logic:\nx = 10\nif x &gt; 5:\n    print(\"Large\")\nelif x == 5:\n    print(\"Exactly 5\")\nelse:\n    print(\"Small\")\nYou’ll also see for and while loops, as well as function definitions like def greet(name): ....",
    "crumbs": [
      "Pixel Process",
      "Errors and Experimentation"
    ]
  },
  {
    "objectID": "jump-in/errors-and-experimentation.html#common-error-messages",
    "href": "jump-in/errors-and-experimentation.html#common-error-messages",
    "title": "Errors and Experimentation",
    "section": "Common Error Messages",
    "text": "Common Error Messages\nYou’ll encounter errors. That’s normal! Learn to read tracebacks.\n\nSyntaxError: Something’s wrong with your code structure.\nNameError: You used a variable that doesn’t exist.\nTypeError: You tried an operation on incompatible types.\nIndentationError: Your code blocks are not properly aligned.",
    "crumbs": [
      "Pixel Process",
      "Errors and Experimentation"
    ]
  },
  {
    "objectID": "jump-in/errors-and-experimentation.html#combining-concepts",
    "href": "jump-in/errors-and-experimentation.html#combining-concepts",
    "title": "Errors and Experimentation",
    "section": "Combining Concepts",
    "text": "Combining Concepts\nTry this small example to use variables, control flow, and output:\nscore = 87\nif score &gt;= 90:\n    grade = 'A'\nelif score &gt;= 80:\n    grade = 'B'\nelse:\n    grade = 'C or below'\n\nprint(f\"Your grade: {grade}\")\n\nVARIABLE NAMING\n\n\n\n\n\n\n\n\n\nExample\nValid?\nReason\n\n\n\n\nage = 25\n✅ Yes\nStarts with a letter, letters only\n\n\nuser_name = “A”\n✅ Yes\nLetters + underscore are allowed\n\n\ntemperature2\n✅ Yes\nDigits allowed, but not at start\n\n\n_hidden\n✅ Yes\nLeading underscore allowed (often used for “private” values)\n\n\nMAX_VALUE\n✅ Yes\nUppercase is allowed, often used for constants\n\n\n2nd_value\n❌ No\nCannot start with a digit\n\n\nuser-name\n❌ No\nDash (-) not allowed\n\n\nfile.path\n❌ No\nDot (.) means “attribute,” not part of a name\n\n\nmy var\n❌ No\nSpaces not allowed\n\n\nreturn = 5\n❌ No\nreturn is a reserved Python keyword\n\n\n\n\n\n\nOPERATIONS\nCheck if tables work.\n\nOperators - ArithmeticOperators - Comparison\n\n\n\n\n\n\nOperator\nExample\nMeaning\nResult\n\n\n\n\n+\n3 + 2\nAddition\n5\n\n\n-\n7 - 4\nSubtraction\n3\n\n\n*\n6 * 2\nMultiplication\n12\n\n\n/\n7 / 2\nDivision (always float)\n3.5\n\n\n//\n7 // 2\nFloor (integer) division\n3\n\n\n%\n7 % 2\nModulus (remainder)\n1\n\n\n**\n2 ** 3\nExponentiation\n8\n\n\n\n\n\n\n\n\n\n\nOperator\nExample\nMeaning\nResult\n\n\n\n\n==\n3 == 3\nEqual to\nTrue\n\n\n!=\n3 != 4\nNot equal to\nTrue\n\n\n&gt;\n5 &gt; 2\nGreater than\nTrue\n\n\n&lt;\n2 &lt; 5\nLess than\nTrue\n\n\n&gt;=\n5 &gt;= 5\nGreater than or equal to\nTrue\n\n\n&lt;=\n4 &lt;= 5\nLess than or equal to\nTrue",
    "crumbs": [
      "Pixel Process",
      "Errors and Experimentation"
    ]
  },
  {
    "objectID": "jump-in/index.html",
    "href": "jump-in/index.html",
    "title": "Jump In",
    "section": "",
    "text": "Jump into Python with hands-on practice, right in your browser. No installations, no setups, no signin. These pages use embedded code blocks to edit and run code. For extra experience, check out the notebooks too!\nTry, test, and tinker — it’s all part of the process.",
    "crumbs": [
      "Pixel Process",
      "Jump In"
    ]
  },
  {
    "objectID": "jump-in/index.html#ready-to-jump-in",
    "href": "jump-in/index.html#ready-to-jump-in",
    "title": "Jump In",
    "section": "Ready to Jump In?",
    "text": "Ready to Jump In?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Pixel Process",
      "Jump In"
    ]
  },
  {
    "objectID": "jump-in/index.html#content-guide",
    "href": "jump-in/index.html#content-guide",
    "title": "Jump In",
    "section": "Content Guide",
    "text": "Content Guide\nBuilt to explore, all notebooks in this section are interactive - no setup required. Content overviews are paired with notebooks to improve learning and encourage testing. Several skill test notebooks are also included to cement what you learn.\n\n Datatypes and Operators  Datatypes and Operators INB\n Variables and Functions  Variables and Functions INB\n\n\n\n\n\n\n\nExperience from Errors\n\n\n\nThese bugs don’t bite! Writing a complex program without errors is as probable as writing a novel without a typo. The key is test for errors, ensure things work properly, and address the issues. Understanding basic errors will allow you tackle much more complex tasks.\nCode Freely — programming is interactive, so try things, test things, experiment\nFail Safely — mistakes don’t hurt here, if something goes wrong simply reset the workspace\nStart Quickly — examine inputs and outputs, read error messages, iterate and experiment if unsure\n\n\n\nHands on is the only true way to learn programming\n\nStart small. Make mistakes. Fix errors. This is a space to experiment freely, play around, and learn by doing. Nothing to break here, no risks, consider this a playground for learning. Jump in!",
    "crumbs": [
      "Pixel Process",
      "Jump In"
    ]
  },
  {
    "objectID": "jump-in/jl-notebooks/files/datatypes-and-operators-nb.html",
    "href": "jump-in/jl-notebooks/files/datatypes-and-operators-nb.html",
    "title": "Datatypes and Operators - Notebook",
    "section": "",
    "text": "4+5\n\n9\n\n\n\nprint('Hello')\n\nHello"
  },
  {
    "objectID": "jump-in/jl-notebooks/jl-build/lab/jupyter-lite.html",
    "href": "jump-in/jl-notebooks/jl-build/lab/jupyter-lite.html",
    "title": "jupyter-lite.ipynb",
    "section": "",
    "text": "This notebook is the preferred source of site-specific runtime configuration for a JupyterLite app, and will override any configuration in jupyter-lite.json."
  },
  {
    "objectID": "jump-in/jl-notebooks/jl-build/lab/jupyter-lite.html#editing-configuration",
    "href": "jump-in/jl-notebooks/jl-build/lab/jupyter-lite.html#editing-configuration",
    "title": "jupyter-lite.ipynb",
    "section": "Editing Configuration",
    "text": "Editing Configuration\nThe configuration is stored in this Notebook’s metadata under the jupyter-lite key. To edit the configuration in JupyterLab.\n\nopen the Property Inspector sidebar\nexpand the Advanced Tools section\nedit the jupyter-lite metadata sub-key\npress the “check” icon\nsave the notebook"
  },
  {
    "objectID": "jump-in/jl-notebooks/jl-build/jupyter-lite.html",
    "href": "jump-in/jl-notebooks/jl-build/jupyter-lite.html",
    "title": "jupyter-lite.ipynb",
    "section": "",
    "text": "This notebook is the preferred source of site-specific runtime configuration for a JupyterLite app, and will override any configuration in jupyter-lite.json."
  },
  {
    "objectID": "jump-in/jl-notebooks/jl-build/jupyter-lite.html#editing-configuration",
    "href": "jump-in/jl-notebooks/jl-build/jupyter-lite.html#editing-configuration",
    "title": "jupyter-lite.ipynb",
    "section": "Editing Configuration",
    "text": "Editing Configuration\nThe configuration is stored in this Notebook’s metadata under the jupyter-lite key. To edit the configuration in JupyterLab.\n\nopen the Property Inspector sidebar\nexpand the Advanced Tools section\nedit the jupyter-lite metadata sub-key\npress the “check” icon\nsave the notebook"
  },
  {
    "objectID": "jump-in/jl-notebooks/jl-build/files/variables-and-functions-nb.html",
    "href": "jump-in/jl-notebooks/jl-build/files/variables-and-functions-nb.html",
    "title": "Variables and Functions - Notebook",
    "section": "",
    "text": "Any programmer knows, the first test to show if you know a language it printing Hello, World!\nThis interactive notebook uses only pure Python, no external resources to get started.\nTopics covered:\n\nHello World\nTypes (int, float, str, bool, list, dict, set, tuple)\nVariables & assignment\nOperators (arithmetic, comparison, logical)\n\n\nprint(\"Hello, World!\")\n\nHello, World!\n\n\nThere are two parts to the syntax above: - print is a builtin function - “Hello, World!” is a string\nWhen print is given a single, string argument like above, it displays that input.\nNext, give yourself a little praise, you’re on your way to mastering Python and more.\nSet up two variables to work with: - compliment - name\n\ncompliment = \"Great work\"\n\n\nname = \"Your name here\"\n\n\nprint(compliment)\nprint(name)\n\nGreat work\nYour name here\n\n\n\n\nVariables are essential to programming. A variable creates a named container for information.\nVariable names can only contain letters, digits, and underscores, must start with a letter or underscore, and can’t be a reserved word (e.g., if, def, return).\nIn Python, use = to assign values: x = 5.\nCreating and working with variables makes programming much easier!\nIt creates a reusable, custom name.\n\nprint(compliment)\nprint(name)\n\nGreat work\nYour name here\n\n\nSomething about the above looks off…\nIt should be on one line.\n\ncompliment_and_name = compliment + name\n\n\nprint(compliment_and_name)\n\nGreat workYour name here\n\n\nStill not quite right…\n\ncompliment_and_name_2 = compliment +', '+ name\n\n\nprint(compliment_and_name_2)\n\nGreat work, Your name here\n\n\nWith feeling!\n\ncompliment_and_name_3 = compliment_and_name_2 +'!'\n\n\nprint(compliment_and_name_3)\n\nGreat work, Your name here!\n\n\n\n\n\nNow you have not only created variables you concatenated them with an operator +.\nKnowing how to manipulate variables is key to understanding programming.\n\n\n\n\n\n\n\n\nExample\nValid?\nReason\n\n\n\n\nage\n✅ Yes\nStarts with a letter, letters only\n\n\nuser_name\n✅ Yes\nLetters + underscore are allowed\n\n\ntemperature2\n✅ Yes\nDigits allowed, but not at start\n\n\n_hidden\n✅ Yes\nLeading underscore allowed (often used for “private” values)\n\n\nMAX_VALUE\n✅ Yes\nUppercase is allowed, often used for constants\n\n\n2nd_value\n❌ No\nCannot start with a digit\n\n\nuser-name\n❌ No\nDash (-) not allowed\n\n\nfile.path\n❌ No\nDot (.) means “attribute,” not part of a name\n\n\nmy var\n❌ No\nSpaces not allowed\n\n\nreturn\n❌ No\nreturn is a reserved Python keyword",
    "crumbs": [
      "Pixel Process",
      "Variables and Functions - Notebook"
    ]
  },
  {
    "objectID": "jump-in/jl-notebooks/jl-build/files/variables-and-functions-nb.html#hello-world",
    "href": "jump-in/jl-notebooks/jl-build/files/variables-and-functions-nb.html#hello-world",
    "title": "Variables and Functions - Notebook",
    "section": "",
    "text": "Any programmer knows, the first test to show if you know a language it printing Hello, World!\nThis interactive notebook uses only pure Python, no external resources to get started.\nTopics covered:\n\nHello World\nTypes (int, float, str, bool, list, dict, set, tuple)\nVariables & assignment\nOperators (arithmetic, comparison, logical)\n\n\nprint(\"Hello, World!\")\n\nHello, World!\n\n\nThere are two parts to the syntax above: - print is a builtin function - “Hello, World!” is a string\nWhen print is given a single, string argument like above, it displays that input.\nNext, give yourself a little praise, you’re on your way to mastering Python and more.\nSet up two variables to work with: - compliment - name\n\ncompliment = \"Great work\"\n\n\nname = \"Your name here\"\n\n\nprint(compliment)\nprint(name)\n\nGreat work\nYour name here\n\n\n\n\nVariables are essential to programming. A variable creates a named container for information.\nVariable names can only contain letters, digits, and underscores, must start with a letter or underscore, and can’t be a reserved word (e.g., if, def, return).\nIn Python, use = to assign values: x = 5.\nCreating and working with variables makes programming much easier!\nIt creates a reusable, custom name.\n\nprint(compliment)\nprint(name)\n\nGreat work\nYour name here\n\n\nSomething about the above looks off…\nIt should be on one line.\n\ncompliment_and_name = compliment + name\n\n\nprint(compliment_and_name)\n\nGreat workYour name here\n\n\nStill not quite right…\n\ncompliment_and_name_2 = compliment +', '+ name\n\n\nprint(compliment_and_name_2)\n\nGreat work, Your name here\n\n\nWith feeling!\n\ncompliment_and_name_3 = compliment_and_name_2 +'!'\n\n\nprint(compliment_and_name_3)\n\nGreat work, Your name here!\n\n\n\n\n\nNow you have not only created variables you concatenated them with an operator +.\nKnowing how to manipulate variables is key to understanding programming.\n\n\n\n\n\n\n\n\nExample\nValid?\nReason\n\n\n\n\nage\n✅ Yes\nStarts with a letter, letters only\n\n\nuser_name\n✅ Yes\nLetters + underscore are allowed\n\n\ntemperature2\n✅ Yes\nDigits allowed, but not at start\n\n\n_hidden\n✅ Yes\nLeading underscore allowed (often used for “private” values)\n\n\nMAX_VALUE\n✅ Yes\nUppercase is allowed, often used for constants\n\n\n2nd_value\n❌ No\nCannot start with a digit\n\n\nuser-name\n❌ No\nDash (-) not allowed\n\n\nfile.path\n❌ No\nDot (.) means “attribute,” not part of a name\n\n\nmy var\n❌ No\nSpaces not allowed\n\n\nreturn\n❌ No\nreturn is a reserved Python keyword",
    "crumbs": [
      "Pixel Process",
      "Variables and Functions - Notebook"
    ]
  },
  {
    "objectID": "jump-in/jl-notebooks/jl-build/files/variables-and-functions-nb.html#datatypes",
    "href": "jump-in/jl-notebooks/jl-build/files/variables-and-functions-nb.html#datatypes",
    "title": "Variables and Functions - Notebook",
    "section": "Datatypes",
    "text": "Datatypes\n\n\n\nCategory\nType\nExample\n\n\n\n\nNumbers\nint\n42\n\n\n\nfloat\n3.14\n\n\n\ncomplex\n2 + 3j\n\n\nText\nstr\n\"hello\"\n\n\nSequences\nlist\n[1, 2, 3]\n\n\n\ntuple\n(1, 2, 3)\n\n\n\nrange\nrange(5)\n\n\nSets\nset\n{1, 2, 3}\n\n\n\nfrozenset\nfrozenset([1, 2, 3])\n\n\nMappings\ndict\n{\"a\": 1, \"b\": 2}\n\n\nBoolean\nbool\nTrue, False\n\n\nSpecial\nNoneType\nNone",
    "crumbs": [
      "Pixel Process",
      "Variables and Functions - Notebook"
    ]
  },
  {
    "objectID": "jump-in/jl-notebooks/jl-build/consoles/jupyter-lite.html",
    "href": "jump-in/jl-notebooks/jl-build/consoles/jupyter-lite.html",
    "title": "jupyter-lite.ipynb",
    "section": "",
    "text": "This notebook is the preferred source of site-specific runtime configuration for a JupyterLite app, and will override any configuration in jupyter-lite.json."
  },
  {
    "objectID": "jump-in/jl-notebooks/jl-build/consoles/jupyter-lite.html#editing-configuration",
    "href": "jump-in/jl-notebooks/jl-build/consoles/jupyter-lite.html#editing-configuration",
    "title": "jupyter-lite.ipynb",
    "section": "Editing Configuration",
    "text": "Editing Configuration\nThe configuration is stored in this Notebook’s metadata under the jupyter-lite key. To edit the configuration in JupyterLab.\n\nopen the Property Inspector sidebar\nexpand the Advanced Tools section\nedit the jupyter-lite metadata sub-key\npress the “check” icon\nsave the notebook"
  },
  {
    "objectID": "jump-in/datatypes-and-operators.html",
    "href": "jump-in/datatypes-and-operators.html",
    "title": "Datatypes and Operators",
    "section": "",
    "text": "This is the first part of the Jump In guide introducing core Python concepts in a hands-on way, using Jupyter notebooks as the primary environment. This page will cover datatypes and operators. Be sure to check out the accompanying notebook for more hands-on experience.\n\n\n\n\n\n\nJupyter Notebooks\n\n\n\nNotebooks are a great way to learn and collaborate with code. Be sure to check out Jupyter Notebooks if you are unfamiliar with them, as they are heavily used in this project.",
    "crumbs": [
      "Pixel Process",
      "Datatypes and Operators"
    ]
  },
  {
    "objectID": "jump-in/datatypes-and-operators.html#get-started-with-python---part-1",
    "href": "jump-in/datatypes-and-operators.html#get-started-with-python---part-1",
    "title": "Datatypes and Operators",
    "section": "",
    "text": "This is the first part of the Jump In guide introducing core Python concepts in a hands-on way, using Jupyter notebooks as the primary environment. This page will cover datatypes and operators. Be sure to check out the accompanying notebook for more hands-on experience.\n\n\n\n\n\n\nJupyter Notebooks\n\n\n\nNotebooks are a great way to learn and collaborate with code. Be sure to check out Jupyter Notebooks if you are unfamiliar with them, as they are heavily used in this project.",
    "crumbs": [
      "Pixel Process",
      "Datatypes and Operators"
    ]
  },
  {
    "objectID": "jump-in/datatypes-and-operators.html#datatypes",
    "href": "jump-in/datatypes-and-operators.html#datatypes",
    "title": "Datatypes and Operators",
    "section": "Datatypes",
    "text": "Datatypes\nPython uses datatypes to organize and process information. The datatype can determine how much data is stored, used, and displayed. Picking the right datatype for a use-case makes coding much smoother. Key categories include numeric, strings, and sequences.\n\n\n\n\nCategory\nType\nExample\n\n\n\n\nNumbers\nint\n42\n\n\nNumbers\nfloat\n3.14\n\n\nNumbers\ncomplex\n2 + 3j\n\n\nText\nstr\n“hello”\n\n\nSequences\nlist\n[1, 2, 3]\n\n\nSequences\ntuple\n(1, 2, 3)\n\n\nSequences\nrange\nrange(5)\n\n\nSets\nset\n{1, 2, 3}\n\n\nSets\nfrozenset\nfrozenset([1, 2, 3])\n\n\nMappings\ndict\n{“a”: 1, “b”: 2}\n\n\nBoolean\nbool\nTrue, False\n\n\nSpecial\nNoneType\nNone",
    "crumbs": [
      "Pixel Process",
      "Datatypes and Operators"
    ]
  },
  {
    "objectID": "jump-in/datatypes-and-operators.html#operators",
    "href": "jump-in/datatypes-and-operators.html#operators",
    "title": "Datatypes and Operators",
    "section": "Operators",
    "text": "Operators\nOperations in programming are used to manipulate and evaluate values. These allow programs to perform calculations, make decisions, and control the flow of execution. Types include:\n\nMathematical\nLogical\nComparison\nAssignment\n\n\nMathematical\nMathematical operators perform arithmetic calculations. In Python, these include + (addition), - (subtraction), * (multiplication), / (division), ** or ^ (exponentiation), and % or %% (modulus). Python uses // for integer division.\n\n\n\n\nOperator\nExample\nMeaning\nResult\n\n\n\n\n+\n3 + 2\nAddition\n5\n\n\n-\n7 - 4\nSubtraction\n3\n\n\n*\n6 * 2\nMultiplication\n12\n\n\n/\n7 / 2\nDivision (always float)\n3.5\n\n\n//\n7 // 2\nFloor (integer) division\n3\n\n\n%\n7 % 2\nModulus (remainder)\n1\n\n\n**\n2 ** 3\nExponentiation\n8\n\n\n\n\n\n\nComparison\nComparison operators evaluate relationships between values. Common operators in Python include == (equal), != (not equal), &lt; (less than), &lt;= (less than or equal to), &gt; (greater than), and &gt;= (greater than or equal to).\nThese return boolean values of True/False.\n\n\n\n\nOperator\nExample\nMeaning\nResult\n\n\n\n\n==\n3 == 3\nEqual to\nTrue\n\n\n!=\n3 != 4\nNot equal to\nTrue\n\n\n&gt;\n5 &gt; 2\nGreater than\nTrue\n\n\n&lt;\n2 &lt; 5\nLess than\nTrue\n\n\n&gt;=\n5 &gt;= 5\nGreater than or equal to\nTrue\n\n\n&lt;=\n4 &lt;= 5\nLess than or equal to\nTrue\n\n\n\n\n\n\nLogical\nLogical operators allow for boolean logic. In Python: and, or, not. These are used in conditionals and loops to combine multiple logical expressions.\n\n\n\n\nOperator\nExample\nMeaning\nResult\n\n\n\n\nand\n(3 &gt; 1) and (2 &lt; 5)\nTrue if both are true\nTrue\n\n\nor\n(3 &gt; 1) or (2 &gt; 5)\nTrue if at least one is true\nTrue\n\n\nnot\nnot (3 &gt; 1)\nNegates (True → False)\nFalse\n\n\n\n\n\n\n\n\n\n\nAssignment and Equality\n\n\n\nAssignment uses = to assign a value to a variable. x = 10 creates or updates the variable x to be 10.\nEquality checking uses == and returns a Boolean (True, or False) depending on if the values are equal. x == 10 will return True if x is set as above.",
    "crumbs": [
      "Pixel Process",
      "Datatypes and Operators"
    ]
  },
  {
    "objectID": "foundations/datasets/dataset-exploration.html",
    "href": "foundations/datasets/dataset-exploration.html",
    "title": "Dataset Exploration",
    "section": "",
    "text": "Tabular data, with information organized by rows and columns, provides that basis for many data analysis techniques and models. Usage includes financials, experimental data, and customer records often in the form of Excel sheets, csv files, and more.\nPython has many utilities for working with tabular data including pandas and polars. Examples here will focus on pandas but similar considerations and tools exist in other packages for tabular data.\n\nDataset Flow\n\n\n\n\n\nflowchart LR\n    A([How much data?]) --&gt; B([What's missing?])\n    B --&gt; C([How is it stored?])\n    C --&gt; D([What does it look like?])\n    D --&gt; E([How variable is?])",
    "crumbs": [
      "Pixel Process",
      "Datasets",
      "Dataset Exploration"
    ]
  },
  {
    "objectID": "foundations/faqs.html",
    "href": "foundations/faqs.html",
    "title": "Foundations: Frequently Asked Questions",
    "section": "",
    "text": "If you’re ready to dive into bigger data and projects, get prepared with these FAQs outlining best tools and practices.\n\nToggle All FAQs\n\n\n\n\n\n\n\nWhat is DRY and why does it matter?\n\nDRY stands for ‘Don’t Repeat Yourself’. It’s a principle that encourages reusable, modular code to reduce duplication and improve maintainability.\n\n\n\n\nWhat is code documentation and why should I use it?\n\nDocumentation explains what your code does and why. Good documentation makes it easier to debug, share, and revisit code later.\n\n\n\n\n\n\n\nShould I use notebooks or scripts?\n\nUse notebooks for exploration and explanation. Use scripts for automation, scalability, and production workflows.\n\n\n\n\nWhy is version control important?\n\nVersion control helps you track changes, collaborate with others, and revert to earlier versions if something goes wrong. Git is the most common system.\n\n\n\n\n\n\n\nWhat are the most common data issues?\n\nCommon data issues include missing values, outliers, and incorrectly formatted or malformed data. These can impact analysis and must be handled carefully before modeling.\n\n\n\n\nWhat are summary statistics?\n\nSummary statistics describe and summarize data. Common examples include mean, median, standard deviation, minimum, and maximum.\n\n\n\n\nWhat is basic analysis?\n\nBasic analysis often includes computing correlations, visualizing distributions, identifying trends, and checking assumptions before building models."
  },
  {
    "objectID": "foundations/faqs.html#faqs",
    "href": "foundations/faqs.html#faqs",
    "title": "Foundations: Frequently Asked Questions",
    "section": "",
    "text": "If you’re ready to dive into bigger data and projects, get prepared with these FAQs outlining best tools and practices.\n\nToggle All FAQs\n\n\n\n\n\n\n\nWhat is DRY and why does it matter?\n\nDRY stands for ‘Don’t Repeat Yourself’. It’s a principle that encourages reusable, modular code to reduce duplication and improve maintainability.\n\n\n\n\nWhat is code documentation and why should I use it?\n\nDocumentation explains what your code does and why. Good documentation makes it easier to debug, share, and revisit code later.\n\n\n\n\n\n\n\nShould I use notebooks or scripts?\n\nUse notebooks for exploration and explanation. Use scripts for automation, scalability, and production workflows.\n\n\n\n\nWhy is version control important?\n\nVersion control helps you track changes, collaborate with others, and revert to earlier versions if something goes wrong. Git is the most common system.\n\n\n\n\n\n\n\nWhat are the most common data issues?\n\nCommon data issues include missing values, outliers, and incorrectly formatted or malformed data. These can impact analysis and must be handled carefully before modeling.\n\n\n\n\nWhat are summary statistics?\n\nSummary statistics describe and summarize data. Common examples include mean, median, standard deviation, minimum, and maximum.\n\n\n\n\nWhat is basic analysis?\n\nBasic analysis often includes computing correlations, visualizing distributions, identifying trends, and checking assumptions before building models."
  },
  {
    "objectID": "foundations/foundations-faqs.html",
    "href": "foundations/foundations-faqs.html",
    "title": "Foundations: Frequently Asked Questions",
    "section": "",
    "text": "If you’re ready to dive into bigger data and projects, get prepared with these FAQs outlining best tools and practices.\n\nToggle All FAQs\n\n\n\n\n\n\n\nWhat is DRY and why does it matter?\n\nDRY stands for ‘Don’t Repeat Yourself’. It’s a principle that encourages reusable, modular code to reduce duplication and improve maintainability.\n\n\n\n\nWhat is code documentation and why should I use it?\n\nDocumentation explains what your code does and why. Good documentation makes it easier to debug, share, and revisit code later.\n\n\n\n\n\n\n\nShould I use notebooks or scripts?\n\nUse notebooks for exploration and explanation. Use scripts for automation, scalability, and production workflows.\n\n\n\n\nWhy is version control important?\n\nVersion control helps you track changes, collaborate with others, and revert to earlier versions if something goes wrong. Git is the most common system.\n\n\n\n\n\n\n\nWhat are the most common data issues?\n\nCommon data issues include missing values, outliers, and incorrectly formatted or malformed data. These can impact analysis and must be handled carefully before modeling.\n\n\n\n\nWhat are summary statistics?\n\nSummary statistics describe and summarize data. Common examples include mean, median, standard deviation, minimum, and maximum.\n\n\n\n\nWhat is basic analysis?\n\nBasic analysis often includes computing correlations, visualizing distributions, identifying trends, and checking assumptions before building models.",
    "crumbs": [
      "Pixel Process",
      "FAQs"
    ]
  },
  {
    "objectID": "foundations/foundations-faqs.html#faqs",
    "href": "foundations/foundations-faqs.html#faqs",
    "title": "Foundations: Frequently Asked Questions",
    "section": "",
    "text": "If you’re ready to dive into bigger data and projects, get prepared with these FAQs outlining best tools and practices.\n\nToggle All FAQs\n\n\n\n\n\n\n\nWhat is DRY and why does it matter?\n\nDRY stands for ‘Don’t Repeat Yourself’. It’s a principle that encourages reusable, modular code to reduce duplication and improve maintainability.\n\n\n\n\nWhat is code documentation and why should I use it?\n\nDocumentation explains what your code does and why. Good documentation makes it easier to debug, share, and revisit code later.\n\n\n\n\n\n\n\nShould I use notebooks or scripts?\n\nUse notebooks for exploration and explanation. Use scripts for automation, scalability, and production workflows.\n\n\n\n\nWhy is version control important?\n\nVersion control helps you track changes, collaborate with others, and revert to earlier versions if something goes wrong. Git is the most common system.\n\n\n\n\n\n\n\nWhat are the most common data issues?\n\nCommon data issues include missing values, outliers, and incorrectly formatted or malformed data. These can impact analysis and must be handled carefully before modeling.\n\n\n\n\nWhat are summary statistics?\n\nSummary statistics describe and summarize data. Common examples include mean, median, standard deviation, minimum, and maximum.\n\n\n\n\nWhat is basic analysis?\n\nBasic analysis often includes computing correlations, visualizing distributions, identifying trends, and checking assumptions before building models.",
    "crumbs": [
      "Pixel Process",
      "FAQs"
    ]
  },
  {
    "objectID": "foundations/data-viz/data-viz-basics-nb.html",
    "href": "foundations/data-viz/data-viz-basics-nb.html",
    "title": "Data Visualization Basics: Notebook",
    "section": "",
    "text": "The tips dataset is a built-in example dataset within the Seaborn library, commonly used for demonstrating data visualization concepts. It contains information about restaurant tips, including variables such as:\nNote, some packages and calls take a 2D df, others require 1D data or even aggregate data.\n## Setup and Imports\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n# Load built-in tips dataset\ntips = sns.load_dataset(\"tips\")\ntips.head()\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4",
    "crumbs": [
      "Pixel Process",
      "Data Visualization",
      "Data Visualization Basics: Notebook"
    ]
  },
  {
    "objectID": "foundations/data-viz/data-viz-basics-nb.html#histogram",
    "href": "foundations/data-viz/data-viz-basics-nb.html#histogram",
    "title": "Data Visualization Basics: Notebook",
    "section": "Histogram",
    "text": "Histogram\nHistograms show distribution of a numerical variable.\nNote: ‘;’ at the end of a python statement suppress returned values-this is used in several below examples to focus on the visual.\n\nMatplotlib\n\nplt.hist(tips['total_bill']);\n\n\n\n\n\n\n\n\n\nplt.hist(tips['tip']);\n\n\n\n\n\n\n\n\n\n\nSeaborn\n\nsns.histplot(tips['total_bill'], bins=30);\n\n\n\n\n\n\n\n\n\nsns.histplot(tips['tip'], bins=30);\n\n\n\n\n\n\n\n\n\n\nPlotly\n\npx.histogram(tips, x='total_bill')\n\n\n\n\n\n\n\n\n\npx.histogram(tips, x='tip')",
    "crumbs": [
      "Pixel Process",
      "Data Visualization",
      "Data Visualization Basics: Notebook"
    ]
  },
  {
    "objectID": "foundations/data-viz/data-viz-basics-nb.html#bar-chart",
    "href": "foundations/data-viz/data-viz-basics-nb.html#bar-chart",
    "title": "Data Visualization Basics: Notebook",
    "section": "Bar Chart",
    "text": "Bar Chart\nBar plots show aggregate values by category.\n\nMatplotlib\n\nplt.bar(tips['day'], tips['total_bill'])\n\n\n\n\n\n\n\n\n\n\nSeaborn\n\nsns.barplot(x='day', y='total_bill', data=tips)\n\n\n\n\n\n\n\n\n\n\nPlotly\n\n# Aggregate data for this plot\nagg = tips.groupby(\"day\", as_index=False)[\"total_bill\"].mean()\nagg\n\n\n\n\n\n\n\n\nday\ntotal_bill\n\n\n\n\n0\nThur\n17.682742\n\n\n1\nFri\n17.151579\n\n\n2\nSat\n20.441379\n\n\n3\nSun\n21.410000\n\n\n\n\n\n\n\n\npx.bar(x=agg['day'], y=agg['total_bill'])\n\n\n\n\n\n\n\n\n\n\nExplore the Data: Customize a graph?\n\n\nDo you see any other interesting comparisons to try? Try changing the x-axis to time instead of day. Or see if tips reflects total_bill by day.\n\n\n\n\nMatplotlib\n\n# Count Plot with Matplotlib\n\n# Example placeholder - customize per chart type\n# Replace with appropriate matplotlib code for Count Plot\nplt.title(\"Count Plot - Matplotlib\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Count Plot with Seaborn\n\n# Replace with appropriate seaborn code for Count Plot\nplt.title(\"Count Plot - Seaborn\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPlotly\n\n# Count Plot with Plotly Express\n# Replace with appropriate plotly.express code for Count Plot\n# e.g., px.bar(...), px.scatter(...)",
    "crumbs": [
      "Pixel Process",
      "Data Visualization",
      "Data Visualization Basics: Notebook"
    ]
  },
  {
    "objectID": "foundations/data-viz/data-viz-basics-nb.html#box-plot",
    "href": "foundations/data-viz/data-viz-basics-nb.html#box-plot",
    "title": "Data Visualization Basics: Notebook",
    "section": "Box Plot",
    "text": "Box Plot\nBox plots summarize distributions and highlight outliers.\nIt can be extra helpful to compare mutliple values, a grouped values.\n\nMatplotlib\n\n# Box Plot with Matplotlib\nplt.boxplot(tips['total_bill']);\n\n\n\n\n\n\n\n\n\n# Box Plot with Matplotlib\nplt.boxplot(x=tips[['total_bill', 'tip']]);\n\n\n\n\n\n\n\n\n\n\nSeaborn\n\nsns.boxplot(x='total_bill', data=tips);\n\n\n\n\n\n\n\n\n\n# Bonus Tip: Seaborn will plot all numerical columns in a wide data format if no x given\nsns.boxplot(data=tips);\n\n\n\n\n\n\n\n\n\n\nPlotly\n\npx.box(tips, x='total_bill')\n\n\n\n\n\n\n\n\n\npx.box(tips, x='day', y='total_bill')",
    "crumbs": [
      "Pixel Process",
      "Data Visualization",
      "Data Visualization Basics: Notebook"
    ]
  },
  {
    "objectID": "foundations/data-viz/data-viz-basics-nb.html#violin-plot",
    "href": "foundations/data-viz/data-viz-basics-nb.html#violin-plot",
    "title": "Data Visualization Basics: Notebook",
    "section": "Violin Plot",
    "text": "Violin Plot\nViolin plots combine box plots with a KDE plot.\n\nMatplotlib\n\nplt.violinplot(tips[['total_bill', 'tip']]);\n\n\n\n\n\n\n\n\n\n\nSeaborn\n\nsns.violinplot(x='day', y='total_bill', data=tips);\n\n\n\n\n\n\n\n\n\n\nPlotly\n\npx.violin(tips, x='day', y='total_bill', box=True)",
    "crumbs": [
      "Pixel Process",
      "Data Visualization",
      "Data Visualization Basics: Notebook"
    ]
  },
  {
    "objectID": "foundations/data-viz/data-viz-basics-nb.html#scatter-plot",
    "href": "foundations/data-viz/data-viz-basics-nb.html#scatter-plot",
    "title": "Data Visualization Basics: Notebook",
    "section": "Scatter Plot",
    "text": "Scatter Plot\nScatter plots show relationships between two numerical variables.\n\nMatplotlib\n\nplt.scatter(tips['total_bill'], y=tips['tip'])\n\n\n\n\n\n\n\n\n\n\nSeaborn\n\nsns.scatterplot(x='total_bill', y='tip', data=tips)\n\n\n\n\n\n\n\n\n\n\nPlotly\n\npx.scatter(tips, x='total_bill', y='tip')",
    "crumbs": [
      "Pixel Process",
      "Data Visualization",
      "Data Visualization Basics: Notebook"
    ]
  },
  {
    "objectID": "foundations/data-viz/data-viz-basics-nb.html#pie-chart",
    "href": "foundations/data-viz/data-viz-basics-nb.html#pie-chart",
    "title": "Data Visualization Basics: Notebook",
    "section": "Pie Chart",
    "text": "Pie Chart\nDo NOT Use\nThere are always much better, clearer ways to present data.",
    "crumbs": [
      "Pixel Process",
      "Data Visualization",
      "Data Visualization Basics: Notebook"
    ]
  },
  {
    "objectID": "foundations/data-viz/data-viz-basics.html",
    "href": "foundations/data-viz/data-viz-basics.html",
    "title": "Data Visualization Basics",
    "section": "",
    "text": "Data visualization is essential for exploring data, identifying patterns, and communicating insights. Python offers a rich ecosystem of libraries for creating everything from basic charts to interactive dashboards.\nSee the associated companion notebook for more extensive code examples.\n\n\n\nExploration: Understand the shape and distribution of your data.\nCommunication: Present insights effectively to different audiences.\nValidation: Check model assumptions and evaluate results.",
    "crumbs": [
      "Pixel Process",
      "Data Visualization",
      "Data Visualization Basics"
    ]
  },
  {
    "objectID": "foundations/data-viz/data-viz-basics.html#data-visualization-in-python",
    "href": "foundations/data-viz/data-viz-basics.html#data-visualization-in-python",
    "title": "Data Visualization Basics",
    "section": "",
    "text": "Data visualization is essential for exploring data, identifying patterns, and communicating insights. Python offers a rich ecosystem of libraries for creating everything from basic charts to interactive dashboards.\nSee the associated companion notebook for more extensive code examples.\n\n\n\nExploration: Understand the shape and distribution of your data.\nCommunication: Present insights effectively to different audiences.\nValidation: Check model assumptions and evaluate results.",
    "crumbs": [
      "Pixel Process",
      "Data Visualization",
      "Data Visualization Basics"
    ]
  },
  {
    "objectID": "foundations/data-viz/data-viz-basics.html#core-libraries",
    "href": "foundations/data-viz/data-viz-basics.html#core-libraries",
    "title": "Data Visualization Basics",
    "section": "Core Libraries",
    "text": "Core Libraries\nThree essential libraries for Python visualization are:\n\nMatplotlib: The foundational plotting library.\nSeaborn: Simplifies statistical plots with attractive defaults.\nPlotly: For interactive and web-ready graphics.\n\n\nMatplotlib\nMatplotlib is the foundational plotting library in Python, supporting a wide range of static, animated, and interactive visualizations.\n\n\nSeaborn\nSeaborn is built on Matplotlib and provides a high-level API for creating attractive statistical graphics.\n\n\nPlotly\nPlotly creates interactive plots ideal for dashboards and web applications.",
    "crumbs": [
      "Pixel Process",
      "Data Visualization",
      "Data Visualization Basics"
    ]
  },
  {
    "objectID": "foundations/data-viz/data-viz-basics.html#common-plot-types",
    "href": "foundations/data-viz/data-viz-basics.html#common-plot-types",
    "title": "Data Visualization Basics",
    "section": "Common Plot Types",
    "text": "Common Plot Types\nBelow are essential plots for data analysis. Each section includes data requirements, ideal use cases, examples for multiple Python libraries, and an example image.\n\nHistogram\nHistograms display the distribution of a dataset.\n\nData RequirementsPackage ExamplesIdeal Uses\n\n\n\n\nType: Univariate\nVariables:\n\nSingle continuous variable\n\n\n\n\n\n\n\nMatplotlib\n\nimport matplotlib.pyplot as plt\nimport numpy as np\ndata = np.random.randn(1000)\nplt.hist(data, bins=30)\nplt.show()\n\n\n\nSeaborn\n\nimport seaborn as sns\ndata = sns.load_dataset('tips')\nsns.histplot(data['total_bill'], bins=30)\n\n\n\nPlotly\n\nimport plotly.express as px\ndf = px.data.tips()\nfig = px.histogram(df, x='total_bill')\nfig.show()\n\n\n\n\n\nVisualize data distribution\nIdentify skewness and outliers\n\n\n\n\n\n\n\nBar Chart\nBar charts represent categorical data with rectangular bars.\n\nData RequirementsPackage ExamplesIdeal Uses\n\n\n\n\nType: Univariate or Bivariate\nVariables:\n\nx: Categorical\ny: Continuous\n\nSuitable for counts or aggregated values.\n\n\n\n\n\n\nMatplotlib\n\nimport matplotlib.pyplot as plt\ncategories = ['A','B','C']\nvalues = [3,7,5]\nplt.bar(categories, values)\nplt.show()\n\n\n\nSeaborn\n\nimport seaborn as sns\ndata = sns.load_dataset('tips')\nsns.barplot(x='day', y='total_bill', data=data)\n\n\n\nPlotly\n\nimport plotly.express as px\ncategories = ['A','B','C']\nvalues = [3,7,5]\nfig = px.bar(x=categories, y=values)\nfig.show()\n\n\n\n\n\nCompare categories\nShow frequency or aggregated values\nIdentify highest and lowest categories\n\n\n\n\n\n\n\nBox Plot\nBox plots summarize data with median, quartiles, and outliers.\n\nData RequirementsPackage ExamplesIdeal Uses\n\n\n\n\nType: Univariate or Bivariate\nVariables:\n\nx: Categorical (optional)\ny: Continuous\n\n\n\n\n\n\n\nMatplotlib\n\nimport matplotlib.pyplot as plt\ndata = [7,8,5,6,4,9]\nplt.boxplot(data)\nplt.show()\n\n\n\nSeaborn\n\nimport seaborn as sns\ndata = sns.load_dataset('tips')\nsns.boxplot(x='day', y='total_bill', data=data)\n\n\n\nPlotly\n\nimport plotly.express as px\ndf = px.data.tips()\nfig = px.box(df, x='day', y='total_bill')\nfig.show()\n\n\n\n\n\nIdentify outliers\nCompare distributions across groups\n\n\n\n\n\n\n\nViolin Plot\nViolin plots combine box plots with kernel density estimates.\n\nData RequirementsPackage ExamplesIdeal Uses\n\n\n\n\nType: Bivariate\nVariables:\n\nx: Categorical\ny: Continuous\n\n\n\n\n\n\n\nMatplotlib\n\nMatplotlib does not have built-in violin plot\n\n\n\nSeaborn\n\nimport seaborn as sns\ndata = sns.load_dataset('tips')\nsns.violinplot(x='day', y='total_bill', data=data)\n\n\n\nPlotly\n\nimport plotly.express as px\ndf = px.data.tips()\nfig = px.violin(df, x='day', y='total_bill', box=True)\nfig.show()\n\n\n\n\n\nVisualize distribution and density\nCompare across categories\n\n\n\n\n\n\n\nScatter Plot\nScatter plots show the relationship between two variables.\n\nData RequirementsPackage ExamplesIdeal Uses\n\n\n\n\nType: Bivariate\nVariables:\n\nx: Continuous\ny: Continuous\n\n\n\n\n\n\n\nMatplotlib\n\npython\nimport matplotlib.pyplot as plt\nx = [1,2,3,4,5]\ny = [5,4,6,5,7]\nplt.scatter(x,y)\nplt.show()\n\n\n\nSeaborn\n\nimport seaborn as sns\ndata = sns.load_dataset('iris')\nsns.scatterplot(x='sepal_length', y='petal_length', data=data)\n\n\n\nPlotly\n\nimport plotly.express as px\ncategories = ['A','B','C']\nvalues = [3,7,5]\nfig = px.scatter(tips, x='total_bill', y='tip')\nfig.show()\n\n\n\n\n\nIdentify correlations\nDetect clusters or outliers",
    "crumbs": [
      "Pixel Process",
      "Data Visualization",
      "Data Visualization Basics"
    ]
  },
  {
    "objectID": "getting-started/faqs.html",
    "href": "getting-started/faqs.html",
    "title": "Getting Started: Frequently Asked Questions",
    "section": "",
    "text": "Learning to program is a big decision. Initially, things can seem overwhelming. This section is designed to address the most common questions that come up early on.\n\nToggle All FAQs\n\n\n\n\n\n\n\nWhat language should I learn?\n\nPython is a great starting point due to its readability, versatility, and large ecosystem. But, the right tool depends on the job. Remember, many programming concepts apply in many languages, so getting started in any can help get you started in all.\nCheck out the TIOBE Index for more information on popular languages and trends.\n\n\n\n\nIs learning to code worth it if I don’t want to be a programmer?\n\nYes. Basic computer and programming literacy is valuable in today’s tech-driven world, even if you don’t plan to work as a full-time developer.\n\n\n\n\nDo I need a computer science degree to get into data science?\n\nNo. Many data scientists come from psychology, economics, or biology. Curiosity and consistent practice matter more.\n\n\n\n\nIs Python the best language for beginners?\n\nYes, in many cases. It’s readable, has lots of learning resources, and is used in data science, web, and automation.\n\n\n\n\nIs it too late to learn programming?\n\nNever. Many adult learners succeed — you likely bring real-world context and discipline younger learners lack.\n\n\n\n\nHow long does it take to learn programming?\n\nIt depends on your goals, but you can be productive in weeks. Keep learning in layers and build small projects.\n\n\n\n\n\n\n\nDo I need to know statistics?\n\nA basic understanding of stats helps you consume and create data responsibly. But you don’t need deep expertise to be effective.\n\n\n\n\nI am not a numbers person, can I still code?\n\nYes. Coding is more about logic and patterns than complex math. Python especially is beginner-friendly.\n\n\n\n\nHow much math do I need to know?\n\nBasic algebra, probability, and statistics are often enough. You’ll learn what you need along the way.\n\n\n\n\nDo I need to memorize syntax?\n\nNo. Focus on understanding logic and patterns. You’ll look up syntax often — even professionals do.\n\n\n\n\nWhat if I’m bad at math or logic?\n\nYou can still learn to code. These skills improve with practice. Start small and keep going.\n\n\n\n\nWhy does my code break all the time?\n\nThat’s normal. Debugging is part of learning. Error messages teach you what went wrong and how to fix it.\n\n\n\n\nShould I use notebooks or scripts?\n\nNotebooks are great for exploration and explanation. Scripts are better for automation and production use.\n\n\n\n\n\n\n\nFrontend\n\nFrontend development involves building the parts of a website or application that users interact with directly — typically using HTML, CSS, and JavaScript. Frameworks like React, Vue, and Angular are popular tools in this area.\n\n\n\n\nBackend\n\nBackend development focuses on server-side logic, databases, and APIs — the behind-the-scenes work that powers an application. Common languages include Python, Java, Node.js, and tools like SQL, Docker, and cloud services.\n\n\n\n\nFull-Stack\n\nFull-stack developers work on both the frontend and backend of an application. They often know a mix of languages and frameworks, and can build entire systems from databases to user interfaces.\n\n\n\n\nData Scientist\n\nData scientists analyze data to extract insights, build models, and make predictions. They work with statistics, machine learning, and tools like Python, pandas, scikit-learn, and cloud platforms.\n\n\n\n\nData Engineer\n\nData engineers build and manage systems that collect, store, and process data. They focus on pipelines, databases, and infrastructure, using tools like SQL, Spark, Airflow, and cloud services.\n\n\n\n\nData Analyst\n\nData analysts explore and interpret data to support business decisions. They use SQL, Excel, and visualization tools like Tableau or Power BI to generate reports and insights.\n\n\n\n\nDevOps\n\nDevOps engineers work at the intersection of development and IT operations. They automate deployment, monitor systems, and ensure reliability using tools like Docker, Kubernetes, CI/CD pipelines, and cloud platforms.\n\n\n\n\nMachine Learning (ML)\n\nMachine learning is a subset of AI that enables systems to learn from data. It involves supervised, unsupervised, and reinforcement learning, often using Python libraries like scikit-learn, TensorFlow, and PyTorch.\n\n\n\n\nArtificial Intelligence (AI)\n\nArtificial Intelligence is a broad field focused on creating systems that simulate human intelligence — including reasoning, learning, and problem-solving. ML, NLP, and computer vision are subfields of AI.\n\n\n\n\nComputer Vision\n\nComputer vision is a field of AI that enables computers to interpret and process visual information from the world, such as images and videos. Applications include facial recognition, object detection, and OCR.\n\n\n\n\nNatural Language Processing (NLP)\n\nNLP enables computers to understand, interpret, and generate human language. It powers tools like chatbots, sentiment analysis, and language translation using libraries like spaCy, NLTK, and transformers.",
    "crumbs": [
      "Pixel Process",
      "FAQs"
    ]
  },
  {
    "objectID": "getting-started/faqs.html#faqs",
    "href": "getting-started/faqs.html#faqs",
    "title": "Getting Started: Frequently Asked Questions",
    "section": "",
    "text": "Learning to program is a big decision. Initially, things can seem overwhelming. This section is designed to address the most common questions that come up early on.\n\nToggle All FAQs\n\n\n\n\n\n\n\nWhat language should I learn?\n\nPython is a great starting point due to its readability, versatility, and large ecosystem. But, the right tool depends on the job. Remember, many programming concepts apply in many languages, so getting started in any can help get you started in all.\nCheck out the TIOBE Index for more information on popular languages and trends.\n\n\n\n\nIs learning to code worth it if I don’t want to be a programmer?\n\nYes. Basic computer and programming literacy is valuable in today’s tech-driven world, even if you don’t plan to work as a full-time developer.\n\n\n\n\nDo I need a computer science degree to get into data science?\n\nNo. Many data scientists come from psychology, economics, or biology. Curiosity and consistent practice matter more.\n\n\n\n\nIs Python the best language for beginners?\n\nYes, in many cases. It’s readable, has lots of learning resources, and is used in data science, web, and automation.\n\n\n\n\nIs it too late to learn programming?\n\nNever. Many adult learners succeed — you likely bring real-world context and discipline younger learners lack.\n\n\n\n\nHow long does it take to learn programming?\n\nIt depends on your goals, but you can be productive in weeks. Keep learning in layers and build small projects.\n\n\n\n\n\n\n\nDo I need to know statistics?\n\nA basic understanding of stats helps you consume and create data responsibly. But you don’t need deep expertise to be effective.\n\n\n\n\nI am not a numbers person, can I still code?\n\nYes. Coding is more about logic and patterns than complex math. Python especially is beginner-friendly.\n\n\n\n\nHow much math do I need to know?\n\nBasic algebra, probability, and statistics are often enough. You’ll learn what you need along the way.\n\n\n\n\nDo I need to memorize syntax?\n\nNo. Focus on understanding logic and patterns. You’ll look up syntax often — even professionals do.\n\n\n\n\nWhat if I’m bad at math or logic?\n\nYou can still learn to code. These skills improve with practice. Start small and keep going.\n\n\n\n\nWhy does my code break all the time?\n\nThat’s normal. Debugging is part of learning. Error messages teach you what went wrong and how to fix it.\n\n\n\n\nShould I use notebooks or scripts?\n\nNotebooks are great for exploration and explanation. Scripts are better for automation and production use.\n\n\n\n\n\n\n\nFrontend\n\nFrontend development involves building the parts of a website or application that users interact with directly — typically using HTML, CSS, and JavaScript. Frameworks like React, Vue, and Angular are popular tools in this area.\n\n\n\n\nBackend\n\nBackend development focuses on server-side logic, databases, and APIs — the behind-the-scenes work that powers an application. Common languages include Python, Java, Node.js, and tools like SQL, Docker, and cloud services.\n\n\n\n\nFull-Stack\n\nFull-stack developers work on both the frontend and backend of an application. They often know a mix of languages and frameworks, and can build entire systems from databases to user interfaces.\n\n\n\n\nData Scientist\n\nData scientists analyze data to extract insights, build models, and make predictions. They work with statistics, machine learning, and tools like Python, pandas, scikit-learn, and cloud platforms.\n\n\n\n\nData Engineer\n\nData engineers build and manage systems that collect, store, and process data. They focus on pipelines, databases, and infrastructure, using tools like SQL, Spark, Airflow, and cloud services.\n\n\n\n\nData Analyst\n\nData analysts explore and interpret data to support business decisions. They use SQL, Excel, and visualization tools like Tableau or Power BI to generate reports and insights.\n\n\n\n\nDevOps\n\nDevOps engineers work at the intersection of development and IT operations. They automate deployment, monitor systems, and ensure reliability using tools like Docker, Kubernetes, CI/CD pipelines, and cloud platforms.\n\n\n\n\nMachine Learning (ML)\n\nMachine learning is a subset of AI that enables systems to learn from data. It involves supervised, unsupervised, and reinforcement learning, often using Python libraries like scikit-learn, TensorFlow, and PyTorch.\n\n\n\n\nArtificial Intelligence (AI)\n\nArtificial Intelligence is a broad field focused on creating systems that simulate human intelligence — including reasoning, learning, and problem-solving. ML, NLP, and computer vision are subfields of AI.\n\n\n\n\nComputer Vision\n\nComputer vision is a field of AI that enables computers to interpret and process visual information from the world, such as images and videos. Applications include facial recognition, object detection, and OCR.\n\n\n\n\nNatural Language Processing (NLP)\n\nNLP enables computers to understand, interpret, and generate human language. It powers tools like chatbots, sentiment analysis, and language translation using libraries like spaCy, NLTK, and transformers.",
    "crumbs": [
      "Pixel Process",
      "FAQs"
    ]
  },
  {
    "objectID": "getting-started/r/data-analysis.html",
    "href": "getting-started/r/data-analysis.html",
    "title": "R: Data Analysis",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\nNote: if you have not installed the finalfit library, use install.packages(\"corrplot\").\n\nlibrary(conflicted)\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"lag\", \"dplyr\")\n\n\n[conflicted] Will prefer dplyr::filter over any other package.\n\n[conflicted] Will prefer dplyr::lag over any other package.\n\n\n\n\n\ninstall.packages(\"corrplot\")\n\n\nThe downloaded binary packages are in\n    /var/folders/dz/k8c4rxzs27v3q7ly5w31c95m0000gn/T//Rtmpdr6WX1/downloaded_packages\n\n\n\nlibrary(tidyverse)\nlibrary(corrplot)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n\n✔ purrr     1.1.0     \n\ncorrplot 0.95 loaded"
  },
  {
    "objectID": "getting-started/r/data-analysis.html#r-markdown",
    "href": "getting-started/r/data-analysis.html#r-markdown",
    "title": "R: Data Analysis",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\nNote: if you have not installed the finalfit library, use install.packages(\"corrplot\").\n\nlibrary(conflicted)\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"lag\", \"dplyr\")\n\n\n[conflicted] Will prefer dplyr::filter over any other package.\n\n[conflicted] Will prefer dplyr::lag over any other package.\n\n\n\n\n\ninstall.packages(\"corrplot\")\n\n\nThe downloaded binary packages are in\n    /var/folders/dz/k8c4rxzs27v3q7ly5w31c95m0000gn/T//Rtmpdr6WX1/downloaded_packages\n\n\n\nlibrary(tidyverse)\nlibrary(corrplot)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n\n✔ purrr     1.1.0     \n\ncorrplot 0.95 loaded"
  },
  {
    "objectID": "getting-started/r/data-analysis.html#mtcars-data",
    "href": "getting-started/r/data-analysis.html#mtcars-data",
    "title": "R: Data Analysis",
    "section": "mtcars data",
    "text": "mtcars data\nWe will use several different data frames for this session as various data formats are needed. Including the introduction of the mtcars data frame.\nLet’s start by exploring the data.\n\nmtcars\n\n\nA data.frame: 32 × 11\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\nDuster 360\n14.3\n8\n360.0\n245\n3.21\n3.570\n15.84\n0\n0\n3\n4\n\n\nMerc 240D\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\nMerc 230\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\nMerc 280\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\nMerc 280C\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\nMerc 450SE\n16.4\n8\n275.8\n180\n3.07\n4.070\n17.40\n0\n0\n3\n3\n\n\nMerc 450SL\n17.3\n8\n275.8\n180\n3.07\n3.730\n17.60\n0\n0\n3\n3\n\n\nMerc 450SLC\n15.2\n8\n275.8\n180\n3.07\n3.780\n18.00\n0\n0\n3\n3\n\n\nCadillac Fleetwood\n10.4\n8\n472.0\n205\n2.93\n5.250\n17.98\n0\n0\n3\n4\n\n\nLincoln Continental\n10.4\n8\n460.0\n215\n3.00\n5.424\n17.82\n0\n0\n3\n4\n\n\nChrysler Imperial\n14.7\n8\n440.0\n230\n3.23\n5.345\n17.42\n0\n0\n3\n4\n\n\nFiat 128\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\nHonda Civic\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n\n\nToyota Corolla\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n\n\nToyota Corona\n21.5\n4\n120.1\n97\n3.70\n2.465\n20.01\n1\n0\n3\n1\n\n\nDodge Challenger\n15.5\n8\n318.0\n150\n2.76\n3.520\n16.87\n0\n0\n3\n2\n\n\nAMC Javelin\n15.2\n8\n304.0\n150\n3.15\n3.435\n17.30\n0\n0\n3\n2\n\n\nCamaro Z28\n13.3\n8\n350.0\n245\n3.73\n3.840\n15.41\n0\n0\n3\n4\n\n\nPontiac Firebird\n19.2\n8\n400.0\n175\n3.08\n3.845\n17.05\n0\n0\n3\n2\n\n\nFiat X1-9\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\nPorsche 914-2\n26.0\n4\n120.3\n91\n4.43\n2.140\n16.70\n0\n1\n5\n2\n\n\nLotus Europa\n30.4\n4\n95.1\n113\n3.77\n1.513\n16.90\n1\n1\n5\n2\n\n\nFord Pantera L\n15.8\n8\n351.0\n264\n4.22\n3.170\n14.50\n0\n1\n5\n4\n\n\nFerrari Dino\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n\n\nMaserati Bora\n15.0\n8\n301.0\n335\n3.54\n3.570\n14.60\n0\n1\n5\n8\n\n\nVolvo 142E\n21.4\n4\n121.0\n109\n4.11\n2.780\n18.60\n1\n1\n4\n2\n\n\n\n\n\nUse the below code to see what the columns represent in the help pane.\n\n?mtcars\n\n\n\n\nmtcars {datasets}\nR Documentation\n\n\n\n\n\nMotor Trend Car Road Tests\n\nDescription\n\nThe data was extracted from the 1974 Motor Trend US magazine,\nand comprises fuel consumption and 10 aspects of\nautomobile design and performance for 32 automobiles (1973–74\nmodels).\n\n\n\nUsage\n\nmtcars\n\n\nFormat\n\nA data frame with 32 observations on 11 (numeric) variables.\n\n\n\n\n\n\n[, 1]\nmpg\nMiles/(US) gallon\n\n\n[, 2]\ncyl\nNumber of cylinders\n\n\n[, 3]\ndisp\nDisplacement (cu.in.)\n\n\n[, 4]\nhp\nGross horsepower\n\n\n[, 5]\ndrat\nRear axle ratio\n\n\n[, 6]\nwt\nWeight (1000 lbs)\n\n\n[, 7]\nqsec\n1/4 mile time\n\n\n[, 8]\nvs\nEngine (0 = V-shaped, 1 = straight)\n\n\n[, 9]\nam\nTransmission (0 = automatic, 1 = manual)\n\n\n[,10]\ngear\nNumber of forward gears\n\n\n[,11]\ncarb\nNumber of carburetors\n\n\n\n\n\n\n\nNote\n\nHenderson and Velleman (1981) comment in a footnote to Table 1:\n‘Hocking [original transcriber]'s noncrucial coding of the\nMazda's rotary engine as a straight six-cylinder engine and the\nPorsche's flat engine as a V engine, as well as the inclusion of the\ndiesel Mercedes 240D, have been retained to enable direct comparisons\nto be made with previous analyses.’\n\n\n\nSource\n\nHenderson and Velleman (1981),\nBuilding multiple regression models interactively.\nBiometrics, 37, 391–411.\n\n\n\nExamples\n\nrequire(graphics)\npairs(mtcars, main = \"mtcars data\", gap = 1/4)\ncoplot(mpg ~ disp | as.factor(cyl), data = mtcars,\n       panel = panel.smooth, rows = 1)\n## possibly more meaningful, e.g., for summary() or bivariate plots:\nmtcars2 &lt;- within(mtcars, {\n   vs &lt;- factor(vs, labels = c(\"V\", \"S\"))\n   am &lt;- factor(am, labels = c(\"automatic\", \"manual\"))\n   cyl  &lt;- ordered(cyl)\n   gear &lt;- ordered(gear)\n   carb &lt;- ordered(carb)\n})\nsummary(mtcars2)\n\n\n[Package datasets version 4.5.1 ]"
  },
  {
    "objectID": "getting-started/r/data-analysis.html#correlation",
    "href": "getting-started/r/data-analysis.html#correlation",
    "title": "R: Data Analysis",
    "section": "Correlation",
    "text": "Correlation\nCorrelations range from 1 to -1. A correlation coefficient of 1 means the two variables track perfectly in a positive direction. -1 represents two variables that are inversely related to each other (as one goes up the other goes down).\nUse the cor command on a data frame to get the correlations of all numeric columns in the data frame. Note: the diagnol should always be 1 as each variable should correlate perfectly with itself. Note: The data in the matrix is redundant. The top half and bottom half show the same relationships.\n\ncar_corr = cor(mtcars)\ncar_corr\n\n\nA matrix: 11 × 11 of type dbl\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nmpg\n1.0000000\n-0.8521620\n-0.8475514\n-0.7761684\n0.68117191\n-0.8676594\n0.41868403\n0.6640389\n0.59983243\n0.4802848\n-0.55092507\n\n\ncyl\n-0.8521620\n1.0000000\n0.9020329\n0.8324475\n-0.69993811\n0.7824958\n-0.59124207\n-0.8108118\n-0.52260705\n-0.4926866\n0.52698829\n\n\ndisp\n-0.8475514\n0.9020329\n1.0000000\n0.7909486\n-0.71021393\n0.8879799\n-0.43369788\n-0.7104159\n-0.59122704\n-0.5555692\n0.39497686\n\n\nhp\n-0.7761684\n0.8324475\n0.7909486\n1.0000000\n-0.44875912\n0.6587479\n-0.70822339\n-0.7230967\n-0.24320426\n-0.1257043\n0.74981247\n\n\ndrat\n0.6811719\n-0.6999381\n-0.7102139\n-0.4487591\n1.00000000\n-0.7124406\n0.09120476\n0.4402785\n0.71271113\n0.6996101\n-0.09078980\n\n\nwt\n-0.8676594\n0.7824958\n0.8879799\n0.6587479\n-0.71244065\n1.0000000\n-0.17471588\n-0.5549157\n-0.69249526\n-0.5832870\n0.42760594\n\n\nqsec\n0.4186840\n-0.5912421\n-0.4336979\n-0.7082234\n0.09120476\n-0.1747159\n1.00000000\n0.7445354\n-0.22986086\n-0.2126822\n-0.65624923\n\n\nvs\n0.6640389\n-0.8108118\n-0.7104159\n-0.7230967\n0.44027846\n-0.5549157\n0.74453544\n1.0000000\n0.16834512\n0.2060233\n-0.56960714\n\n\nam\n0.5998324\n-0.5226070\n-0.5912270\n-0.2432043\n0.71271113\n-0.6924953\n-0.22986086\n0.1683451\n1.00000000\n0.7940588\n0.05753435\n\n\ngear\n0.4802848\n-0.4926866\n-0.5555692\n-0.1257043\n0.69961013\n-0.5832870\n-0.21268223\n0.2060233\n0.79405876\n1.0000000\n0.27407284\n\n\ncarb\n-0.5509251\n0.5269883\n0.3949769\n0.7498125\n-0.09078980\n0.4276059\n-0.65624923\n-0.5696071\n0.05753435\n0.2740728\n1.00000000\n\n\n\n\n\nIt can be very helpful to visualize the correlations. Here we use the corrplot command from the corrplot package. THe darker the color, the stronger the correlation. Red represents negative correlations while blue is used for positive correlations.\n\ncorrplot(car_corr, method=\"number\", type=\"lower\")\n\n\n\n\n\n\n\n\nTo get more details on a single correlation and to test it’s significance, use the cor.test command.\n\ncor.test(mtcars$mpg, mtcars$hp)\n\n\n    Pearson's product-moment correlation\n\ndata:  mtcars$mpg and mtcars$hp\nt = -6.7424, df = 30, p-value = 1.788e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.8852686 -0.5860994\nsample estimates:\n       cor \n-0.7761684 \n\n\nThis shows a significant correlation (p value less than .05).\nLet’s visualize the relationship!\n\nggplot(mtcars, aes(x=mpg, y=hp)) + geom_point() + geom_smooth(method=lm)\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "getting-started/r/data-analysis.html#t-tests",
    "href": "getting-started/r/data-analysis.html#t-tests",
    "title": "R: Data Analysis",
    "section": "t-tests",
    "text": "t-tests\n\nSingle sample t-test\nFor a single sample t-test, a vector of numeric numbers is tested against a known value (mu). The test determines if the vector differs from the set/known value.\nLet’s take a look at the weight of the cars in the dataset with a histogram to begin.\n\nhist(mtcars$wt)\n\n\n\n\n\n\n\n\nIt looks like the distribution centers around 3, so let’s test the weights against the value of 3.\n\nt.test(mtcars$wt, mu=3)\n\n\n    One Sample t-test\n\ndata:  mtcars$wt\nt = 1.256, df = 31, p-value = 0.2185\nalternative hypothesis: true mean is not equal to 3\n95 percent confidence interval:\n 2.864478 3.570022\nsample estimates:\nmean of x \n  3.21725 \n\n\nHere, we see that the values do not differ from 3 (p-value greater than .05). This makes sense since we chose a mu that based on the data.\nIn the real-world, this type of analysis may be useful to see if a sample differs from a known population value. For example, if the height residents of a country differ from the known global average.\n\n\nTwo samples independent t-test\nA two samples independent t-test is used to compare two vectors and see if they come from the same sample.\nFor this example, we will compare V-shaped engines with straight engines on mpg.\nHere we can see that the column vs has two levels, 0 and 1.\n\nunique(mtcars$vs)\n\n\n01\n\n\nLet’s also find out how many of each type there are.\n\ntable(mtcars$vs)\n\n\n 0  1 \n18 14 \n\n\nThere are two ways to run the t-test. We can specify a numeric column and a binary identifier column for groups or we can use two numeric vectors.\nBoth methods will be demonstrated here, although they should yeild identical results.\n\nt.test(mtcars$mpg~mtcars$vs)\n\n\n    Welch Two Sample t-test\n\ndata:  mtcars$mpg by mtcars$vs\nt = -4.6671, df = 22.716, p-value = 0.0001098\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -11.462508  -4.418445\nsample estimates:\nmean in group 0 mean in group 1 \n       16.61667        24.55714 \n\n\nWe see from the results, that there is a significant difference in mpg based on engine type. The means at the bottom tell us the average for each type.\nNext, remember that you can subset data using a condition and column selection. We will create two vectors, one for each engine type and re-run the analysis. This is for demonstration purposes, you can choose either option and get the same results.\n\nv_mpg = mtcars[mtcars$vs==0, \"mpg\"]\nv_mpg\ns_mpg = mtcars[mtcars$vs==1, \"mpg\"]\ns_mpg\nt.test(v_mpg, s_mpg)\n\n\n212118.714.316.417.315.210.410.414.715.515.213.319.22615.819.715\n\n\n\n22.821.418.124.422.819.217.832.430.433.921.527.330.421.4\n\n\n\n    Welch Two Sample t-test\n\ndata:  v_mpg and s_mpg\nt = -4.6671, df = 22.716, p-value = 0.0001098\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -11.462508  -4.418445\nsample estimates:\nmean of x mean of y \n 16.61667  24.55714 \n\n\nNotice, the results are identical. This just demonstrates that the test can be run with data in various formats.\nLet’s visualize the results with a bar graph.\n\nggplot(mtcars, aes(x=vs, y=mpg)) + \n  geom_bar(stat = \"summary\", fun=\"mean\")\n\n\n\n\n\n\n\n\nAdd error bars to the plot! Begin by calculating mean and standard deviation.\n\nmpg_barplot_data &lt;- mtcars %&gt;% group_by(vs) %&gt;% summarize(mpg_M = mean(mpg), mpg_SD = sd(mpg))\nmpg_barplot_data\n\n\nA tibble: 2 × 3\n\n\nvs\nmpg_M\nmpg_SD\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0\n16.61667\n3.860699\n\n\n1\n24.55714\n5.378978\n\n\n\n\n\nCreate the plot with the new dataset.\n\nggplot(mpg_barplot_data, aes(x=vs, y=mpg_M, fill=vs)) + \n  geom_bar(stat=\"identity\", position=\"dodge\") +\n  geom_errorbar(data = mpg_barplot_data, \n      aes(ymin = mpg_M - mpg_SD, ymax = mpg_M + mpg_SD, y = mpg_M),\n        position = position_dodge(width = .9))\n\n\n\n\n\n\n\n\n\n\nChickWeight Data\nWe need another dataset to run a paired samples t-test. So we will load the ChickWeight data set. We will start by examining the data and reshaping it. Then we will run a paired samples t-test.\n\nhead(ChickWeight)\n\n\nA nfnGroupedData: 6 × 4\n\n\n\nweight\nTime\nChick\nDiet\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;ord&gt;\n&lt;fct&gt;\n\n\n\n\n1\n42\n0\n1\n1\n\n\n2\n51\n2\n1\n1\n\n\n3\n59\n4\n1\n1\n\n\n4\n64\n6\n1\n1\n\n\n5\n76\n8\n1\n1\n\n\n6\n93\n10\n1\n1\n\n\n\n\n\nSee the help for definitions of each column.\n\n?ChickWeight\n\n\n\n\nChickWeight {datasets}\nR Documentation\n\n\n\n\n\nWeight versus age of chicks on different diets\n\nDescription\n\nThe ChickWeight data frame has 578 rows and 4 columns from an\nexperiment on the effect of diet on early growth of chicks.\n\n\n\nUsage\n\nChickWeight\n\n\nFormat\n\nAn object of class\nc(\"nfnGroupedData\", \"nfGroupedData\", \"groupedData\", \"data.frame\")\ncontaining the following columns:\n\n\n\nweight\na numeric vector giving the body weight of the chick (gm).\n\n\nTime\na numeric vector giving the number of days since birth when\nthe measurement was made.\n\n\nChick\nan ordered factor with levels\n18 &lt; ... &lt; 48\ngiving a unique identifier for the chick.  The ordering of\nthe levels groups chicks on the same diet together and\norders them according to their final weight (lightest to\nheaviest) within diet.\n\n\nDiet\na factor with levels 1, ..., 4 indicating which\nexperimental diet the chick received.\n\n\n\n\n\n\nDetails\n\nThe body weights of the chicks were measured at birth and every\nsecond day thereafter until day 20.  They were also measured on day\n21.  There were four groups on chicks on different protein diets.\n\nThis dataset was originally part of package nlme, and that has\nmethods (including for [, as.data.frame, plot and\nprint) for its grouped-data classes.\n\n\n\nSource\n\nCrowder, M. and Hand, D. (1990), Analysis of Repeated Measures,\nChapman and Hall (example 5.3)\n\nHand, D. and Crowder, M. (1996), Practical Longitudinal Data\nAnalysis, Chapman and Hall (table A.2)\n\nPinheiro, J. C. and Bates, D. M. (2000) Mixed-effects Models in\nS and S-PLUS, Springer.\n\n\n\nSee Also\n\nSSlogis for models fitted to this dataset.\n\n\n\nExamples\n\nrequire(graphics)\ncoplot(weight ~ Time | Chick, data = ChickWeight,\n       type = \"b\", show.given = FALSE)\n\n\n[Package datasets version 4.5.1 ]\n\n\n\n\nUse the summary function to get an idea of the data in each column.\n\nsummary(ChickWeight)\n\n     weight           Time           Chick     Diet   \n Min.   : 35.0   Min.   : 0.00   13     : 12   1:220  \n 1st Qu.: 63.0   1st Qu.: 4.00   9      : 12   2:120  \n Median :103.0   Median :10.00   20     : 12   3:120  \n Mean   :121.8   Mean   :10.72   10     : 12   4:118  \n 3rd Qu.:163.8   3rd Qu.:16.00   17     : 12          \n Max.   :373.0   Max.   :21.00   19     : 12          \n                                 (Other):506          \n\n\nWe want to reshape the data so we can easily pull each day from its own column.\nWe use the reshape method to turn the data from long to wide format.\n\nChickWeightWide = reshape(ChickWeight, idvar = c(\"Chick\", \"Diet\"), timevar = \"Time\", direction=\"wide\")\nChickWeightWide\n\n\nA data.frame: 50 × 14\n\n\n\nChick\nDiet\nweight.0\nweight.2\nweight.4\nweight.6\nweight.8\nweight.10\nweight.12\nweight.14\nweight.16\nweight.18\nweight.20\nweight.21\n\n\n\n&lt;ord&gt;\n&lt;fct&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1\n1\n42\n51\n59\n64\n76\n93\n106\n125\n149\n171\n199\n205\n\n\n13\n2\n1\n40\n49\n58\n72\n84\n103\n122\n138\n162\n187\n209\n215\n\n\n25\n3\n1\n43\n39\n55\n67\n84\n99\n115\n138\n163\n187\n198\n202\n\n\n37\n4\n1\n42\n49\n56\n67\n74\n87\n102\n108\n136\n154\n160\n157\n\n\n49\n5\n1\n41\n42\n48\n60\n79\n106\n141\n164\n197\n199\n220\n223\n\n\n61\n6\n1\n41\n49\n59\n74\n97\n124\n141\n148\n155\n160\n160\n157\n\n\n73\n7\n1\n41\n49\n57\n71\n89\n112\n146\n174\n218\n250\n288\n305\n\n\n85\n8\n1\n42\n50\n61\n71\n84\n93\n110\n116\n126\n134\n125\nNA\n\n\n96\n9\n1\n42\n51\n59\n68\n85\n96\n90\n92\n93\n100\n100\n98\n\n\n108\n10\n1\n41\n44\n52\n63\n74\n81\n89\n96\n101\n112\n120\n124\n\n\n120\n11\n1\n43\n51\n63\n84\n112\n139\n168\n177\n182\n184\n181\n175\n\n\n132\n12\n1\n41\n49\n56\n62\n72\n88\n119\n135\n162\n185\n195\n205\n\n\n144\n13\n1\n41\n48\n53\n60\n65\n67\n71\n70\n71\n81\n91\n96\n\n\n156\n14\n1\n41\n49\n62\n79\n101\n128\n164\n192\n227\n248\n259\n266\n\n\n168\n15\n1\n41\n49\n56\n64\n68\n68\n67\n68\nNA\nNA\nNA\nNA\n\n\n176\n16\n1\n41\n45\n49\n51\n57\n51\n54\nNA\nNA\nNA\nNA\nNA\n\n\n183\n17\n1\n42\n51\n61\n72\n83\n89\n98\n103\n113\n123\n133\n142\n\n\n195\n18\n1\n39\n35\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n197\n19\n1\n43\n48\n55\n62\n65\n71\n82\n88\n106\n120\n144\n157\n\n\n209\n20\n1\n41\n47\n54\n58\n65\n73\n77\n89\n98\n107\n115\n117\n\n\n221\n21\n2\n40\n50\n62\n86\n125\n163\n217\n240\n275\n307\n318\n331\n\n\n233\n22\n2\n41\n55\n64\n77\n90\n95\n108\n111\n131\n148\n164\n167\n\n\n245\n23\n2\n43\n52\n61\n73\n90\n103\n127\n135\n145\n163\n170\n175\n\n\n257\n24\n2\n42\n52\n58\n74\n66\n68\n70\n71\n72\n72\n76\n74\n\n\n269\n25\n2\n40\n49\n62\n78\n102\n124\n146\n164\n197\n231\n259\n265\n\n\n281\n26\n2\n42\n48\n57\n74\n93\n114\n136\n147\n169\n205\n236\n251\n\n\n293\n27\n2\n39\n46\n58\n73\n87\n100\n115\n123\n144\n163\n185\n192\n\n\n305\n28\n2\n39\n46\n58\n73\n92\n114\n145\n156\n184\n207\n212\n233\n\n\n317\n29\n2\n39\n48\n59\n74\n87\n106\n134\n150\n187\n230\n279\n309\n\n\n329\n30\n2\n42\n48\n59\n72\n85\n98\n115\n122\n143\n151\n157\n150\n\n\n341\n31\n3\n42\n53\n62\n73\n85\n102\n123\n138\n170\n204\n235\n256\n\n\n353\n32\n3\n41\n49\n65\n82\n107\n129\n159\n179\n221\n263\n291\n305\n\n\n365\n33\n3\n39\n50\n63\n77\n96\n111\n137\n144\n151\n146\n156\n147\n\n\n377\n34\n3\n41\n49\n63\n85\n107\n134\n164\n186\n235\n294\n327\n341\n\n\n389\n35\n3\n41\n53\n64\n87\n123\n158\n201\n238\n287\n332\n361\n373\n\n\n401\n36\n3\n39\n48\n61\n76\n98\n116\n145\n166\n198\n227\n225\n220\n\n\n413\n37\n3\n41\n48\n56\n68\n80\n83\n103\n112\n135\n157\n169\n178\n\n\n425\n38\n3\n41\n49\n61\n74\n98\n109\n128\n154\n192\n232\n280\n290\n\n\n437\n39\n3\n42\n50\n61\n78\n89\n109\n130\n146\n170\n214\n250\n272\n\n\n449\n40\n3\n41\n55\n66\n79\n101\n120\n154\n182\n215\n262\n295\n321\n\n\n461\n41\n4\n42\n51\n66\n85\n103\n124\n155\n153\n175\n184\n199\n204\n\n\n473\n42\n4\n42\n49\n63\n84\n103\n126\n160\n174\n204\n234\n269\n281\n\n\n485\n43\n4\n42\n55\n69\n96\n131\n157\n184\n188\n197\n198\n199\n200\n\n\n497\n44\n4\n42\n51\n65\n86\n103\n118\n127\n138\n145\n146\nNA\nNA\n\n\n507\n45\n4\n41\n50\n61\n78\n98\n117\n135\n141\n147\n174\n197\n196\n\n\n519\n46\n4\n40\n52\n62\n82\n101\n120\n144\n156\n173\n210\n231\n238\n\n\n531\n47\n4\n41\n53\n66\n79\n100\n123\n148\n157\n168\n185\n210\n205\n\n\n543\n48\n4\n39\n50\n62\n80\n104\n125\n154\n170\n222\n261\n303\n322\n\n\n555\n49\n4\n40\n53\n64\n85\n108\n128\n152\n166\n184\n203\n233\n237\n\n\n567\n50\n4\n41\n54\n67\n84\n105\n122\n155\n175\n205\n234\n264\n264\n\n\n\n\n\nMissing data will throw off the analysis, so we want to check for missing data.\n\nany(is.na(ChickWeightWide))\nsum(is.na(ChickWeightWide))\n\nTRUE\n\n\n22\n\n\nWe see that there are 22 missing data points.\nWe use the complete.cases function to drop rows with missing data.\n\nChickWeightWide = ChickWeightWide[complete.cases(ChickWeightWide), ]\nChickWeightWide\n\n\nA data.frame: 45 × 14\n\n\n\nChick\nDiet\nweight.0\nweight.2\nweight.4\nweight.6\nweight.8\nweight.10\nweight.12\nweight.14\nweight.16\nweight.18\nweight.20\nweight.21\n\n\n\n&lt;ord&gt;\n&lt;fct&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1\n1\n42\n51\n59\n64\n76\n93\n106\n125\n149\n171\n199\n205\n\n\n13\n2\n1\n40\n49\n58\n72\n84\n103\n122\n138\n162\n187\n209\n215\n\n\n25\n3\n1\n43\n39\n55\n67\n84\n99\n115\n138\n163\n187\n198\n202\n\n\n37\n4\n1\n42\n49\n56\n67\n74\n87\n102\n108\n136\n154\n160\n157\n\n\n49\n5\n1\n41\n42\n48\n60\n79\n106\n141\n164\n197\n199\n220\n223\n\n\n61\n6\n1\n41\n49\n59\n74\n97\n124\n141\n148\n155\n160\n160\n157\n\n\n73\n7\n1\n41\n49\n57\n71\n89\n112\n146\n174\n218\n250\n288\n305\n\n\n96\n9\n1\n42\n51\n59\n68\n85\n96\n90\n92\n93\n100\n100\n98\n\n\n108\n10\n1\n41\n44\n52\n63\n74\n81\n89\n96\n101\n112\n120\n124\n\n\n120\n11\n1\n43\n51\n63\n84\n112\n139\n168\n177\n182\n184\n181\n175\n\n\n132\n12\n1\n41\n49\n56\n62\n72\n88\n119\n135\n162\n185\n195\n205\n\n\n144\n13\n1\n41\n48\n53\n60\n65\n67\n71\n70\n71\n81\n91\n96\n\n\n156\n14\n1\n41\n49\n62\n79\n101\n128\n164\n192\n227\n248\n259\n266\n\n\n183\n17\n1\n42\n51\n61\n72\n83\n89\n98\n103\n113\n123\n133\n142\n\n\n197\n19\n1\n43\n48\n55\n62\n65\n71\n82\n88\n106\n120\n144\n157\n\n\n209\n20\n1\n41\n47\n54\n58\n65\n73\n77\n89\n98\n107\n115\n117\n\n\n221\n21\n2\n40\n50\n62\n86\n125\n163\n217\n240\n275\n307\n318\n331\n\n\n233\n22\n2\n41\n55\n64\n77\n90\n95\n108\n111\n131\n148\n164\n167\n\n\n245\n23\n2\n43\n52\n61\n73\n90\n103\n127\n135\n145\n163\n170\n175\n\n\n257\n24\n2\n42\n52\n58\n74\n66\n68\n70\n71\n72\n72\n76\n74\n\n\n269\n25\n2\n40\n49\n62\n78\n102\n124\n146\n164\n197\n231\n259\n265\n\n\n281\n26\n2\n42\n48\n57\n74\n93\n114\n136\n147\n169\n205\n236\n251\n\n\n293\n27\n2\n39\n46\n58\n73\n87\n100\n115\n123\n144\n163\n185\n192\n\n\n305\n28\n2\n39\n46\n58\n73\n92\n114\n145\n156\n184\n207\n212\n233\n\n\n317\n29\n2\n39\n48\n59\n74\n87\n106\n134\n150\n187\n230\n279\n309\n\n\n329\n30\n2\n42\n48\n59\n72\n85\n98\n115\n122\n143\n151\n157\n150\n\n\n341\n31\n3\n42\n53\n62\n73\n85\n102\n123\n138\n170\n204\n235\n256\n\n\n353\n32\n3\n41\n49\n65\n82\n107\n129\n159\n179\n221\n263\n291\n305\n\n\n365\n33\n3\n39\n50\n63\n77\n96\n111\n137\n144\n151\n146\n156\n147\n\n\n377\n34\n3\n41\n49\n63\n85\n107\n134\n164\n186\n235\n294\n327\n341\n\n\n389\n35\n3\n41\n53\n64\n87\n123\n158\n201\n238\n287\n332\n361\n373\n\n\n401\n36\n3\n39\n48\n61\n76\n98\n116\n145\n166\n198\n227\n225\n220\n\n\n413\n37\n3\n41\n48\n56\n68\n80\n83\n103\n112\n135\n157\n169\n178\n\n\n425\n38\n3\n41\n49\n61\n74\n98\n109\n128\n154\n192\n232\n280\n290\n\n\n437\n39\n3\n42\n50\n61\n78\n89\n109\n130\n146\n170\n214\n250\n272\n\n\n449\n40\n3\n41\n55\n66\n79\n101\n120\n154\n182\n215\n262\n295\n321\n\n\n461\n41\n4\n42\n51\n66\n85\n103\n124\n155\n153\n175\n184\n199\n204\n\n\n473\n42\n4\n42\n49\n63\n84\n103\n126\n160\n174\n204\n234\n269\n281\n\n\n485\n43\n4\n42\n55\n69\n96\n131\n157\n184\n188\n197\n198\n199\n200\n\n\n507\n45\n4\n41\n50\n61\n78\n98\n117\n135\n141\n147\n174\n197\n196\n\n\n519\n46\n4\n40\n52\n62\n82\n101\n120\n144\n156\n173\n210\n231\n238\n\n\n531\n47\n4\n41\n53\n66\n79\n100\n123\n148\n157\n168\n185\n210\n205\n\n\n543\n48\n4\n39\n50\n62\n80\n104\n125\n154\n170\n222\n261\n303\n322\n\n\n555\n49\n4\n40\n53\n64\n85\n108\n128\n152\n166\n184\n203\n233\n237\n\n\n567\n50\n4\n41\n54\n67\n84\n105\n122\n155\n175\n205\n234\n264\n264\n\n\n\n\n\nNote, we lost 5 rows of data due to missing data.\n\n\nPaired Samples t-test\nWe can now compare the first and second time points.\n\nt.test(ChickWeightWide$weight.0, ChickWeightWide$weight.2, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  ChickWeightWide$weight.0 and ChickWeightWide$weight.2\nt = -17.409, df = 44, p-value &lt; 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -9.496402 -7.525820\nsample estimates:\nmean difference \n      -8.511111 \n\n\nWe can plot the average weight at each point and see that, as we might expect, weight goes up at each subsequent timepoint.\n\nggplot(ChickWeight, aes(x=Time, y=weight, fill=Time)) + \n  geom_bar(stat = \"summary\", fun=\"mean\")\n\n\n\n\n\n\n\n\nThis concludes the introduction to analysis in R!"
  },
  {
    "objectID": "getting-started/r/import-export.html",
    "href": "getting-started/r/import-export.html",
    "title": "Importing Data",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.\nYou can embed an R code chunk like below.\nTry executing this chunk by clicking the Run button within the chunk or by placing your cursor inside it and pressing Cmd+Shift+Enter.\n\nlibrary(conflicted)\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"lag\", \"dplyr\")\n\n\n[conflicted] Will prefer dplyr::filter over any other package.\n\n[conflicted] Will prefer dplyr::lag over any other package.\n\n\n\n\n\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n\n✔ purrr     1.1.0"
  },
  {
    "objectID": "getting-started/r/import-export.html#r-markdown",
    "href": "getting-started/r/import-export.html#r-markdown",
    "title": "Importing Data",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.\nYou can embed an R code chunk like below.\nTry executing this chunk by clicking the Run button within the chunk or by placing your cursor inside it and pressing Cmd+Shift+Enter.\n\nlibrary(conflicted)\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"lag\", \"dplyr\")\n\n\n[conflicted] Will prefer dplyr::filter over any other package.\n\n[conflicted] Will prefer dplyr::lag over any other package.\n\n\n\n\n\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n\n✔ purrr     1.1.0"
  },
  {
    "objectID": "getting-started/r/import-export.html#saving-data-to-local-file",
    "href": "getting-started/r/import-export.html#saving-data-to-local-file",
    "title": "Importing Data",
    "section": "Saving data to local file",
    "text": "Saving data to local file\nTo start with, we will load the built-in iris data set.\nWe will then manipulate the data, adding a column, and save the data.\nData will be exported in multiple formats to show R capabilities.\nA data folder has been created for you to save the files in to help keep the repo organized.\n\niris\n\n\nA data.frame: 150 × 5\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.1\nsetosa\n\n\n5.4\n3.7\n1.5\n0.2\nsetosa\n\n\n4.8\n3.4\n1.6\n0.2\nsetosa\n\n\n4.8\n3.0\n1.4\n0.1\nsetosa\n\n\n4.3\n3.0\n1.1\n0.1\nsetosa\n\n\n5.8\n4.0\n1.2\n0.2\nsetosa\n\n\n5.7\n4.4\n1.5\n0.4\nsetosa\n\n\n5.4\n3.9\n1.3\n0.4\nsetosa\n\n\n5.1\n3.5\n1.4\n0.3\nsetosa\n\n\n5.7\n3.8\n1.7\n0.3\nsetosa\n\n\n5.1\n3.8\n1.5\n0.3\nsetosa\n\n\n5.4\n3.4\n1.7\n0.2\nsetosa\n\n\n5.1\n3.7\n1.5\n0.4\nsetosa\n\n\n4.6\n3.6\n1.0\n0.2\nsetosa\n\n\n5.1\n3.3\n1.7\n0.5\nsetosa\n\n\n4.8\n3.4\n1.9\n0.2\nsetosa\n\n\n5.0\n3.0\n1.6\n0.2\nsetosa\n\n\n5.0\n3.4\n1.6\n0.4\nsetosa\n\n\n5.2\n3.5\n1.5\n0.2\nsetosa\n\n\n5.2\n3.4\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.6\n0.2\nsetosa\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n6.9\n3.2\n5.7\n2.3\nvirginica\n\n\n5.6\n2.8\n4.9\n2.0\nvirginica\n\n\n7.7\n2.8\n6.7\n2.0\nvirginica\n\n\n6.3\n2.7\n4.9\n1.8\nvirginica\n\n\n6.7\n3.3\n5.7\n2.1\nvirginica\n\n\n7.2\n3.2\n6.0\n1.8\nvirginica\n\n\n6.2\n2.8\n4.8\n1.8\nvirginica\n\n\n6.1\n3.0\n4.9\n1.8\nvirginica\n\n\n6.4\n2.8\n5.6\n2.1\nvirginica\n\n\n7.2\n3.0\n5.8\n1.6\nvirginica\n\n\n7.4\n2.8\n6.1\n1.9\nvirginica\n\n\n7.9\n3.8\n6.4\n2.0\nvirginica\n\n\n6.4\n2.8\n5.6\n2.2\nvirginica\n\n\n6.3\n2.8\n5.1\n1.5\nvirginica\n\n\n6.1\n2.6\n5.6\n1.4\nvirginica\n\n\n7.7\n3.0\n6.1\n2.3\nvirginica\n\n\n6.3\n3.4\n5.6\n2.4\nvirginica\n\n\n6.4\n3.1\n5.5\n1.8\nvirginica\n\n\n6.0\n3.0\n4.8\n1.8\nvirginica\n\n\n6.9\n3.1\n5.4\n2.1\nvirginica\n\n\n6.7\n3.1\n5.6\n2.4\nvirginica\n\n\n6.9\n3.1\n5.1\n2.3\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n6.8\n3.2\n5.9\n2.3\nvirginica\n\n\n6.7\n3.3\n5.7\n2.5\nvirginica\n\n\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n\nNow let’s add a column to the data.\n\niris_mutate &lt;- mutate(iris, Sepal.Ratio=Sepal.Width/Sepal.Length) \niris_mutate\n\n\nA data.frame: 150 × 6\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\nSepal.Ratio\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n&lt;dbl&gt;\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n0.6862745\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n0.6122449\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n0.6808511\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n0.6739130\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n0.7200000\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n0.7222222\n\n\n4.6\n3.4\n1.4\n0.3\nsetosa\n0.7391304\n\n\n5.0\n3.4\n1.5\n0.2\nsetosa\n0.6800000\n\n\n4.4\n2.9\n1.4\n0.2\nsetosa\n0.6590909\n\n\n4.9\n3.1\n1.5\n0.1\nsetosa\n0.6326531\n\n\n5.4\n3.7\n1.5\n0.2\nsetosa\n0.6851852\n\n\n4.8\n3.4\n1.6\n0.2\nsetosa\n0.7083333\n\n\n4.8\n3.0\n1.4\n0.1\nsetosa\n0.6250000\n\n\n4.3\n3.0\n1.1\n0.1\nsetosa\n0.6976744\n\n\n5.8\n4.0\n1.2\n0.2\nsetosa\n0.6896552\n\n\n5.7\n4.4\n1.5\n0.4\nsetosa\n0.7719298\n\n\n5.4\n3.9\n1.3\n0.4\nsetosa\n0.7222222\n\n\n5.1\n3.5\n1.4\n0.3\nsetosa\n0.6862745\n\n\n5.7\n3.8\n1.7\n0.3\nsetosa\n0.6666667\n\n\n5.1\n3.8\n1.5\n0.3\nsetosa\n0.7450980\n\n\n5.4\n3.4\n1.7\n0.2\nsetosa\n0.6296296\n\n\n5.1\n3.7\n1.5\n0.4\nsetosa\n0.7254902\n\n\n4.6\n3.6\n1.0\n0.2\nsetosa\n0.7826087\n\n\n5.1\n3.3\n1.7\n0.5\nsetosa\n0.6470588\n\n\n4.8\n3.4\n1.9\n0.2\nsetosa\n0.7083333\n\n\n5.0\n3.0\n1.6\n0.2\nsetosa\n0.6000000\n\n\n5.0\n3.4\n1.6\n0.4\nsetosa\n0.6800000\n\n\n5.2\n3.5\n1.5\n0.2\nsetosa\n0.6730769\n\n\n5.2\n3.4\n1.4\n0.2\nsetosa\n0.6538462\n\n\n4.7\n3.2\n1.6\n0.2\nsetosa\n0.6808511\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n6.9\n3.2\n5.7\n2.3\nvirginica\n0.4637681\n\n\n5.6\n2.8\n4.9\n2.0\nvirginica\n0.5000000\n\n\n7.7\n2.8\n6.7\n2.0\nvirginica\n0.3636364\n\n\n6.3\n2.7\n4.9\n1.8\nvirginica\n0.4285714\n\n\n6.7\n3.3\n5.7\n2.1\nvirginica\n0.4925373\n\n\n7.2\n3.2\n6.0\n1.8\nvirginica\n0.4444444\n\n\n6.2\n2.8\n4.8\n1.8\nvirginica\n0.4516129\n\n\n6.1\n3.0\n4.9\n1.8\nvirginica\n0.4918033\n\n\n6.4\n2.8\n5.6\n2.1\nvirginica\n0.4375000\n\n\n7.2\n3.0\n5.8\n1.6\nvirginica\n0.4166667\n\n\n7.4\n2.8\n6.1\n1.9\nvirginica\n0.3783784\n\n\n7.9\n3.8\n6.4\n2.0\nvirginica\n0.4810127\n\n\n6.4\n2.8\n5.6\n2.2\nvirginica\n0.4375000\n\n\n6.3\n2.8\n5.1\n1.5\nvirginica\n0.4444444\n\n\n6.1\n2.6\n5.6\n1.4\nvirginica\n0.4262295\n\n\n7.7\n3.0\n6.1\n2.3\nvirginica\n0.3896104\n\n\n6.3\n3.4\n5.6\n2.4\nvirginica\n0.5396825\n\n\n6.4\n3.1\n5.5\n1.8\nvirginica\n0.4843750\n\n\n6.0\n3.0\n4.8\n1.8\nvirginica\n0.5000000\n\n\n6.9\n3.1\n5.4\n2.1\nvirginica\n0.4492754\n\n\n6.7\n3.1\n5.6\n2.4\nvirginica\n0.4626866\n\n\n6.9\n3.1\n5.1\n2.3\nvirginica\n0.4492754\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n0.4655172\n\n\n6.8\n3.2\n5.9\n2.3\nvirginica\n0.4705882\n\n\n6.7\n3.3\n5.7\n2.5\nvirginica\n0.4925373\n\n\n6.7\n3.0\n5.2\n2.3\nvirginica\n0.4477612\n\n\n6.3\n2.5\n5.0\n1.9\nvirginica\n0.3968254\n\n\n6.5\n3.0\n5.2\n2.0\nvirginica\n0.4615385\n\n\n6.2\n3.4\n5.4\n2.3\nvirginica\n0.5483871\n\n\n5.9\n3.0\n5.1\n1.8\nvirginica\n0.5084746\n\n\n\n\n\nBefore writing out the table, let’s figure out where we are in the repo.\n\ngetwd()\n\n'/Users/dsl/Desktop/notebooks'\n\n\nThe output shows we are in the notebooks folder. So we will have to go up a directory and then down into the data folder to save to the correct location.\nOnce you have calculated new information in our table you may want to write the table to a file. You can use the write.table function for this.\nThe file will show up in the files tab to the right with the name ../data/pbc_mutate.txt. The .. means to go up a directory.\n\n# Check if a directory exists before creating it\nif (!dir.exists(\"data\")) {\n  dir.create(\"data\")\n}\n\n\nwrite.table(iris_mutate,\"data/iris_mutate.txt\",row.names=F,sep=\"\\t\")\n\nWe can also save the data into a csv format. The syntax is very similar.\n\nwrite.csv(iris_mutate,\"data/iris_mutate.csv\",row.names=F)"
  },
  {
    "objectID": "getting-started/r/import-export.html#read-data-in-from-local-file",
    "href": "getting-started/r/import-export.html#read-data-in-from-local-file",
    "title": "Importing Data",
    "section": "Read data in from local file",
    "text": "Read data in from local file\nWe will now read the data we saved back into R.\nLet’s start with the text table saved first.\n\niris_mutate_txt = read.table(\"data/iris_mutate.txt\", header = TRUE, sep = \"\\t\")\niris_mutate_txt\n\n\nA data.frame: 150 × 6\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\nSepal.Ratio\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n0.6862745\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n0.6122449\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n0.6808511\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n0.6739130\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n0.7200000\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n0.7222222\n\n\n4.6\n3.4\n1.4\n0.3\nsetosa\n0.7391304\n\n\n5.0\n3.4\n1.5\n0.2\nsetosa\n0.6800000\n\n\n4.4\n2.9\n1.4\n0.2\nsetosa\n0.6590909\n\n\n4.9\n3.1\n1.5\n0.1\nsetosa\n0.6326531\n\n\n5.4\n3.7\n1.5\n0.2\nsetosa\n0.6851852\n\n\n4.8\n3.4\n1.6\n0.2\nsetosa\n0.7083333\n\n\n4.8\n3.0\n1.4\n0.1\nsetosa\n0.6250000\n\n\n4.3\n3.0\n1.1\n0.1\nsetosa\n0.6976744\n\n\n5.8\n4.0\n1.2\n0.2\nsetosa\n0.6896552\n\n\n5.7\n4.4\n1.5\n0.4\nsetosa\n0.7719298\n\n\n5.4\n3.9\n1.3\n0.4\nsetosa\n0.7222222\n\n\n5.1\n3.5\n1.4\n0.3\nsetosa\n0.6862745\n\n\n5.7\n3.8\n1.7\n0.3\nsetosa\n0.6666667\n\n\n5.1\n3.8\n1.5\n0.3\nsetosa\n0.7450980\n\n\n5.4\n3.4\n1.7\n0.2\nsetosa\n0.6296296\n\n\n5.1\n3.7\n1.5\n0.4\nsetosa\n0.7254902\n\n\n4.6\n3.6\n1.0\n0.2\nsetosa\n0.7826087\n\n\n5.1\n3.3\n1.7\n0.5\nsetosa\n0.6470588\n\n\n4.8\n3.4\n1.9\n0.2\nsetosa\n0.7083333\n\n\n5.0\n3.0\n1.6\n0.2\nsetosa\n0.6000000\n\n\n5.0\n3.4\n1.6\n0.4\nsetosa\n0.6800000\n\n\n5.2\n3.5\n1.5\n0.2\nsetosa\n0.6730769\n\n\n5.2\n3.4\n1.4\n0.2\nsetosa\n0.6538462\n\n\n4.7\n3.2\n1.6\n0.2\nsetosa\n0.6808511\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n6.9\n3.2\n5.7\n2.3\nvirginica\n0.4637681\n\n\n5.6\n2.8\n4.9\n2.0\nvirginica\n0.5000000\n\n\n7.7\n2.8\n6.7\n2.0\nvirginica\n0.3636364\n\n\n6.3\n2.7\n4.9\n1.8\nvirginica\n0.4285714\n\n\n6.7\n3.3\n5.7\n2.1\nvirginica\n0.4925373\n\n\n7.2\n3.2\n6.0\n1.8\nvirginica\n0.4444444\n\n\n6.2\n2.8\n4.8\n1.8\nvirginica\n0.4516129\n\n\n6.1\n3.0\n4.9\n1.8\nvirginica\n0.4918033\n\n\n6.4\n2.8\n5.6\n2.1\nvirginica\n0.4375000\n\n\n7.2\n3.0\n5.8\n1.6\nvirginica\n0.4166667\n\n\n7.4\n2.8\n6.1\n1.9\nvirginica\n0.3783784\n\n\n7.9\n3.8\n6.4\n2.0\nvirginica\n0.4810127\n\n\n6.4\n2.8\n5.6\n2.2\nvirginica\n0.4375000\n\n\n6.3\n2.8\n5.1\n1.5\nvirginica\n0.4444444\n\n\n6.1\n2.6\n5.6\n1.4\nvirginica\n0.4262295\n\n\n7.7\n3.0\n6.1\n2.3\nvirginica\n0.3896104\n\n\n6.3\n3.4\n5.6\n2.4\nvirginica\n0.5396825\n\n\n6.4\n3.1\n5.5\n1.8\nvirginica\n0.4843750\n\n\n6.0\n3.0\n4.8\n1.8\nvirginica\n0.5000000\n\n\n6.9\n3.1\n5.4\n2.1\nvirginica\n0.4492754\n\n\n6.7\n3.1\n5.6\n2.4\nvirginica\n0.4626866\n\n\n6.9\n3.1\n5.1\n2.3\nvirginica\n0.4492754\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n0.4655172\n\n\n6.8\n3.2\n5.9\n2.3\nvirginica\n0.4705882\n\n\n6.7\n3.3\n5.7\n2.5\nvirginica\n0.4925373\n\n\n6.7\n3.0\n5.2\n2.3\nvirginica\n0.4477612\n\n\n6.3\n2.5\n5.0\n1.9\nvirginica\n0.3968254\n\n\n6.5\n3.0\n5.2\n2.0\nvirginica\n0.4615385\n\n\n6.2\n3.4\n5.4\n2.3\nvirginica\n0.5483871\n\n\n5.9\n3.0\n5.1\n1.8\nvirginica\n0.5084746\n\n\n\n\n\nNext, let’s load the csv data back into r with a new variable name.\n\niris_mutate_csv = read.csv(\"data/iris_mutate.csv\", header = TRUE)\niris_mutate_csv\n\n\nA data.frame: 150 × 6\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\nSepal.Ratio\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n0.6862745\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n0.6122449\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n0.6808511\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n0.6739130\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n0.7200000\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n0.7222222\n\n\n4.6\n3.4\n1.4\n0.3\nsetosa\n0.7391304\n\n\n5.0\n3.4\n1.5\n0.2\nsetosa\n0.6800000\n\n\n4.4\n2.9\n1.4\n0.2\nsetosa\n0.6590909\n\n\n4.9\n3.1\n1.5\n0.1\nsetosa\n0.6326531\n\n\n5.4\n3.7\n1.5\n0.2\nsetosa\n0.6851852\n\n\n4.8\n3.4\n1.6\n0.2\nsetosa\n0.7083333\n\n\n4.8\n3.0\n1.4\n0.1\nsetosa\n0.6250000\n\n\n4.3\n3.0\n1.1\n0.1\nsetosa\n0.6976744\n\n\n5.8\n4.0\n1.2\n0.2\nsetosa\n0.6896552\n\n\n5.7\n4.4\n1.5\n0.4\nsetosa\n0.7719298\n\n\n5.4\n3.9\n1.3\n0.4\nsetosa\n0.7222222\n\n\n5.1\n3.5\n1.4\n0.3\nsetosa\n0.6862745\n\n\n5.7\n3.8\n1.7\n0.3\nsetosa\n0.6666667\n\n\n5.1\n3.8\n1.5\n0.3\nsetosa\n0.7450980\n\n\n5.4\n3.4\n1.7\n0.2\nsetosa\n0.6296296\n\n\n5.1\n3.7\n1.5\n0.4\nsetosa\n0.7254902\n\n\n4.6\n3.6\n1.0\n0.2\nsetosa\n0.7826087\n\n\n5.1\n3.3\n1.7\n0.5\nsetosa\n0.6470588\n\n\n4.8\n3.4\n1.9\n0.2\nsetosa\n0.7083333\n\n\n5.0\n3.0\n1.6\n0.2\nsetosa\n0.6000000\n\n\n5.0\n3.4\n1.6\n0.4\nsetosa\n0.6800000\n\n\n5.2\n3.5\n1.5\n0.2\nsetosa\n0.6730769\n\n\n5.2\n3.4\n1.4\n0.2\nsetosa\n0.6538462\n\n\n4.7\n3.2\n1.6\n0.2\nsetosa\n0.6808511\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n6.9\n3.2\n5.7\n2.3\nvirginica\n0.4637681\n\n\n5.6\n2.8\n4.9\n2.0\nvirginica\n0.5000000\n\n\n7.7\n2.8\n6.7\n2.0\nvirginica\n0.3636364\n\n\n6.3\n2.7\n4.9\n1.8\nvirginica\n0.4285714\n\n\n6.7\n3.3\n5.7\n2.1\nvirginica\n0.4925373\n\n\n7.2\n3.2\n6.0\n1.8\nvirginica\n0.4444444\n\n\n6.2\n2.8\n4.8\n1.8\nvirginica\n0.4516129\n\n\n6.1\n3.0\n4.9\n1.8\nvirginica\n0.4918033\n\n\n6.4\n2.8\n5.6\n2.1\nvirginica\n0.4375000\n\n\n7.2\n3.0\n5.8\n1.6\nvirginica\n0.4166667\n\n\n7.4\n2.8\n6.1\n1.9\nvirginica\n0.3783784\n\n\n7.9\n3.8\n6.4\n2.0\nvirginica\n0.4810127\n\n\n6.4\n2.8\n5.6\n2.2\nvirginica\n0.4375000\n\n\n6.3\n2.8\n5.1\n1.5\nvirginica\n0.4444444\n\n\n6.1\n2.6\n5.6\n1.4\nvirginica\n0.4262295\n\n\n7.7\n3.0\n6.1\n2.3\nvirginica\n0.3896104\n\n\n6.3\n3.4\n5.6\n2.4\nvirginica\n0.5396825\n\n\n6.4\n3.1\n5.5\n1.8\nvirginica\n0.4843750\n\n\n6.0\n3.0\n4.8\n1.8\nvirginica\n0.5000000\n\n\n6.9\n3.1\n5.4\n2.1\nvirginica\n0.4492754\n\n\n6.7\n3.1\n5.6\n2.4\nvirginica\n0.4626866\n\n\n6.9\n3.1\n5.1\n2.3\nvirginica\n0.4492754\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n0.4655172\n\n\n6.8\n3.2\n5.9\n2.3\nvirginica\n0.4705882\n\n\n6.7\n3.3\n5.7\n2.5\nvirginica\n0.4925373\n\n\n6.7\n3.0\n5.2\n2.3\nvirginica\n0.4477612\n\n\n6.3\n2.5\n5.0\n1.9\nvirginica\n0.3968254\n\n\n6.5\n3.0\n5.2\n2.0\nvirginica\n0.4615385\n\n\n6.2\n3.4\n5.4\n2.3\nvirginica\n0.5483871\n\n\n5.9\n3.0\n5.1\n1.8\nvirginica\n0.5084746\n\n\n\n\n\nNow that we have two data frames loaded, let’s check if they are the same.\n\nidentical(iris_mutate_txt, iris_mutate_csv)\n\nTRUE\n\n\nThis shows that even though we saved the data in two formats, when we read the data back into R, the data frames are identical.\nSo how you save your data is up to your preference, as long as you specify the correct parameters when saving and importing the data.\nThis concludes the data import/export module."
  },
  {
    "objectID": "getting-started/r/data-viz.html",
    "href": "getting-started/r/data-viz.html",
    "title": "Data Visualization",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nlibrary(conflicted)\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"lag\", \"dplyr\")\n\n\n[conflicted] Will prefer dplyr::filter over any other package.\n\n[conflicted] Will prefer dplyr::lag over any other package.\n\n\n\n\n\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n\n✔ purrr     1.1.0     \n\n\n\n\n\n\n\ngetwd()\n\n'/Users/dsl/Desktop/notebooks'"
  },
  {
    "objectID": "getting-started/r/data-viz.html#r-markdown",
    "href": "getting-started/r/data-viz.html#r-markdown",
    "title": "Data Visualization",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nlibrary(conflicted)\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"lag\", \"dplyr\")\n\n\n[conflicted] Will prefer dplyr::filter over any other package.\n\n[conflicted] Will prefer dplyr::lag over any other package.\n\n\n\n\n\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n\n✔ purrr     1.1.0     \n\n\n\n\n\n\n\ngetwd()\n\n'/Users/dsl/Desktop/notebooks'"
  },
  {
    "objectID": "getting-started/r/data-viz.html#create-a-directory-for-output-of-images",
    "href": "getting-started/r/data-viz.html#create-a-directory-for-output-of-images",
    "title": "Data Visualization",
    "section": "Create a directory for output of images",
    "text": "Create a directory for output of images\n\n# Check if a directory exists before creating it\nimg_path = \"images\"\nif (!dir.exists(img_path)) {\n  dir.create(img_path)\n}"
  },
  {
    "objectID": "getting-started/shoulders-of-giants.html",
    "href": "getting-started/shoulders-of-giants.html",
    "title": "On the Shoulders of Giants",
    "section": "",
    "text": "Learning to program (or any new skill) is guaranteed to come with roadblocks. Code won’t run, an error message pops up, or things just don’t make sense. This isn’t a failure—it’s normal! The key difference between frustrated beginners and successful learners isn’t intelligence, it’s approach.\nIn this section, you’ll learn practical tools and mindsets for getting unstuck: reading documentation, focused searches, even reading the actual source code.\n\nIt’s about solving, not knowing\nIt’s about solutions, not syntax\nIt’s about process, not memorization",
    "crumbs": [
      "Pixel Process",
      "Basics",
      "On the Shoulders of Giants"
    ]
  },
  {
    "objectID": "getting-started/shoulders-of-giants.html#troubleshooting-required",
    "href": "getting-started/shoulders-of-giants.html#troubleshooting-required",
    "title": "On the Shoulders of Giants",
    "section": "",
    "text": "Learning to program (or any new skill) is guaranteed to come with roadblocks. Code won’t run, an error message pops up, or things just don’t make sense. This isn’t a failure—it’s normal! The key difference between frustrated beginners and successful learners isn’t intelligence, it’s approach.\nIn this section, you’ll learn practical tools and mindsets for getting unstuck: reading documentation, focused searches, even reading the actual source code.\n\nIt’s about solving, not knowing\nIt’s about solutions, not syntax\nIt’s about process, not memorization",
    "crumbs": [
      "Pixel Process",
      "Basics",
      "On the Shoulders of Giants"
    ]
  },
  {
    "objectID": "getting-started/shoulders-of-giants.html#troubleshooting-checklist",
    "href": "getting-started/shoulders-of-giants.html#troubleshooting-checklist",
    "title": "On the Shoulders of Giants",
    "section": "Troubleshooting Checklist",
    "text": "Troubleshooting Checklist\n\nPause & Get Oriented\n\nDon’t rush: Take a moment to read the message, consider what is happening\nFocus on what changed: Did you add something new, update a package, change an environment, or move files?\n\n\n\nRead the Error Message\n\nLook for keywords: ImportError, SyntaxError, FileNotFoundError\nLook for YOUR keywords: do you see variables you defined or created? Unique things in your code\nCheck the traceback: Try to identify where the problem started the last lines are often the most important\n\n\n\nBegin Basic\n\nTypos and missing punctuation: anything in the traceback or output look even a little off?\nFile paths: Are inputs and outputs showing correct paths? Did anything get moved?\nCopy-paste errors: Did you accidentally copy a character or break a line?\nRestart and rerun: Sometimes, especially in notebooks, things get out of sync-so turn if off and on again\n\n\n\nIsolate\n\nIs it reproducible?Does the error happen every time? On other machines or environments?\n“On and off again works: Try restarting your program, kernel, or computer-working broader\nSimplify: Can you trigger the error with a tiny bit of code, or does it only happen with everything together? Test on another variable or dataframe\nComment out code: Temporarily remove sections or run cell-by-cell to see where the error appears.\n\n\n\nRead the Docs\n\nBuilt-in help tools: Python: help(); ?, or double-tab in notebooks; R: ?function_name, help(function_name); Shell: man, --help\nLook up function/class documentation: Official docs, docstrings, and usage examples\n\n\n\nSearch the Web\n\nCopy-paste the error message: Remove file-specific info if needed\nAdd keywords: “python”, “R”, or the library you’re using\nSearch for common forums: Stack Overflow, GitHub Issues, official forums\n\n\n\nCheck Your Environment and Versions\n\nAre you using the right Python/R version?: Check notebook kernels and paths\nWhich environment is active? (Conda, virtualenv, Renv, etc.)**: Make sure your packages are installed with proper versions\n\n\n\nScope the Issue\n\nIs it a ‘just here’ problem: Typos, filepaths, incorrect variable types-only happen in one place\nIs it a project or environment problem: Does other project code now have issues? Is it my system? Known Issue: Is it a commonly reported bug or issue on forums?\n\n\n\n\n\n\n\n\nExperience from Errors\n\n\n\nErrors are part of the process. Solid troubleshooting and debugging approaches are part of the job. Know your resources and use them.\n\n\n\nWhen in doubt, read the docs\n\nCoding requires testing and debugging. Use these guidelines to avoid getting stuck. Chances are, you are not the only person to encounter issues. Learn on the many established and helpful resources of the coding community.",
    "crumbs": [
      "Pixel Process",
      "Basics",
      "On the Shoulders of Giants"
    ]
  },
  {
    "objectID": "workflow/index.html",
    "href": "workflow/index.html",
    "title": "Workflow",
    "section": "",
    "text": "Great workflows don’t happen by accident — they’re built through intention, iteration, and small choices that add up. This section is about taking control of your coding experience: streamlining your setup, automating what slows you down, and shaping tools to fit the way you work. From the command line to color schemes, you’ll explore ways to reduce friction, stay organized, and build a development environment that’s not just efficient, but genuinely satisfying to use.\nCode happy!",
    "crumbs": [
      "Pixel Process",
      "Workflow"
    ]
  },
  {
    "objectID": "workflow/index.html#workflow-topics",
    "href": "workflow/index.html#workflow-topics",
    "title": "Workflow",
    "section": "Workflow Topics",
    "text": "Workflow Topics\n\nEasy Automation: Streamline your environment with aliases, reusable project templates, and shortcuts that save time.\nSoftware & Tools: Choose tools and setups that support your flow — from editors and terminals to colors and ergonomics.\nVersioning & Environments: Track your work and manage environments to stay organized, reproducible, and collaborative.\n\n\n\n\nTools\n\n\n\n Tools Overview → Editors, linters, and formatters, oh my!\n\n\n Scripts and Notebooks → When and why to use notebooks vs scripts\n\n\n\n\n\nCustomization\n\n\n\n Command Line Interface → Don’t just learn CLI, optimize it\n\n\n\n\n\n\n\n\n\n\n\nBuilt by a Lazy Perfectionist\n\n\n\nI’ll spend four times longer automating a task than doing it manually — not just because I’m lazy, but because I’m interested. Writing a script is fun. Repeating the same task isn’t. If there’s room for human error, or a chance I’ll need it again later, I’d rather build it right the first time — even if that means diving into the rabbit hole. Hopefully, all the time I’ve spent trying to be lazy makes things a little easier for you, too.\nSave Time\nAvoid Errors\nCreate Flow\n\n\n\nCreate your own paradise.\n\nWorkflow isn’t about perfection — it’s about making your day-to-day smoother, more consistent, and less frustrating. A few smart decisions now can prevent hours of future pain. Standardize your structure. Automate the annoying stuff. Document enough to help your future self (and your teammates). Whether you’re a team of one or part of a larger group, good workflow habits make everything else easier.",
    "crumbs": [
      "Pixel Process",
      "Workflow"
    ]
  },
  {
    "objectID": "workflow/file-basics.html",
    "href": "workflow/file-basics.html",
    "title": "Working with Files and Directories",
    "section": "",
    "text": "The command line gives you precise control over your file system. Here you’ll learn how to move around directories, manage files, and check basic permissions."
  },
  {
    "objectID": "workflow/file-basics.html#introduction",
    "href": "workflow/file-basics.html#introduction",
    "title": "Working with Files and Directories",
    "section": "",
    "text": "The command line gives you precise control over your file system. Here you’ll learn how to move around directories, manage files, and check basic permissions."
  },
  {
    "objectID": "workflow/file-basics.html#navigation-commands",
    "href": "workflow/file-basics.html#navigation-commands",
    "title": "Working with Files and Directories",
    "section": "Navigation Commands",
    "text": "Navigation Commands"
  },
  {
    "objectID": "workflow/file-basics.html#managing-files-and-directories",
    "href": "workflow/file-basics.html#managing-files-and-directories",
    "title": "Working with Files and Directories",
    "section": "Managing Files and Directories",
    "text": "Managing Files and Directories"
  },
  {
    "objectID": "workflow/file-basics.html#checking-permissions",
    "href": "workflow/file-basics.html#checking-permissions",
    "title": "Working with Files and Directories",
    "section": "Checking Permissions",
    "text": "Checking Permissions"
  },
  {
    "objectID": "workflow/file-basics.html#manual-and-help",
    "href": "workflow/file-basics.html#manual-and-help",
    "title": "Working with Files and Directories",
    "section": "Manual and Help",
    "text": "Manual and Help"
  },
  {
    "objectID": "workflow/tools-overview.html",
    "href": "workflow/tools-overview.html",
    "title": "Tools Overview",
    "section": "",
    "text": "Visual Studio Code (VS Code) - A lightweight, highly customizable code editor with powerful extensions for Python development.\nPyCharm - An advanced IDE with comprehensive features for professional Python development, available in both free and paid versions.\nJupyter - Variety of tools including interactive web-based environments ideal for data science, prototyping, and collaboration.\nSublime Text - A fast and versatile text editor with a wide range of plugins for Python programming.",
    "crumbs": [
      "Pixel Process",
      "Tools",
      "Tools Overview"
    ]
  },
  {
    "objectID": "workflow/tools-overview.html#code-editors-and-integrated-development-environments-ides",
    "href": "workflow/tools-overview.html#code-editors-and-integrated-development-environments-ides",
    "title": "Tools Overview",
    "section": "",
    "text": "Visual Studio Code (VS Code) - A lightweight, highly customizable code editor with powerful extensions for Python development.\nPyCharm - An advanced IDE with comprehensive features for professional Python development, available in both free and paid versions.\nJupyter - Variety of tools including interactive web-based environments ideal for data science, prototyping, and collaboration.\nSublime Text - A fast and versatile text editor with a wide range of plugins for Python programming.",
    "crumbs": [
      "Pixel Process",
      "Tools",
      "Tools Overview"
    ]
  },
  {
    "objectID": "workflow/tools-overview.html#linters-and-code-formatters",
    "href": "workflow/tools-overview.html#linters-and-code-formatters",
    "title": "Tools Overview",
    "section": "Linters and Code Formatters",
    "text": "Linters and Code Formatters\n\nPylint - Analyzes Python code to identify errors, enforce coding standards, and suggest improvements.\nFlake8 - A tool that combines several Python tools to check the style and quality of your code.\nBlack - An uncompromising code formatter that automatically formats your code to adhere to PEP 8 standards.",
    "crumbs": [
      "Pixel Process",
      "Tools",
      "Tools Overview"
    ]
  },
  {
    "objectID": "workflow/tools-overview.html#debugging-tools",
    "href": "workflow/tools-overview.html#debugging-tools",
    "title": "Tools Overview",
    "section": "Debugging Tools",
    "text": "Debugging Tools\n\npdb - The built-in Python debugger for stepping through code, inspecting variables, and finding bugs.\nipdb - An enhanced version of pdb with additional features provided by IPython.\nEditor & IDE Extensions - Most modern programming platforms have debugging options adn extensions available.",
    "crumbs": [
      "Pixel Process",
      "Tools",
      "Tools Overview"
    ]
  },
  {
    "objectID": "workflow/tools-overview.html#version-control",
    "href": "workflow/tools-overview.html#version-control",
    "title": "Tools Overview",
    "section": "Version Control",
    "text": "Version Control\n\nGit - A distributed version control system that tracks changes in your code and allows you to manage versions, branches, and collaboration locally and remotely.\nGitHub - A cloud-based platform for hosting Git repositories, enabling collaboration, code review, and project management features.\nGitHub Desktop -A user-friendly GUI for Git and GitHub that simplifies committing, branching, and syncing changes without using the command line.\n\nNote: Alternative options exist for this, but this is standard. Stick with it unless you have reason not to.",
    "crumbs": [
      "Pixel Process",
      "Tools",
      "Tools Overview"
    ]
  },
  {
    "objectID": "workflow/tools-overview.html#environment-and-dependency-management",
    "href": "workflow/tools-overview.html#environment-and-dependency-management",
    "title": "Tools Overview",
    "section": "Environment and Dependency Management",
    "text": "Environment and Dependency Management\n\nvenv - A built-in Python module for creating isolated virtual environments to manage dependencies.\nAnaconda - Environment and package manager that supports multiple languages, ideal for data science projects.\nPip - The recommended tool for installing Python packages.",
    "crumbs": [
      "Pixel Process",
      "Tools",
      "Tools Overview"
    ]
  },
  {
    "objectID": "workflow/tools-overview.html#documentation",
    "href": "workflow/tools-overview.html#documentation",
    "title": "Tools Overview",
    "section": "Documentation",
    "text": "Documentation\n\nSphinx - A documentation generator that converts reStructuredText files into HTML, LaTeX, and other formats.\nMkDocs - A static site generator for project documentation using Markdown. -PEP Style Guides - Conventions for best practices in Python.",
    "crumbs": [
      "Pixel Process",
      "Tools",
      "Tools Overview"
    ]
  },
  {
    "objectID": "machine-learning/classification-iris.html",
    "href": "machine-learning/classification-iris.html",
    "title": "Iris Classification",
    "section": "",
    "text": "The Iris Flower Data Set is well documented and commonly used for introducing multivariate classification.\nNotebook goals:",
    "crumbs": [
      "Pixel Process",
      "Classification",
      "Iris Classification"
    ]
  },
  {
    "objectID": "machine-learning/classification-iris.html#data",
    "href": "machine-learning/classification-iris.html#data",
    "title": "Iris Classification",
    "section": "Data",
    "text": "Data\n\nGet the Data\n\nImport the data from seaborn\nBasic data overview\n\n\ndf = sns.load_dataset(\"iris\")\n\n\ntype(df)\n\npandas.core.frame.DataFrame\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\ndf.shape\n\n(150, 5)\n\n\n\ndf['species'].value_counts()\n\nspecies\nsetosa        50\nversicolor    50\nvirginica     50\nName: count, dtype: int64\n\n\n\n# Create list of feature columns\nfeature_cols = [\n    'sepal_length', \n    'sepal_width', \n    'petal_length', \n    'petal_width'\n]",
    "crumbs": [
      "Pixel Process",
      "Classification",
      "Iris Classification"
    ]
  },
  {
    "objectID": "machine-learning/classification-iris.html#eda",
    "href": "machine-learning/classification-iris.html#eda",
    "title": "Iris Classification",
    "section": "EDA",
    "text": "EDA\nDescriptive summary and statistics and data visualization offer insight into patterns and relationships.\nEDA methods change based on data, goals, and analysis.\nThis tutorial focus on:\n\nSummary statistics\nContinuous variables as:\n\nSingle variables (univariate)\n\nBoxplots\nViolin plots\n\nMultiple variables (multivariate)\n\nGrouped barplots\nScatterplots\n\n\n\n\nSummary Statistics\n\npandas df builtins:\n\ninfo\n\nNumber or rows and columns\nInfo on missing values\nDatatype of each column\n\ndescribe:\n\nMeasures of central tendency for numeric columns\n\n\nskimpy skim:\n\nMore comprehensive and visually appealing summary information\n\n\n\n# Built-in pandas function\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  150 non-null    float64\n 1   sepal_width   150 non-null    float64\n 2   petal_length  150 non-null    float64\n 3   petal_width   150 non-null    float64\n 4   species       150 non-null    object \ndtypes: float64(4), object(1)\nmemory usage: 6.0+ KB\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n\n\n\n\n\n\n\n\nskim(df)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ Dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 150    │ │ float64     │ 4     │                                                          │\n│ │ Number of columns │ 5      │ │ string      │ 1     │                                                          │\n│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━━━━┓  │\n│ ┃ column            ┃ NA   ┃ NA %   ┃ mean     ┃ sd        ┃ p0    ┃ p25   ┃ p50    ┃ p75  ┃ p100  ┃ hist    ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━━━━┩  │\n│ │ sepal_length      │    0 │      0 │    5.843 │    0.8281 │   4.3 │   5.1 │    5.8 │  6.4 │   7.9 │ ▃██▇▅▂  │  │\n│ │ sepal_width       │    0 │      0 │    3.057 │    0.4359 │     2 │   2.8 │      3 │  3.3 │   4.4 │ ▁▇█▇▂▁  │  │\n│ │ petal_length      │    0 │      0 │    3.758 │     1.765 │     1 │   1.6 │   4.35 │  5.1 │   6.9 │ █ ▂▇▆▂  │  │\n│ │ petal_width       │    0 │      0 │    1.199 │    0.7622 │   0.1 │   0.3 │    1.3 │  1.8 │   2.5 │ █ ▂▆▄▄  │  │\n│ └───────────────────┴──────┴────────┴──────────┴───────────┴───────┴───────┴────────┴──────┴───────┴─────────┘  │\n│                                                     string                                                      │\n│ ┏━━━━━━━━━┳━━━━┳━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓  │\n│ ┃         ┃    ┃      ┃          ┃            ┃        ┃           ┃ chars per   ┃ words per   ┃             ┃  │\n│ ┃ column  ┃ NA ┃ NA % ┃ shortest ┃ longest    ┃ min    ┃ max       ┃ row         ┃ row         ┃ total words ┃  │\n│ ┡━━━━━━━━━╇━━━━╇━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩  │\n│ │ species │  0 │    0 │ setosa   │ versicolor │ setosa │ virginica │        8.33 │           1 │         150 │  │\n│ └─────────┴────┴──────┴──────────┴────────────┴────────┴───────────┴─────────────┴─────────────┴─────────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\n\n\nData Transformation & Aggregation\nData can be visualized in many ways and must often be transformed for it.\nOur EDA requires two new dfs: - Grouped/aggregated data with means of our numerical data based on species category - Long form data for creating facet visuals in the same figure\n\ngrouped_df=df.groupby('species', as_index=False)[feature_cols].mean()\ngrouped_df\n\n\n\n\n\n\n\n\nspecies\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\nsetosa\n5.006\n3.428\n1.462\n0.246\n\n\n1\nversicolor\n5.936\n2.770\n4.260\n1.326\n\n\n2\nvirginica\n6.588\n2.974\n5.552\n2.026\n\n\n\n\n\n\n\n\nprint(f'Type of grouped_df: {type(grouped_df)}')\nprint(f'Shape of grouped_df: {grouped_df.shape}')\n\nType of grouped_df: &lt;class 'pandas.core.frame.DataFrame'&gt;\nShape of grouped_df: (3, 5)\n\n\n\nlong_df = df.melt(id_vars='species', value_vars=feature_cols)\nlong_df.head()\n\n\n\n\n\n\n\n\nspecies\nvariable\nvalue\n\n\n\n\n0\nsetosa\nsepal_length\n5.1\n\n\n1\nsetosa\nsepal_length\n4.9\n\n\n2\nsetosa\nsepal_length\n4.7\n\n\n3\nsetosa\nsepal_length\n4.6\n\n\n4\nsetosa\nsepal_length\n5.0\n\n\n\n\n\n\n\n\nprint(f'Type of long_df: {type(long_df)}')\nprint(f'Shape of long_df: {long_df.shape}')\n\nType of long_df: &lt;class 'pandas.core.frame.DataFrame'&gt;\nShape of long_df: (600, 3)\n\n\n\nLong DF Subsets\nWe’ll plot petal and sepal features separately.\nThese subset dfs will be used for that.\n\nlong_df.head()\n\n\n\n\n\n\n\n\nspecies\nvariable\nvalue\n\n\n\n\n0\nsetosa\nsepal_length\n5.1\n\n\n1\nsetosa\nsepal_length\n4.9\n\n\n2\nsetosa\nsepal_length\n4.7\n\n\n3\nsetosa\nsepal_length\n4.6\n\n\n4\nsetosa\nsepal_length\n5.0\n\n\n\n\n\n\n\n\nlong_df.shape\n\n(600, 3)\n\n\n\nlong_df['variable'].unique()\n\narray(['sepal_length', 'sepal_width', 'petal_length', 'petal_width'],\n      dtype=object)\n\n\n\nsepal_features = ['sepal_length', 'sepal_width']\npetal_features = ['petal_length', 'petal_width']\n\n\nsepal_df = long_df[long_df['variable'].isin(sepal_features)]\nsepal_df.shape\n\n(300, 3)\n\n\n\npetal_df = long_df[long_df['variable'].isin(petal_features)]\npetal_df.shape\n\n(300, 3)\n\n\n\nprint(sepal_df['variable'].unique())\nprint(petal_df['variable'].unique())\n\n['sepal_length' 'sepal_width']\n['petal_length' 'petal_width']\n\n\n\n\n\nVisualizations\nData will be examined by species using several approaches.\nAll visualizations here offer extensive customization including:\n\nRenaming titles, axes, and subplots\nColor and opacity controls\nChanging font size\nCustomizing or removing legend\nSaving figures and more Limited customization is used here for simplicity’s sake.\n\nNote: Plotly visuals are interative and hovering provides more detail.\n\nBarplots\nThe grouped bar plot below shows the mean for each of the 4 features based on species.\n\nPetal width is shortest feature in all species\nSetosa shows a distinct pattern from the other two where sepal width&gt;petal length\n\n\npx.bar(grouped_df, x=\"species\", y=feature_cols, \n             barmode=\"group\", title=\"Feature Averages by Species\")\n\n\n\n\n\n\n\n\n\n\nBoxplots\nBoxplots are helpful in showing distributions and identifying outliers.\nSee Data Visualization notebook for more details.\nSepal Length by Species\n\nCompares all species on a single feature, sepal length\nThe middle line of each represents the me\nVirginica shows a single, low outlier\n\n\npx.box(df, x=\"species\", y=\"sepal_length\",\n             title=\"Sepal Length by Species\", color=\"species\")\n\n\n\n\n\n\n\n\nSepal Features by Species\n\nProvides length and width boxplots on a shared y-axis\n\nUsing a shared axis facilitates comparison of the features\n\nLength is overall longer (in cm) than width\nNote: sepal length is identical to the above\n\n\npx.box(sepal_df, x=\"species\", y=\"value\", \n             facet_col=\"variable\", title=\"Sepal Features by Species\",\n             color=\"species\")\n\n\n\n\n\n\n\n\nPetal Features by Species\n\nProvides length and width boxplots on a shared y-axis\n\nUsing a shared axis facilitates comparison of the features\n\nLength is overall longer (in cm) than width by species\n\nHowever, setosa petal length is lower than virginica petal width\n\n\n\npx.box(petal_df, x=\"species\", y=\"value\", \n             facet_col=\"variable\", title=\"Petal Features by Species\",\n             color=\"species\")\n\n\n\n\n\n\n\n\n\n\nViolin Plots\nViolons are also helpful in showing distributions and identifying outliers.\nA key distinction between boxplots and violins is the ability to see the distribution pattern.\nSee Data Visualization notebook for more details.\nSepal Length by Species\n\nCompares all species on a single feature, sepal length\nEach species shows a unique density\nSetosa is noticable wider and flatter compared to the other two\nVirginica shows a single, low outlier\n\n\npx.violin(df, x=\"species\", y=\"sepal_length\", \n            color=\"species\", title=\"Sepal Length by Species\")\n\n\n\n\n\n\n\n\nSepal Features by Species\n\nProvides a violin for sepal length and width\nOutliers exist in several features\nNote: sepal length is identical to the above\n\n\npx.violin(sepal_df, x=\"species\", y=\"value\", \n            facet_col=\"variable\", color=\"species\",\n            title=\"Sepal Features by Species\")\n\n\n\n\n\n\n\n\nPetal Features by Species\nProvides a violin for petal length and width\n\npx.violin(petal_df, x=\"species\", y=\"value\", \n            facet_col=\"variable\", color=\"species\",\n            title=\"Sepal Features by Species\")\n\n\n\n\n\n\n\n\n\n\nScatter Plots\nHelp visualize 2-D relationships.\nUsed here to show petal length-width and sepal length-width by species.\nOpacity is set to better visualize overlapping datapoints.\nSee Data Visualization notebook for more details.\n\npx.scatter(df, x=\"petal_length\", y=\"petal_width\", \n                 color=\"species\", opacity=0.6, title=\"Petal Scatter\")\n\n\n\n\n\n\n\n\n\npx.scatter(df, x=\"sepal_length\", y=\"sepal_width\", \n                 color=\"species\", opacity=0.6, title=\"Sepal Scatter\")",
    "crumbs": [
      "Pixel Process",
      "Classification",
      "Iris Classification"
    ]
  },
  {
    "objectID": "machine-learning/classification-iris.html#train-test-split",
    "href": "machine-learning/classification-iris.html#train-test-split",
    "title": "Iris Classification",
    "section": "Train-Test Split",
    "text": "Train-Test Split\nBest practice is to train models only on training data to avoid data leakage. Failure to do so can result in overfit models and less-generalizable results. Beware of model evaluation metrics that do not use a test data set.\n\n\nCrash Report: Why X and y?\n\n\nIn machine learning, X and y follow a math convention:\n\n\n\nX (uppercase): The feature matrix — multiple rows (samples) and columns (features).\n\n\ny (lowercase): The target vector — one value per sample.\n\n\n\nThink of it like this:\n\n\nX = [ [f11, f12, f13],\n      [f21, f22, f23],\n      [f31, f32, f33] ]  # features matrix\n\ny = [ y1, y2, y3 ]       # target vector\n  \n\nRule of Thumb: X is big because it holds all your inputs. y is small because it’s just the outputs.\n\n\n\ny = df['species']\nX = df[feature_cols]\n\n\nX.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n\n\n\n\n\n\ny.head()\n\n0    setosa\n1    setosa\n2    setosa\n3    setosa\n4    setosa\nName: species, dtype: object\n\n\n\n# Ensure matching row dimensions\nprint(f'X shape: {X.shape}')\nprint(f'y shape: {y.shape}')\n\nX shape: (150, 4)\ny shape: (150,)\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                test_size=0.33, random_state=42, stratify=y)",
    "crumbs": [
      "Pixel Process",
      "Classification",
      "Iris Classification"
    ]
  },
  {
    "objectID": "machine-learning/classification-iris.html#model-training",
    "href": "machine-learning/classification-iris.html#model-training",
    "title": "Iris Classification",
    "section": "Model Training",
    "text": "Model Training\nUsing sklearn models that follow an instantiate, fit, predict pattern.\n\nInstantiate: Select model with preferred parameters\nFit: Trains the models on training data\nPredicts: Apply model on previously unseen data for evaluation and predictions\n\nNote: Many algorithms and models use random states/seeds. Setting this value ensures consistent results which is useful for teaching and direct replication.\n\nModel Instantiation\nUsing “out-of-the-box” sklearn models with little to specify.\nWill set max_iter=1000 instead of default 100 to increase performance since we are using a small dataset.\n\nlr = LogisticRegression(max_iter=1000, random_state=42)\n\n\nrf = RandomForestClassifier(random_state=42)\n\n\n\nModel Training\nSimply call fit on our models and give our X features, and y target.\nNOTE: models are fit ‘in-place’ so no new variable assignment is needed.\n\nlr.fit(X_train, y_train)\n\nLogisticRegression(max_iter=1000, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression(max_iter=1000, random_state=42) \n\n\n\nrf.fit(X_train, y_train)\n\nRandomForestClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(random_state=42) \n\n\n\n\nModel Predict\nCall predict, on the now fit models, with a new set of feature data and get back predictions.\nSave the predications to new variables for evaluation below.\nNOTE: We will evaluation both training and testing performance.\n\nlr_train_preds = lr.predict(X_train)\nlr_test_preds = lr.predict(X_test)\n\n\nlr_test_preds\n\narray(['versicolor', 'versicolor', 'setosa', 'versicolor', 'virginica',\n       'versicolor', 'versicolor', 'setosa', 'versicolor', 'versicolor',\n       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'virginica',\n       'virginica', 'versicolor', 'virginica', 'versicolor', 'virginica',\n       'versicolor', 'setosa', 'virginica', 'setosa', 'virginica',\n       'virginica', 'setosa', 'setosa', 'virginica', 'virginica',\n       'virginica', 'setosa', 'versicolor', 'setosa', 'setosa',\n       'virginica', 'versicolor', 'virginica', 'versicolor', 'versicolor',\n       'versicolor', 'setosa', 'setosa', 'virginica', 'versicolor',\n       'virginica', 'versicolor', 'versicolor', 'virginica'], dtype=object)\n\n\n\nrf_train_preds = rf.predict(X_train)\nrf_test_preds = rf.predict(X_test)\n\n\nrf_test_preds\n\narray(['versicolor', 'versicolor', 'setosa', 'versicolor', 'virginica',\n       'versicolor', 'versicolor', 'setosa', 'versicolor', 'versicolor',\n       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'virginica',\n       'versicolor', 'versicolor', 'virginica', 'versicolor', 'virginica',\n       'versicolor', 'setosa', 'virginica', 'setosa', 'virginica',\n       'virginica', 'setosa', 'setosa', 'virginica', 'virginica',\n       'virginica', 'setosa', 'versicolor', 'setosa', 'setosa',\n       'virginica', 'versicolor', 'virginica', 'versicolor', 'versicolor',\n       'versicolor', 'setosa', 'setosa', 'versicolor', 'versicolor',\n       'virginica', 'versicolor', 'versicolor', 'virginica'], dtype=object)",
    "crumbs": [
      "Pixel Process",
      "Classification",
      "Iris Classification"
    ]
  },
  {
    "objectID": "machine-learning/classification-iris.html#model-evaluation",
    "href": "machine-learning/classification-iris.html#model-evaluation",
    "title": "Iris Classification",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nMany evalutation metrics exists and appropriate choices depend on usage.\nAccuracy will be the focus here, but other metrics explored as well.\nConfusion matrices are used to visualize results.\n\n\nCrash Report: Which Metrics Matter?\n\n\nAlways report model performance on unseen test data. Training metrics are only useful for diagnosing overfitting or underfitting during learning. Never tune your model on the test set!\n\n\n\nAccuracy Scores\nAccuracy = (Number of Correct Predictions) / (Total Number of Samples)\nUseful classification metric with balanced datasets.\nInsights:\n\nTrain accuracy is higher than test for both models (expected)\nTest accuracy is worse for rf than lr despite rf having 100% training accuracy\nImplies potential overfitting by the rf model\n\n\nlr_train_acc = accuracy_score(y_train, lr_train_preds)\nlr_test_acc =  accuracy_score(y_test, lr_test_preds)\nprint(f'Logistic Regression Train Accuracy Score: {lr_train_acc: .2f} ')\nprint(f'Logistic Regression Test Accuracy Score: {lr_test_acc: .2f} ')\nprint(f'\\n')\n\nrf_train_acc = accuracy_score(y_train, rf_train_preds)\nrf_test_acc =  accuracy_score(y_test, rf_test_preds)\nprint(f'Random Forest Train Accuracy Score: {rf_train_acc: .2f} ')\nprint(f'Random Forest Test Accuracy Score: {rf_test_acc: .2f} ')\n\nLogistic Regression Train Accuracy Score:  0.97 \nLogistic Regression Test Accuracy Score:  0.94 \n\n\nRandom Forest Train Accuracy Score:  1.00 \nRandom Forest Test Accuracy Score:  0.90 \n\n\n\n\nClassification Reports\nIn addition to accuracy, reports precision, recall, f-1 score, and support. Additionally, includes metrics for each class and overall model.\nSee Classification Metrics notebook for more details.\nInsights:\n\nTrain performance is higher than test for both models on all metrics\nSetosa (class 0) was the most easily identified (see EDA above to guess why)\nReport syntax is identical for each evalution-only change is pairing the correct true and predicted values\n\n\nLogistic Regression Train Classification Report\n\nprint(classification_report(y_train, lr_train_preds))\n\n              precision    recall  f1-score   support\n\n      setosa       1.00      1.00      1.00        34\n  versicolor       0.97      0.94      0.95        33\n   virginica       0.94      0.97      0.96        33\n\n    accuracy                           0.97       100\n   macro avg       0.97      0.97      0.97       100\nweighted avg       0.97      0.97      0.97       100\n\n\n\n\n\nLogistic Regression Test Classification Report\n\nprint(classification_report(y_test, lr_test_preds))\n\n              precision    recall  f1-score   support\n\n      setosa       1.00      1.00      1.00        16\n  versicolor       0.89      0.94      0.91        17\n   virginica       0.94      0.88      0.91        17\n\n    accuracy                           0.94        50\n   macro avg       0.94      0.94      0.94        50\nweighted avg       0.94      0.94      0.94        50\n\n\n\n\n\nRandom Forest Train Classification Report\n\nprint(classification_report(y_train, rf_train_preds))\n\n              precision    recall  f1-score   support\n\n      setosa       1.00      1.00      1.00        34\n  versicolor       1.00      1.00      1.00        33\n   virginica       1.00      1.00      1.00        33\n\n    accuracy                           1.00       100\n   macro avg       1.00      1.00      1.00       100\nweighted avg       1.00      1.00      1.00       100\n\n\n\n\n\nRandom Forest Test Classification Report\n\nprint(classification_report(y_test, rf_test_preds))\n\n              precision    recall  f1-score   support\n\n      setosa       1.00      1.00      1.00        16\n  versicolor       0.80      0.94      0.86        17\n   virginica       0.93      0.76      0.84        17\n\n    accuracy                           0.90        50\n   macro avg       0.91      0.90      0.90        50\nweighted avg       0.91      0.90      0.90        50\n\n\n\n\n\n\nConfusion Matrix\nA visualization of classification model performance.\nThe y-axis if true label, the x-axis is predicted. So the diagonal represents correct predictions. It is a great approach to understanding which classes are being confused and how.\nInsights:\n\nTrain performance is higher than test for both models on all metrics\nSetosa was the most easily identified (see EDA above to figure out why)\nReport syntax is identical for each evalution-only change is pairing the correct true and predicted values\n\n\nLogistic Regression Train Confusion Matrix\n\nConfusionMatrixDisplay.from_predictions(\n    y_true=y_train, y_pred=lr_train_preds, cmap='Blues')\n\n\n\n\n\n\n\n\n\n\nLogistic Regression Test Confusion Matrix\n\nConfusionMatrixDisplay.from_predictions(\n    y_true=y_test, y_pred=lr_test_preds, cmap='Blues')\n\n\n\n\n\n\n\n\n\n\nRandom Forest Train Confusion Matrix\n\nConfusionMatrixDisplay.from_predictions(\n    y_true=y_train, y_pred=rf_train_preds, cmap='Blues')\n\n\n\n\n\n\n\n\n\n\nRandom Forest Test Confusion Matrix\n\nConfusionMatrixDisplay.from_predictions(\n    y_true=y_test, y_pred=rf_test_preds, cmap='Blues')",
    "crumbs": [
      "Pixel Process",
      "Classification",
      "Iris Classification"
    ]
  },
  {
    "objectID": "machine-learning/data-science-topics.html",
    "href": "machine-learning/data-science-topics.html",
    "title": "Data Science Topics",
    "section": "",
    "text": "Supervised learning involves training a model on a labeled dataset, which means that each training example is paired with an output label. The goal is for the model to learn a mapping from inputs to outputs so it can predict the output for new, unseen inputs.\n\n\n\n\nUnsupervised learning involves training a model on data without labeled responses. The goal is to find hidden patterns or intrinsic structures in the input data.\n\n\n\n\nClassification is a type of supervised learning where the goal is to predict a discrete class label for a given input.\n\n\n\n\nRegression is a type of supervised learning where the goal is to predict a continuous value for a given input.\n\n\n\nComputer vision is a field of artificial intelligence that enables computers to interpret and make decisions based on visual data from the world.\n\n\n\n\nNLP involves the interaction between computers and humans through natural language. The goal is to enable computers to understand, interpret, and generate human language.",
    "crumbs": [
      "Pixel Process",
      "Models & Metrics",
      "Data Science Topics"
    ]
  },
  {
    "objectID": "machine-learning/data-science-topics.html#data-science-applications",
    "href": "machine-learning/data-science-topics.html#data-science-applications",
    "title": "Data Science Topics",
    "section": "",
    "text": "Supervised learning involves training a model on a labeled dataset, which means that each training example is paired with an output label. The goal is for the model to learn a mapping from inputs to outputs so it can predict the output for new, unseen inputs.\n\n\n\n\nUnsupervised learning involves training a model on data without labeled responses. The goal is to find hidden patterns or intrinsic structures in the input data.\n\n\n\n\nClassification is a type of supervised learning where the goal is to predict a discrete class label for a given input.\n\n\n\n\nRegression is a type of supervised learning where the goal is to predict a continuous value for a given input.\n\n\n\nComputer vision is a field of artificial intelligence that enables computers to interpret and make decisions based on visual data from the world.\n\n\n\n\nNLP involves the interaction between computers and humans through natural language. The goal is to enable computers to understand, interpret, and generate human language.",
    "crumbs": [
      "Pixel Process",
      "Models & Metrics",
      "Data Science Topics"
    ]
  },
  {
    "objectID": "machine-learning/index.html",
    "href": "machine-learning/index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Machine learning isn’t magic — it’s a process. This section walks through the core workflow: exploring and shaping data, building and training models, and testing how well they perform. Expect practical tools, clear examples, and real evaluation — not black boxes or buzzwords.\nIt’s not the model — it’s what you do with it.",
    "crumbs": [
      "Pixel Process",
      "Machine Learning"
    ]
  },
  {
    "objectID": "machine-learning/index.html#machine-learning-topics",
    "href": "machine-learning/index.html#machine-learning-topics",
    "title": "Machine Learning",
    "section": "Machine Learning Topics",
    "text": "Machine Learning Topics\n\nExplorator Data Analysis (EDA): Understand data properties, data types, missing data, outliers, and more key factors to effective ML.\nClassification: Models that work on categorized predictions (spam/not-spam).\nRegression: Models that prediction continuous values (stock prices).\nEvaluation Metrics: Determine how to evaluate a model and assess performance.\n\nNeed a place to start? Check out the FAQs!\n\n\n\nModels & Metrics\n\n\n\n What can ML do? → Overview of data and algorithms for ML\n\n\n Random Forest (NB) → Deep dive into decision trees and random forest models\n\n\n\n\n\nClassification\n\n\n\n Iris Classification (NB) → Complete classification pipeline based on Iris dataset\n\n\n\n\n\nRegression\n\n\n\n Housing Regression (NB) → Complete regression pipeline based on housing dataset\n\n\n\n\n\n\n\n\n\n\n\nChasing Scores, Missing Stories\n\n\n\nI used to run dozens of models, hyperparameter tuning, and comparitive metrics. While it taught me a lot, the biggest lesson was:\nModeling is a tool, meaning is the goal.\nUnderstand data — no good models are built on bad data\nFit Models — no good inference comes from inappropriate models\nTest Performance — all models are poor until proven otherwise\n\n\n\n“I fear not the man who has practiced 10,000 kicks once, but I fear the man who has practiced one kick 10,000 times”— Bruce Lee\n\nMachine learning isn’t magic, and it’s not about finding the model — it’s about applying the right tools to meaningful questions. The most advanced models require on proper input and programming.",
    "crumbs": [
      "Pixel Process",
      "Machine Learning"
    ]
  }
]